{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/boneeyah/QTW/blob/main/Case%20Study%207/CaseStudy7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OZryWKHR3JAd"
      },
      "outputs": [],
      "source": [
        "#!pip install scikit-learn-intelex\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import tensorflow.keras.backend as K\n",
        "#from sklearnex import patch_sklearn\n",
        "#patch_sklearn()\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from numpy import random\n",
        "from itertools import product\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, classification_report\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "#import xgboost as xgb\n",
        "#from xgboost import DMatrix\n",
        "\n",
        "#from numpy import random\n",
        "#from itertools import product\n",
        "#from sklearn.utils import shuffle\n",
        "\n",
        "import datetime"
      ],
      "metadata": {
        "id": "HrVUsV_b8Ezp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3090fd6c-cf98-4f2f-dd1f-35a1a1f52425"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-07-30 03:51:15.030669: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-30 03:51:15.687881: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('drive')"
      ],
      "metadata": {
        "id": "T6H6L4bc8YlZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df = pd.read_csv(\"drive/My Drive/final_project(5).csv\")\n",
        "df = pd.read_csv(\"/home/migue/Downloads/final_project(5).csv\")"
      ],
      "metadata": {
        "id": "hngzaeNL-EXs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()\n",
        "#there are NAs will need handling"
      ],
      "metadata": {
        "id": "kjzOue9y_SuK",
        "outputId": "9db532b2-033d-4a83-d9fb-30608f605c1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 160000 entries, 0 to 159999\n",
            "Data columns (total 51 columns):\n",
            " #   Column  Non-Null Count   Dtype  \n",
            "---  ------  --------------   -----  \n",
            " 0   x0      159974 non-null  float64\n",
            " 1   x1      159975 non-null  float64\n",
            " 2   x2      159962 non-null  float64\n",
            " 3   x3      159963 non-null  float64\n",
            " 4   x4      159974 non-null  float64\n",
            " 5   x5      159963 non-null  float64\n",
            " 6   x6      159974 non-null  float64\n",
            " 7   x7      159973 non-null  float64\n",
            " 8   x8      159979 non-null  float64\n",
            " 9   x9      159970 non-null  float64\n",
            " 10  x10     159957 non-null  float64\n",
            " 11  x11     159970 non-null  float64\n",
            " 12  x12     159964 non-null  float64\n",
            " 13  x13     159969 non-null  float64\n",
            " 14  x14     159966 non-null  float64\n",
            " 15  x15     159965 non-null  float64\n",
            " 16  x16     159974 non-null  float64\n",
            " 17  x17     159973 non-null  float64\n",
            " 18  x18     159960 non-null  float64\n",
            " 19  x19     159965 non-null  float64\n",
            " 20  x20     159962 non-null  float64\n",
            " 21  x21     159971 non-null  float64\n",
            " 22  x22     159973 non-null  float64\n",
            " 23  x23     159953 non-null  float64\n",
            " 24  x24     159972 non-null  object \n",
            " 25  x25     159978 non-null  float64\n",
            " 26  x26     159964 non-null  float64\n",
            " 27  x27     159970 non-null  float64\n",
            " 28  x28     159965 non-null  float64\n",
            " 29  x29     159970 non-null  object \n",
            " 30  x30     159970 non-null  object \n",
            " 31  x31     159961 non-null  float64\n",
            " 32  x32     159969 non-null  object \n",
            " 33  x33     159959 non-null  float64\n",
            " 34  x34     159959 non-null  float64\n",
            " 35  x35     159970 non-null  float64\n",
            " 36  x36     159973 non-null  float64\n",
            " 37  x37     159977 non-null  object \n",
            " 38  x38     159969 non-null  float64\n",
            " 39  x39     159977 non-null  float64\n",
            " 40  x40     159964 non-null  float64\n",
            " 41  x41     159960 non-null  float64\n",
            " 42  x42     159974 non-null  float64\n",
            " 43  x43     159963 non-null  float64\n",
            " 44  x44     159960 non-null  float64\n",
            " 45  x45     159971 non-null  float64\n",
            " 46  x46     159969 non-null  float64\n",
            " 47  x47     159963 non-null  float64\n",
            " 48  x48     159968 non-null  float64\n",
            " 49  x49     159968 non-null  float64\n",
            " 50  y       160000 non-null  int64  \n",
            "dtypes: float64(45), int64(1), object(5)\n",
            "memory usage: 62.3+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in df.columns:\n",
        "    if df[i].isna().sum() > 0:\n",
        "        print('{} has {}% NAs'.format(i,(df[i].isna().sum() / len(df))*100))"
      ],
      "metadata": {
        "id": "CC3yFJ6BzzOw",
        "outputId": "571929ab-44af-4e28-84d4-90bb78795162",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x0 has 0.01625% NAs\n",
            "x1 has 0.015625% NAs\n",
            "x2 has 0.02375% NAs\n",
            "x3 has 0.023125% NAs\n",
            "x4 has 0.01625% NAs\n",
            "x5 has 0.023125% NAs\n",
            "x6 has 0.01625% NAs\n",
            "x7 has 0.016875% NAs\n",
            "x8 has 0.013125% NAs\n",
            "x9 has 0.01875% NAs\n",
            "x10 has 0.026875% NAs\n",
            "x11 has 0.01875% NAs\n",
            "x12 has 0.0225% NAs\n",
            "x13 has 0.019375% NAs\n",
            "x14 has 0.021249999999999998% NAs\n",
            "x15 has 0.021875000000000002% NAs\n",
            "x16 has 0.01625% NAs\n",
            "x17 has 0.016875% NAs\n",
            "x18 has 0.025% NAs\n",
            "x19 has 0.021875000000000002% NAs\n",
            "x20 has 0.02375% NAs\n",
            "x21 has 0.018125% NAs\n",
            "x22 has 0.016875% NAs\n",
            "x23 has 0.029375000000000002% NAs\n",
            "x24 has 0.017499999999999998% NAs\n",
            "x25 has 0.01375% NAs\n",
            "x26 has 0.0225% NAs\n",
            "x27 has 0.01875% NAs\n",
            "x28 has 0.021875000000000002% NAs\n",
            "x29 has 0.01875% NAs\n",
            "x30 has 0.01875% NAs\n",
            "x31 has 0.024374999999999997% NAs\n",
            "x32 has 0.019375% NAs\n",
            "x33 has 0.025625000000000002% NAs\n",
            "x34 has 0.025625000000000002% NAs\n",
            "x35 has 0.01875% NAs\n",
            "x36 has 0.016875% NAs\n",
            "x37 has 0.014374999999999999% NAs\n",
            "x38 has 0.019375% NAs\n",
            "x39 has 0.014374999999999999% NAs\n",
            "x40 has 0.0225% NAs\n",
            "x41 has 0.025% NAs\n",
            "x42 has 0.01625% NAs\n",
            "x43 has 0.023125% NAs\n",
            "x44 has 0.025% NAs\n",
            "x45 has 0.018125% NAs\n",
            "x46 has 0.019375% NAs\n",
            "x47 has 0.023125% NAs\n",
            "x48 has 0.02% NAs\n",
            "x49 has 0.02% NAs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['x24'].value_counts()"
      ],
      "metadata": {
        "id": "TybuIbMLz6Ng",
        "outputId": "db7ad0b3-b45d-4b90-ecaa-13e4702287e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "x24\n",
              "asia       138965\n",
              "euorpe      16538\n",
              "america      4469\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.bar(x=df['x24'].value_counts().index,height=df['x24'].value_counts().values)"
      ],
      "metadata": {
        "id": "ME56MKaIz7sN",
        "outputId": "a905e437-ff1f-4b51-ab62-863e0db54df8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 3 artists>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['x29'].value_counts()"
      ],
      "metadata": {
        "id": "P7rMlCalz9WH",
        "outputId": "75868e5f-81b8-469f-aed2-f17f3d8b3f8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "x29\n",
              "July       45569\n",
              "Jun        41329\n",
              "Aug        29406\n",
              "May        21939\n",
              "sept.      10819\n",
              "Apr         6761\n",
              "Oct         2407\n",
              "Mar         1231\n",
              "Nov          337\n",
              "Feb          140\n",
              "Dev           23\n",
              "January        9\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#clean up month formatting to standard 3 letters\n",
        "df['x29'].replace(to_replace=['July','sept.','Dev','January'],value=['Jul','Sep','Dec','Jan'],inplace=True)"
      ],
      "metadata": {
        "id": "gB-gkjC2z_fK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mo = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\n",
        "heights = []\n",
        "for i in mo:\n",
        "    x = df['x29'].to_list().count(i)\n",
        "    heights.append(x)\n",
        "plt.bar(x=mo,height=heights)"
      ],
      "metadata": {
        "id": "eZTNOL9P0CM3",
        "outputId": "b71301c7-6fad-4af4-c277-0885d34aef72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 12 artists>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['x30'].value_counts()"
      ],
      "metadata": {
        "id": "_1LSKgy80ENF",
        "outputId": "2f592b7c-31d1-4bc2-cbdd-536b8ca75efe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "x30\n",
              "wednesday    101535\n",
              "thurday       29429\n",
              "tuesday       27954\n",
              "friday          564\n",
              "monday          488\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# thursday is incorrect\n",
        "df['x30'].replace(to_replace = 'thurday',value='thursday')\n",
        "days = ['monday','tuesday','wednesday','thursday','friday']\n",
        "d_heights = []\n",
        "for i in days:\n",
        "    x = df['x30'].to_list().count(i)\n",
        "    d_heights.append(x)\n",
        "plt.bar(x=days,height = d_heights)"
      ],
      "metadata": {
        "id": "3hFwgArL0GAB",
        "outputId": "1e5e239c-c470-48de-ac9e-b4f54a3dc0cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 5 artists>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkIAAAGdCAYAAAD+JxxnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA0E0lEQVR4nO3de1xVVf7/8TeXuIgeUFQuX0ktb/j1roVoXhpJKvM7lDNfLafUQW0STGRKsQztatmUlzIda0Zs0kfm9NUaTYp01FIGFS+pCVpp6ihoKaCUorB+f/Rg/zziJBaEuF7Px+M8Hp69PnvttRebc97us8/GwxhjBAAAYCHPmh4AAABATSEIAQAAaxGEAACAtQhCAADAWgQhAABgLYIQAACwFkEIAABYiyAEAACs5V3TA7ialZWV6ciRI6pXr548PDxqejgAAKASjDE6deqUwsPD5en54+d8CEI/4siRI4qIiKjpYQAAgJ/g0KFDatKkyY/WEIR+RL169ST9MJEul6uGRwMAACqjqKhIERERzvv4jyEI/Yjyj8NcLhdBCACAWqYyl7VwsTQAALAWQQgAAFiLIAQAAKxFEAIAANYiCAEAAGsRhAAAgLUIQgAAwFoEIQAAYC2CEAAAsBZBCAAAWIsgBAAArEUQAgAA1iIIAQAAaxGEAACAtbxregAA7NAsZWVND6HWOPD8gJoeAmCNKz4jtH79eg0cOFDh4eHy8PDQ8uXL3dqNMUpNTVVYWJj8/f0VExOjffv2udWcOHFCQ4cOlcvlUlBQkOLj43X69Gm3ms8++0y9evWSn5+fIiIiNH369ApjWbp0qdq0aSM/Pz+1b99eH3zwwRWPBQAA2OuKg1BxcbE6duyoOXPmXLJ9+vTpmj17tubNm6esrCwFBAQoNjZWZ86ccWqGDh2q3bt3KyMjQytWrND69es1evRop72oqEj9+/dX06ZNlZ2drRdffFFTp07V/PnznZqNGzfq3nvvVXx8vLZt26a4uDjFxcVp165dVzQWAABgLw9jjPnJK3t4aNmyZYqLi5P0wxmY8PBw/fGPf9QjjzwiSSosLFRISIjS0tI0ZMgQ7dmzR23bttXmzZvVrVs3SVJ6erruvPNOHT58WOHh4Zo7d64ef/xx5eXlycfHR5KUkpKi5cuXKycnR5I0ePBgFRcXa8WKFc54unfvrk6dOmnevHmVGsvlFBUVKTAwUIWFhXK5XD91mgCIj8auBB+NAT/Plbx/V+nF0vv371deXp5iYmKcZYGBgYqKilJmZqYkKTMzU0FBQU4IkqSYmBh5enoqKyvLqendu7cTgiQpNjZWubm5OnnypFNz4XbKa8q3U5mxAAAAu1XpxdJ5eXmSpJCQELflISEhTlteXp4aN27sPghvbzVo0MCtpnnz5hX6KG+rX7++8vLyLrudy43lYmfPntXZs2ed50VFRZfZYwAAUJvx9fkLTJs2TYGBgc4jIiKipocEAACqUZUGodDQUElSfn6+2/L8/HynLTQ0VMeOHXNrP3/+vE6cOOFWc6k+LtzGf6q5sP1yY7nYpEmTVFhY6DwOHTpUib0GAAC1VZUGoebNmys0NFSrV692lhUVFSkrK0vR0dGSpOjoaBUUFCg7O9upWbNmjcrKyhQVFeXUrF+/XufOnXNqMjIy1Lp1a9WvX9+puXA75TXl26nMWC7m6+srl8vl9gAAANeuKw5Cp0+f1vbt27V9+3ZJP1yUvH37dh08eFAeHh5KSkrSM888o/fff187d+7UAw88oPDwcOebZZGRkbr99ts1atQobdq0SRs2bFBiYqKGDBmi8PBwSdJ9990nHx8fxcfHa/fu3VqyZIlmzZql5ORkZxzjxo1Tenq6XnrpJeXk5Gjq1KnasmWLEhMTJalSYwEAAHa74oult2zZoltvvdV5Xh5Ohg0bprS0NE2YMEHFxcUaPXq0CgoKdMsttyg9PV1+fn7OOosWLVJiYqL69esnT09PDRo0SLNnz3baAwMD9dFHHykhIUFdu3ZVw4YNlZqa6navoR49emjx4sWaPHmyHnvsMbVs2VLLly9Xu3btnJrKjAUAANjrZ91H6FrHfYSAqsN9hCqP+wgBP0+N3UcIAACgNiEIAQAAaxGEAACAtQhCAADAWgQhAABgLYIQAACwFkEIAABYiyAEAACsRRACAADWIggBAABrEYQAAIC1CEIAAMBaBCEAAGAtghAAALAWQQgAAFiLIAQAAKxFEAIAANYiCAEAAGsRhAAAgLUIQgAAwFoEIQAAYC2CEAAAsBZBCAAAWIsgBAAArEUQAgAA1iIIAQAAaxGEAACAtQhCAADAWgQhAABgLYIQAACwFkEIAABYiyAEAACsRRACAADWIggBAABrEYQAAIC1CEIAAMBaBCEAAGAtghAAALAWQQgAAFiLIAQAAKxFEAIAANYiCAEAAGsRhAAAgLUIQgAAwFoEIQAAYC2CEAAAsBZBCAAAWIsgBAAArEUQAgAA1iIIAQAAaxGEAACAtQhCAADAWgQhAABgLYIQAACwFkEIAABYiyAEAACsRRACAADWIggBAABrEYQAAIC1qjwIlZaW6oknnlDz5s3l7++vG2+8UU8//bSMMU6NMUapqakKCwuTv7+/YmJitG/fPrd+Tpw4oaFDh8rlcikoKEjx8fE6ffq0W81nn32mXr16yc/PTxEREZo+fXqF8SxdulRt2rSRn5+f2rdvrw8++KCqdxkAANRSVR6EXnjhBc2dO1evvvqq9uzZoxdeeEHTp0/XK6+84tRMnz5ds2fP1rx585SVlaWAgADFxsbqzJkzTs3QoUO1e/duZWRkaMWKFVq/fr1Gjx7ttBcVFal///5q2rSpsrOz9eKLL2rq1KmaP3++U7Nx40bde++9io+P17Zt2xQXF6e4uDjt2rWrqncbAADUQh7mwlM1VeCuu+5SSEiI/vKXvzjLBg0aJH9/f7311lsyxig8PFx//OMf9cgjj0iSCgsLFRISorS0NA0ZMkR79uxR27ZttXnzZnXr1k2SlJ6erjvvvFOHDx9WeHi45s6dq8cff1x5eXny8fGRJKWkpGj58uXKycmRJA0ePFjFxcVasWKFM5bu3burU6dOmjdv3mX3paioSIGBgSosLJTL5aqyOQJs1CxlZU0PodY48PyAmh4CUKtdyft3lZ8R6tGjh1avXq29e/dKknbs2KFPP/1Ud9xxhyRp//79ysvLU0xMjLNOYGCgoqKilJmZKUnKzMxUUFCQE4IkKSYmRp6ensrKynJqevfu7YQgSYqNjVVubq5Onjzp1Fy4nfKa8u1c7OzZsyoqKnJ7AACAa5d3VXeYkpKioqIitWnTRl5eXiotLdWzzz6roUOHSpLy8vIkSSEhIW7rhYSEOG15eXlq3Lix+0C9vdWgQQO3mubNm1foo7ytfv36ysvL+9HtXGzatGl68sknf8puAwCAWqjKzwi98847WrRokRYvXqytW7dq4cKF+tOf/qSFCxdW9aaq3KRJk1RYWOg8Dh06VNNDAgAA1ajKzwg9+uijSklJ0ZAhQyRJ7du319dff61p06Zp2LBhCg0NlSTl5+crLCzMWS8/P1+dOnWSJIWGhurYsWNu/Z4/f14nTpxw1g8NDVV+fr5bTfnzy9WUt1/M19dXvr6+P2W3AQBALVTlZ4S+++47eXq6d+vl5aWysjJJUvPmzRUaGqrVq1c77UVFRcrKylJ0dLQkKTo6WgUFBcrOznZq1qxZo7KyMkVFRTk169ev17lz55yajIwMtW7dWvXr13dqLtxOeU35dgAAgN2qPAgNHDhQzz77rFauXKkDBw5o2bJlevnll3X33XdLkjw8PJSUlKRnnnlG77//vnbu3KkHHnhA4eHhiouLkyRFRkbq9ttv16hRo7Rp0yZt2LBBiYmJGjJkiMLDwyVJ9913n3x8fBQfH6/du3dryZIlmjVrlpKTk52xjBs3Tunp6XrppZeUk5OjqVOnasuWLUpMTKzq3QYAALVQlX809sorr+iJJ57QmDFjdOzYMYWHh+vBBx9UamqqUzNhwgQVFxdr9OjRKigo0C233KL09HT5+fk5NYsWLVJiYqL69esnT09PDRo0SLNnz3baAwMD9dFHHykhIUFdu3ZVw4YNlZqa6navoR49emjx4sWaPHmyHnvsMbVs2VLLly9Xu3btqnq3AQBALVTl9xG6lnAfIaDqcB+hyuM+QsDPU6P3EQIAAKgtCEIAAMBaBCEAAGAtghAAALAWQQgAAFiLIAQAAKxFEAIAANYiCAEAAGsRhAAAgLUIQgAAwFoEIQAAYC2CEAAAsBZBCAAAWIsgBAAArEUQAgAA1iIIAQAAaxGEAACAtQhCAADAWgQhAABgLYIQAACwFkEIAABYiyAEAACsRRACAADWIggBAABrEYQAAIC1CEIAAMBaBCEAAGAtghAAALAWQQgAAFiLIAQAAKxFEAIAANYiCAEAAGsRhAAAgLUIQgAAwFoEIQAAYC2CEAAAsBZBCAAAWIsgBAAArEUQAgAA1iIIAQAAaxGEAACAtQhCAADAWgQhAABgLYIQAACwFkEIAABYiyAEAACsRRACAADWIggBAABrEYQAAIC1CEIAAMBaBCEAAGAtghAAALAWQQgAAFiLIAQAAKxFEAIAANYiCAEAAGsRhAAAgLUIQgAAwFrVEoT+/e9/63e/+52Cg4Pl7++v9u3ba8uWLU67MUapqakKCwuTv7+/YmJitG/fPrc+Tpw4oaFDh8rlcikoKEjx8fE6ffq0W81nn32mXr16yc/PTxEREZo+fXqFsSxdulRt2rSRn5+f2rdvrw8++KA6dhkAANRCVR6ETp48qZ49e+q6667TqlWr9Pnnn+ull15S/fr1nZrp06dr9uzZmjdvnrKyshQQEKDY2FidOXPGqRk6dKh2796tjIwMrVixQuvXr9fo0aOd9qKiIvXv319NmzZVdna2XnzxRU2dOlXz5893ajZu3Kh7771X8fHx2rZtm+Li4hQXF6ddu3ZV9W4DAIBayMMYY6qyw5SUFG3YsEGffPLJJduNMQoPD9cf//hHPfLII5KkwsJChYSEKC0tTUOGDNGePXvUtm1bbd68Wd26dZMkpaen684779Thw4cVHh6uuXPn6vHHH1deXp58fHycbS9fvlw5OTmSpMGDB6u4uFgrVqxwtt+9e3d16tRJ8+bNu+y+FBUVKTAwUIWFhXK5XD9rXgDbNUtZWdNDqDUOPD+gpocA1GpX8v5d5WeE3n//fXXr1k2//e1v1bhxY3Xu3Fmvv/66075//37l5eUpJibGWRYYGKioqChlZmZKkjIzMxUUFOSEIEmKiYmRp6ensrKynJrevXs7IUiSYmNjlZubq5MnTzo1F26nvKZ8Oxc7e/asioqK3B4AAODaVeVB6KuvvtLcuXPVsmVLffjhh3rooYf08MMPa+HChZKkvLw8SVJISIjbeiEhIU5bXl6eGjdu7Nbu7e2tBg0auNVcqo8Lt/GfasrbLzZt2jQFBgY6j4iIiCvefwAAUHtUeRAqKytTly5d9Nxzz6lz584aPXq0Ro0aVamPomrapEmTVFhY6DwOHTpU00MCAADVqMqDUFhYmNq2beu2LDIyUgcPHpQkhYaGSpLy8/PdavLz85220NBQHTt2zK39/PnzOnHihFvNpfq4cBv/qaa8/WK+vr5yuVxuDwAAcO2q8iDUs2dP5ebmui3bu3evmjZtKklq3ry5QkNDtXr1aqe9qKhIWVlZio6OliRFR0eroKBA2dnZTs2aNWtUVlamqKgop2b9+vU6d+6cU5ORkaHWrVs731CLjo522055Tfl2AACA3ao8CI0fP17/+te/9Nxzz+mLL77Q4sWLNX/+fCUkJEiSPDw8lJSUpGeeeUbvv/++du7cqQceeEDh4eGKi4uT9MMZpNtvv12jRo3Spk2btGHDBiUmJmrIkCEKDw+XJN13333y8fFRfHy8du/erSVLlmjWrFlKTk52xjJu3Dilp6frpZdeUk5OjqZOnaotW7YoMTGxqncbAADUQt5V3eFNN92kZcuWadKkSXrqqafUvHlzzZw5U0OHDnVqJkyYoOLiYo0ePVoFBQW65ZZblJ6eLj8/P6dm0aJFSkxMVL9+/eTp6alBgwZp9uzZTntgYKA++ugjJSQkqGvXrmrYsKFSU1Pd7jXUo0cPLV68WJMnT9Zjjz2mli1bavny5WrXrl1V7zYAAKiFqvw+QtcS7iMEVB3uI1R53EcI+Hlq9D5CAAAAtQVBCAAAWIsgBAAArEUQAgAA1iIIAQAAaxGEAACAtQhCAADAWgQhAABgLYIQAACwFkEIAABYiyAEAACsRRACAADWIggBAABrEYQAAIC1CEIAAMBaBCEAAGAtghAAALAWQQgAAFiLIAQAAKxFEAIAANYiCAEAAGsRhAAAgLUIQgAAwFoEIQAAYC2CEAAAsBZBCAAAWIsgBAAArEUQAgAA1iIIAQAAaxGEAACAtQhCAADAWgQhAABgLYIQAACwFkEIAABYiyAEAACsRRACAADWIggBAABrEYQAAIC1CEIAAMBaBCEAAGAtghAAALAWQQgAAFiLIAQAAKxFEAIAANYiCAEAAGsRhAAAgLUIQgAAwFoEIQAAYC2CEAAAsBZBCAAAWIsgBAAArEUQAgAA1iIIAQAAaxGEAACAtQhCAADAWgQhAABgLYIQAACwFkEIAABYq9qD0PPPPy8PDw8lJSU5y86cOaOEhAQFBwerbt26GjRokPLz893WO3jwoAYMGKA6deqocePGevTRR3X+/Hm3mrVr16pLly7y9fVVixYtlJaWVmH7c+bMUbNmzeTn56eoqCht2rSpOnYTAADUQtUahDZv3qw///nP6tChg9vy8ePH6x//+IeWLl2qdevW6ciRI7rnnnuc9tLSUg0YMEAlJSXauHGjFi5cqLS0NKWmpjo1+/fv14ABA3Trrbdq+/btSkpK0siRI/Xhhx86NUuWLFFycrKmTJmirVu3qmPHjoqNjdWxY8eqc7cBAEAt4WGMMdXR8enTp9WlSxe99tpreuaZZ9SpUyfNnDlThYWFatSokRYvXqzf/OY3kqScnBxFRkYqMzNT3bt316pVq3TXXXfpyJEjCgkJkSTNmzdPEydO1PHjx+Xj46OJEydq5cqV2rVrl7PNIUOGqKCgQOnp6ZKkqKgo3XTTTXr11VclSWVlZYqIiNDYsWOVkpJy2X0oKipSYGCgCgsL5XK5qnqKAKs0S1lZ00OoNQ48P6CmhwDUalfy/l1tZ4QSEhI0YMAAxcTEuC3Pzs7WuXPn3Ja3adNG119/vTIzMyVJmZmZat++vROCJCk2NlZFRUXavXu3U3Nx37GxsU4fJSUlys7Odqvx9PRUTEyMUwMAAOzmXR2dvv3229q6das2b95coS0vL08+Pj4KCgpyWx4SEqK8vDyn5sIQVN5e3vZjNUVFRfr+++918uRJlZaWXrImJyfnkuM+e/aszp496zwvKiqqxN4CAIDaqsrPCB06dEjjxo3TokWL5OfnV9XdV6tp06YpMDDQeURERNT0kAAAQDWq8iCUnZ2tY8eOqUuXLvL29pa3t7fWrVun2bNny9vbWyEhISopKVFBQYHbevn5+QoNDZUkhYaGVvgWWfnzy9W4XC75+/urYcOG8vLyumRNeR8XmzRpkgoLC53HoUOHfvI8AACAq1+VB6F+/fpp586d2r59u/Po1q2bhg4d6vz7uuuu0+rVq511cnNzdfDgQUVHR0uSoqOjtXPnTrdvd2VkZMjlcqlt27ZOzYV9lNeU9+Hj46OuXbu61ZSVlWn16tVOzcV8fX3lcrncHgAA4NpV5dcI1atXT+3atXNbFhAQoODgYGd5fHy8kpOT1aBBA7lcLo0dO1bR0dHq3r27JKl///5q27at7r//fk2fPl15eXmaPHmyEhIS5OvrK0n6wx/+oFdffVUTJkzQ73//e61Zs0bvvPOOVq78/99MSU5O1rBhw9StWzfdfPPNmjlzpoqLizVixIiq3m0AAFALVcvF0pczY8YMeXp6atCgQTp79qxiY2P12muvOe1eXl5asWKFHnroIUVHRysgIEDDhg3TU0895dQ0b95cK1eu1Pjx4zVr1iw1adJEb7zxhmJjY52awYMH6/jx40pNTVVeXp46deqk9PT0ChdQAwAAO1XbfYSuBdxHCKg63Eeo8riPEPDzXBX3EQIAALjaEYQAAIC1CEIAAMBaBCEAAGAtghAAALAWQQgAAFiLIAQAAKxFEAIAANYiCAEAAGsRhAAAgLUIQgAAwFoEIQAAYC2CEAAAsBZBCAAAWIsgBAAArEUQAgAA1iIIAQAAaxGEAACAtQhCAADAWgQhAABgLYIQAACwFkEIAABYiyAEAACsRRACAADWIggBAABrEYQAAIC1CEIAAMBaBCEAAGAtghAAALAWQQgAAFiLIAQAAKxFEAIAANYiCAEAAGsRhAAAgLUIQgAAwFreNT0A4JfWLGVlTQ+h1jjw/ICaHgIAVCvOCAEAAGsRhAAAgLUIQgAAwFoEIQAAYC2CEAAAsBZBCAAAWIsgBAAArEUQAgAA1iIIAQAAaxGEAACAtQhCAADAWgQhAABgLYIQAACwFkEIAABYiyAEAACsRRACAADWIggBAABrEYQAAIC1CEIAAMBaBCEAAGAtghAAALAWQQgAAFiLIAQAAKxV5UFo2rRpuummm1SvXj01btxYcXFxys3Ndas5c+aMEhISFBwcrLp162rQoEHKz893qzl48KAGDBigOnXqqHHjxnr00Ud1/vx5t5q1a9eqS5cu8vX1VYsWLZSWllZhPHPmzFGzZs3k5+enqKgobdq0qap3GQAA1FJVHoTWrVunhIQE/etf/1JGRobOnTun/v37q7i42KkZP368/vGPf2jp0qVat26djhw5onvuucdpLy0t1YABA1RSUqKNGzdq4cKFSktLU2pqqlOzf/9+DRgwQLfeequ2b9+upKQkjRw5Uh9++KFTs2TJEiUnJ2vKlCnaunWrOnbsqNjYWB07dqyqdxsAANRCHsYYU50bOH78uBo3bqx169apd+/eKiwsVKNGjbR48WL95je/kSTl5OQoMjJSmZmZ6t69u1atWqW77rpLR44cUUhIiCRp3rx5mjhxoo4fPy4fHx9NnDhRK1eu1K5du5xtDRkyRAUFBUpPT5ckRUVF6aabbtKrr74qSSorK1NERITGjh2rlJSUy469qKhIgYGBKiwslMvlquqpQQ1plrKypodQaxx4fkCV9cW8V15Vzjtgoyt5/672a4QKCwslSQ0aNJAkZWdn69y5c4qJiXFq2rRpo+uvv16ZmZmSpMzMTLVv394JQZIUGxuroqIi7d6926m5sI/ymvI+SkpKlJ2d7Vbj6empmJgYp+ZiZ8+eVVFRkdsDAABcu6o1CJWVlSkpKUk9e/ZUu3btJEl5eXny8fFRUFCQW21ISIjy8vKcmgtDUHl7eduP1RQVFen777/XN998o9LS0kvWlPdxsWnTpikwMNB5RERE/LQdBwAAtUK1BqGEhATt2rVLb7/9dnVupspMmjRJhYWFzuPQoUM1PSQAAFCNvKur48TERK1YsULr169XkyZNnOWhoaEqKSlRQUGB21mh/Px8hYaGOjUXf7ur/FtlF9Zc/E2z/Px8uVwu+fv7y8vLS15eXpesKe/jYr6+vvL19f1pOwwAAGqdKj8jZIxRYmKili1bpjVr1qh58+Zu7V27dtV1112n1atXO8tyc3N18OBBRUdHS5Kio6O1c+dOt293ZWRkyOVyqW3btk7NhX2U15T34ePjo65du7rVlJWVafXq1U4NAACwW5WfEUpISNDixYv13nvvqV69es71OIGBgfL391dgYKDi4+OVnJysBg0ayOVyaezYsYqOjlb37t0lSf3791fbtm11//33a/r06crLy9PkyZOVkJDgnLH5wx/+oFdffVUTJkzQ73//e61Zs0bvvPOOVq78/99MSU5O1rBhw9StWzfdfPPNmjlzpoqLizVixIiq3m0AAFALVXkQmjt3riSpb9++bssXLFig4cOHS5JmzJghT09PDRo0SGfPnlVsbKxee+01p9bLy0srVqzQQw89pOjoaAUEBGjYsGF66qmnnJrmzZtr5cqVGj9+vGbNmqUmTZrojTfeUGxsrFMzePBgHT9+XKmpqcrLy1OnTp2Unp5e4QJqAABgp2q/j1Btxn2Erk3cz6byuI9QzeA+QsDPc1XdRwgAAOBqRRACAADWIggBAABrEYQAAIC1CEIAAMBaBCEAAGAtghAAALAWQQgAAFiLIAQAAKxFEAIAANYiCAEAAGsRhAAAgLUIQgAAwFoEIQAAYC2CEAAAsBZBCAAAWIsgBAAArEUQAgAA1iIIAQAAaxGEAACAtQhCAADAWgQhAABgLYIQAACwFkEIAABYiyAEAACsRRACAADWIggBAABrEYQAAIC1CEIAAMBaBCEAAGAtghAAALAWQQgAAFiLIAQAAKxFEAIAANYiCAEAAGsRhAAAgLUIQgAAwFoEIQAAYC2CEAAAsBZBCAAAWIsgBAAArEUQAgAA1iIIAQAAaxGEAACAtQhCAADAWgQhAABgLYIQAACwFkEIAABYiyAEAACsRRACAADWIggBAABrEYQAAIC1CEIAAMBaBCEAAGAtghAAALAWQQgAAFiLIAQAAKxFEAIAANayIgjNmTNHzZo1k5+fn6KiorRp06aaHhIAALgKXPNBaMmSJUpOTtaUKVO0detWdezYUbGxsTp27FhNDw0AANQw75oeQHV7+eWXNWrUKI0YMUKSNG/ePK1cuVJ//etflZKSUsOjAwBci5qlrKzpIdQaB54fUKPbv6aDUElJibKzszVp0iRnmaenp2JiYpSZmVmh/uzZszp79qzzvLCwUJJUVFRULeNrN+XDaun3WrTrydgq66vs7HdV1te1riqPfea98qrrNQe/HI73yquO4728T2PMZWuv6SD0zTffqLS0VCEhIW7LQ0JClJOTU6F+2rRpevLJJyssj4iIqLYxonICZ9b0COzEvNcM5h02qc7j/dSpUwoMDPzRmms6CF2pSZMmKTk52XleVlamEydOKDg4WB4eHjU4sl9GUVGRIiIidOjQIblcrpoejjWY95rBvNcM5r1m2DbvxhidOnVK4eHhl629poNQw4YN5eXlpfz8fLfl+fn5Cg0NrVDv6+srX19ft2VBQUHVOcSrksvlsuIX5WrDvNcM5r1mMO81w6Z5v9yZoHLX9LfGfHx81LVrV61evdpZVlZWptWrVys6OroGRwYAAK4G1/QZIUlKTk7WsGHD1K1bN918882aOXOmiouLnW+RAQAAe13zQWjw4ME6fvy4UlNTlZeXp06dOik9Pb3CBdT44aPBKVOmVPh4ENWLea8ZzHvNYN5rBvP+n3mYyny3DAAA4Bp0TV8jBAAA8GMIQgAAwFoEIQAAYC2CEK6Ih4eHli9fXtPDsE6zZs00c+bMmh7GVetqmJ/hw4crLi6uRsdQFdauXSsPDw8VFBTU9FAkXR0/25pkjNHo0aPVoEEDeXh4aPv27Zesu9xr84EDB350fZsRhIAf0bdvXyUlJdX0MIBqwzF+dUtPT1daWppWrFiho0ePql27dpesO3r0qO64445feHTXhmv+6/MAgJpXUlIiHx+fmh5GrfPll18qLCxMPXr0uGR7+bxe6q8loHI4I1QL9O3bV2PHjlVSUpLq16+vkJAQvf76686NIevVq6cWLVpo1apVzjrr1q3TzTffLF9fX4WFhSklJUXnz5936/Phhx/WhAkT1KBBA4WGhmrq1Klu2923b5969+4tPz8/tW3bVhkZGRXGNnHiRLVq1Up16tTRDTfcoCeeeELnzp2T9MOpWE9PT23ZssVtnZkzZ6pp06YqKyurwlmqesOHD9e6des0a9YseXh4yMPDQ2lpaRX+7Mry5csr/C269957T126dJGfn59uuOEGPfnkk878G2M0depUXX/99fL19VV4eLgefvhhZ91jx45p4MCB8vf3V/PmzbVo0aIKY3v55ZfVvn17BQQEKCIiQmPGjNHp06clScXFxXK5XPr73/9eYZwBAQE6depUVUxPpa1YsUJBQUEqLS2VJG3fvl0eHh5KSUlxakaOHKnf/e53kqRPP/1UvXr1kr+/vyIiIvTwww+ruLjYqa3M/Hh4eOiNN97Q3XffrTp16qhly5Z6//333Wp27dqlO+64Q3Xr1lVISIjuv/9+ffPNN0773//+d7Vv317+/v4KDg5WTEyMM47S0lIlJycrKChIwcHBmjBhQoW/cp2enq5bbrnFqbnrrrv05ZdfOu2/+tWvlJiY6LbO8ePH5ePj43Y3/Op0qWP8wIEDkqTs7Gx169ZNderUUY8ePZSbm+u23sUfAyYlJalv377O8759+yoxMVFJSUlq2LChYmNjrTv2f67hw4dr7NixOnjwoDw8PNSsWbNLzqtU8aOxTZs2qXPnzvLz81O3bt20bds2t75LS0sVHx+v5s2by9/fX61bt9asWbOc9vXr1+u6665TXl6e23pJSUnq1atX9e10TTC46vXp08fUq1fPPP3002bv3r3m6aefNl5eXuaOO+4w8+fPN3v37jUPPfSQCQ4ONsXFxebw4cOmTp06ZsyYMWbPnj1m2bJlpmHDhmbKlClufbpcLjN16lSzd+9es3DhQuPh4WE++ugjY4wxpaWlpl27dqZfv35m+/btZt26daZz585Gklm2bJnTz9NPP202bNhg9u/fb95//30TEhJiXnjhBaf9tttuM2PGjHHbnw4dOpjU1NRqnbOqUFBQYKKjo82oUaPM0aNHzdGjR80bb7xhAgMD3eqWLVtmLvxVWr9+vXG5XCYtLc18+eWX5qOPPjLNmjUzU6dONcYYs3TpUuNyucwHH3xgvv76a5OVlWXmz5/vrH/HHXeYjh07mszMTLNlyxbTo0cP4+/vb2bMmOHUzJgxw6xZs8bs37/frF692rRu3do89NBDTvuoUaPMnXfe6TbO//mf/zEPPPBAFc5Q5RQUFBhPT0+zefNmY4wxM2fONA0bNjRRUVFOTYsWLczrr79uvvjiCxMQEGBmzJhh9u7dazZs2GA6d+5shg8f7tRWZn4kmSZNmpjFixebffv2mYcfftjUrVvXfPvtt8YYY06ePGkaNWpkJk2aZPbs2WO2bt1qbrvtNnPrrbcaY4w5cuSI8fb2Ni+//LLZv3+/+eyzz8ycOXPMqVOnjDHGvPDCC6Z+/frm3XffNZ9//rmJj4839erVM7/+9a+dMfz973837777rtm3b5/Ztm2bGThwoGnfvr0pLS01xhizaNEiU79+fXPmzBlnnZdfftk0a9bMlJWVVe0P4T+41DH+8ccfG0kmKirKrF271uzevdv06tXL9OjRw1lv2LBhbvtqjDHjxo0zffr0cZ736dPH1K1b1zz66KMmJyfH5OTkWHfs/1wFBQXmqaeeMk2aNDFHjx41x44du+S8GmPcXptPnTplGjVqZO677z6za9cu849//MPccMMNRpLZtm2bMcaYkpISk5qaajZv3my++uor89Zbb5k6deqYJUuWONtv1aqVmT59uvO8pKTENGzY0Pz1r3/9xebgl0AQqgX69OljbrnlFuf5+fPnTUBAgLn//vudZUePHjWSTGZmpnnsscdM69at3V5M58yZY+rWreu8CF/cpzHG3HTTTWbixInGGGM+/PBD4+3tbf7973877atWraoQhC724osvmq5duzrPlyxZ4vZin52dbTw8PMz+/fuvfCJqQJ8+fcy4ceOc5wsWLLhsEOrXr5957rnn3Gr+9re/mbCwMGOMMS+99JJp1aqVKSkpqbC93NxcI8ls2rTJWbZnzx4jye3N4GJLly41wcHBzvOsrCzj5eVljhw5YowxJj8/33h7e5u1a9dedp+rQ5cuXcyLL75ojDEmLi7OPPvss8bHx8ecOnXKHD582Egye/fuNfHx8Wb06NFu637yySfG09PTfP/995WeH0lm8uTJzvPTp08bSWbVqlXGmB8CfP/+/d22c+jQISPJ5ObmmuzsbCPJHDhw4JL7ExYW5vYGce7cOdOkSZMK4eBCx48fN5LMzp07jTHGfP/996Z+/fpubzwdOnRwAvMv5eJj/J///KeRZD7++GNn2cqVK40k8/333xtjKh+EOnfu7FZj47H/c82YMcM0bdrUeX6peTXGPQj9+c9/NsHBwc7Pyxhj5s6d6xaELiUhIcEMGjTIef7CCy+YyMhI5/m7775r6tata06fPv3Td+gqxEdjtUSHDh2cf3t5eSk4OFjt27d3lpX/yZBjx45pz549io6Odvu4pmfPnjp9+rQOHz58yT4lKSwsTMeOHZMk7dmzRxEREQoPD3faL/WHapcsWaKePXsqNDRUdevW1eTJk3Xw4EGnPS4uTl5eXlq2bJkkKS0tTbfeequaNWv2U6ahVtixY4eeeuop1a1b13mMGjVKR48e1Xfffaff/va3+v7773XDDTdo1KhRWrZsmfOx2Z49e+Tt7a2uXbs6/bVp06bCx3Eff/yx+vXrp//6r/9SvXr1dP/99+vbb7/Vd999J0m6+eab9d///d9auHChJOmtt95S06ZN1bt3719mEi7Sp08frV27VsYYffLJJ7rnnnsUGRmpTz/9VOvWrVN4eLhatmypHTt2KC0tzW3uYmNjVVZWpv3791d6fiT34zsgIEAul8s5vnfs2KF//vOfbttp06aNpB+uyejYsaP69eun9u3b67e//a1ef/11nTx5UpJUWFioo0ePKioqyunf29tb3bp1c9v+vn37dO+99+qGG26Qy+Vyjvny3w8/Pz/df//9+utf/ypJ2rp1q3bt2qXhw4f/vMmuIhfOX1hYmCQ581dZF/6cJFl57FeHi+f1Ynv27FGHDh3k5+fnLLvU6/ecOXPUtWtXNWrUSHXr1tX8+fPdXr+HDx+uL774Qv/6178k/fD6/b//+78KCAiooj25OhCEaonrrrvO7bmHh4fbsvLQcyXX3VyqzytZPzMzU0OHDtWdd96pFStWaNu2bXr88cdVUlLi1Pj4+OiBBx7QggULVFJSosWLF+v3v/99pbdxtfH09KxwLUj5NVHlTp8+rSeffFLbt293Hjt37tS+ffvk5+eniIgI5ebm6rXXXpO/v7/GjBmj3r17V+jnPzlw4IDuuusudejQQe+++66ys7M1Z84cSXKb+5EjRyotLU2StGDBAo0YMaLCtUy/lL59++rTTz/Vjh07dN1116lNmzbq27ev1q5dq3Xr1qlPnz6Sfpi7Bx980G3uduzYoX379unGG2+8om3+2PF9+vRpDRw40G0727dvd66L8/LyUkZGhlatWqW2bdvqlVdeUevWrbV///5Kb3/gwIE6ceKEXn/9dWVlZSkrK0tSxZ9RRkaGDh8+rAULFuhXv/qVmjZtekX7WV1+7PWlMr8Hkiq8Ydp47FeHqggib7/9th555BHFx8fro48+0vbt2zVixAi3eWzcuLEGDhyoBQsWKD8/X6tWrarVr9//CUHoGhQZGanMzEy3F6oNGzaoXr16atKkSaX7OHTokI4ePeosK/9fQbmNGzeqadOmevzxx9WtWze1bNlSX3/9dYW+Ro4cqY8//livvfaazp8/r3vuuecn7tkvz8fHx7nIV5IaNWqkU6dOuV28e/F9Obp06aLc3Fy1aNGiwsPT84dfOX9/fw0cOFCzZ8/W2rVrlZmZqZ07d6pNmzY6f/68srOznf5yc3Pd7umSnZ2tsrIyvfTSS+revbtatWqlI0eOVBj77373O3399deaPXu2Pv/8cw0bNqyKZuXK9erVS6dOndKMGTOc0FMehNauXetcZNulSxd9/vnnl5w7Hx+fSs1PZXTp0kW7d+9Ws2bNKmyn/E3Gw8NDPXv21JNPPqlt27bJx8dHy5YtU2BgoMLCwpxgI6nCmL799lvl5uZq8uTJ6tevnyIjI50zShdq3769unXrptdff73G/pNw8TFeGY0aNXJ7bZAq/h78J7Yd+zUhMjJSn332mc6cOeMsu/j1e8OGDerRo4fGjBmjzp07q0WLFm4X85cbOXKklixZovnz5+vGG29Uz549q338vzSC0DVozJgxOnTokMaOHaucnBy99957mjJlipKTk5034suJiYlRq1atNGzYMO3YsUOffPKJHn/8cbeali1b6uDBg3r77bf15Zdfavbs2c5HYBeKjIxU9+7dNXHiRN17773y9/evkv38JTRr1kxZWVk6cOCAvvnmG0VFRalOnTp67LHH9OWXX2rx4sXO/zzLpaam6s0339STTz6p3bt3a8+ePXr77bc1efJkST+cXv7LX/6iXbt26auvvtJbb70lf39/NW3aVK1bt9btt9+uBx98UFlZWcrOztbIkSPd5qxFixY6d+6cXnnlFX311Vf629/+pnnz5lUYe/369XXPPffo0UcfVf/+/SsdgqtD/fr11aFDBy1atMgJPb1799bWrVu1d+9eJxxNnDhRGzduVGJionOG5r333nO+XVWZ+amMhIQEnThxQvfee682b96sL7/8Uh9++KFGjBih0tJSZWVl6bnnntOWLVt08OBB/d///Z+OHz+uyMhISdK4ceP0/PPPa/ny5crJydGYMWPc3rDr16+v4OBgzZ8/X1988YXWrFmj5OTkS45l5MiRev7552WM0d13332FM/vzXXyMV+as8K9+9Stt2bJFb775pvbt26cpU6Zo165dl13PxmO/Jtx3333y8PDQqFGj9Pnnn+uDDz7Qn/70J7eali1basuWLfrwww+1d+9ePfHEE9q8eXOFvmJjY+VyufTMM89oxIgRv9Qu/LJq9AolVMrFFzMaY0zTpk0rXECoCy6WW7t2rbnpppuMj4+PCQ0NNRMnTjTnzp370T5//etfm2HDhjnPc3NzzS233GJ8fHxMq1atTHp6eoWLpR999FETHBxs6tatawYPHmxmzJhR4WJiY4z5y1/+UuFCyNogNzfXdO/e3fj7+xtJZv/+/WbZsmWmRYsWxt/f39x1111m/vz55uJfpfT0dOcbLy6Xy9x8883Ot2OWLVtmoqKijMvlMgEBAaZ79+5uF6YePXrUDBgwwPj6+prrr7/evPnmmxV+3i+//LIJCwsz/v7+JjY21rz55ptGkjl58qTbOFavXm0kmXfeeafa5qiyxo0bZySZPXv2OMs6duxoQkND3eo2bdpkbrvtNlO3bl0TEBBgOnToYJ599lmnvTLzc/FxaowxgYGBZsGCBc7zvXv3mrvvvtsEBQUZf39/06ZNG5OUlGTKysrM559/bmJjY02jRo2Mr6+vadWqlXnllVecdc+dO2fGjRtnXC6XCQoKMsnJyeaBBx5wu4A4IyPDREZGGl9fX9OhQwezdu3aS47r1KlTzrc8a8LFx/iCBQsqHEvbtm1zjv9yqampJiQkxAQGBprx48ebxMTEChdLX/waY+ux/3Nc6mLpi+fVmIrHfGZmpunYsaPx8fExnTp1Mu+++67bxdJnzpwxw4cPN4GBgSYoKMg89NBDJiUlxXTs2LFC30888YTbBejXGg9jLvqgF6gGTz/9tJYuXarPPvuspodilb/97W8aP368jhw5ws3srlIHDhzQjTfeqM2bN6tLly41PZxrBsd+1YmPj9fx48cr3IvrWsGdpVGtTp8+rQMHDujVV1/VM888U9PDscZ3332no0eP6vnnn9eDDz7IG8FV6Ny5c/r22281efJkde/enRBURTj2q05hYaF27typxYsXX7MhSOIaIVSzxMREde3aVX379r0mv21wtZo+fbratGmj0NBQTZo0qaaHg0vYsGGDwsLCtHnz5kte54KfhmO/6vz6179W//799Yc//EG33XZbTQ+n2vDRGAAAsBZnhAAAgLUIQgAAwFoEIQAAYC2CEAAAsBZBCAAAWIsgBAAArEUQAgAA1iIIAQAAaxGEAACAtf4fPQyC5FFrqpsAAAAASUVORK5CYII="
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['x32'].value_counts()"
      ],
      "metadata": {
        "id": "WErTHaJ70Ia2",
        "outputId": "25c81c71-6042-49a7-8df4-08ed5e9a6a25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "x32\n",
              "0.01%     40767\n",
              "-0.01%    34094\n",
              "0.0%      33923\n",
              "-0.0%     30492\n",
              "-0.02%     9924\n",
              "0.02%      7987\n",
              "-0.03%     1727\n",
              "0.03%       855\n",
              "-0.04%      138\n",
              "0.04%        55\n",
              "-0.05%        6\n",
              "0.05%         1\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['x37'].value_counts()\n"
      ],
      "metadata": {
        "id": "r98Zehw50LS1",
        "outputId": "b800fb0d-168c-4b48-ac57-c40bfc5b6943",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "x37\n",
              "$-311.26     6\n",
              "$-336.77     6\n",
              "$237.4       6\n",
              "$72.42       6\n",
              "$341.26      6\n",
              "            ..\n",
              "$-505.21     1\n",
              "$770.07      1\n",
              "$74.62       1\n",
              "$-1082.96    1\n",
              "$-1229.34    1\n",
              "Name: count, Length: 129198, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### x32 and x37 are actually numeric with string format (even though x32 has few \"levels\" or possible values)\n",
        "### x32 is in percentage, but don't think it needs to be transformed to proportion since data will be scaled later (will double check, but should make no difference)\n",
        "\n",
        "df['x32'] = df['x32'].transform(lambda x: float(str(x).replace('%','')))\n",
        "df['x37'] = df['x37'].transform(lambda x: float(str(x).replace('$','')))"
      ],
      "metadata": {
        "id": "-8HSIli70VDX"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#all significanlty less than 5%, should be no issue imputing,\n",
        "#imputing categorical features with modes\n",
        "# imputing continent with mode, then dates by mode grouped by continent\n",
        "\n",
        "print('imputing continent with mode: {}'.format(df['x24'].aggregate(pd.Series.mode)))\n",
        "df['x24'] = df['x24'].transform(lambda x: x.fillna(pd.Series.mode(x)[0]))\n",
        "\n",
        "print('imputing month with mode by cont: {}'.format(df[['x24','x29']].groupby(by='x24').aggregate(pd.Series.mode)))\n",
        "idx_29 = df.loc[df['x29'].isna(),:].index #saving NAs indices to quick check what they've been imputed with\n",
        "df['x29'] = df[['x24','x29']].groupby(by='x24').transform(lambda grp: grp.fillna(pd.Series.mode(grp)[0]))\n",
        "\n",
        "# will fix index showing on print later\n"
      ],
      "metadata": {
        "id": "qyAc5R1u209T",
        "outputId": "5b268328-fe3b-4c2b-fa23-1fcea02f8fab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "imputing continent with mode: 0    asia\n",
            "Name: x24, dtype: object\n",
            "imputing month with mode by cont:          x29\n",
            "x24         \n",
            "america  Jul\n",
            "asia     Jul\n",
            "euorpe   Jul\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[['x24','x29','x30']].groupby(by=['x24','x29']).aggregate(pd.Series.mode)\n",
        "# could just impute with wednesday since Wed is mode per month, but adding continent gives more context and different days for some months"
      ],
      "metadata": {
        "id": "2iV02zDI3Emu",
        "outputId": "282e5310-eccd-43ee-d0fa-1c0c159bf62b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   x30\n",
              "x24     x29           \n",
              "america Apr  wednesday\n",
              "        Aug  wednesday\n",
              "        Feb    tuesday\n",
              "        Jul  wednesday\n",
              "        Jun  wednesday\n",
              "        Mar  wednesday\n",
              "        May  wednesday\n",
              "        Nov  wednesday\n",
              "        Oct  wednesday\n",
              "        Sep  wednesday\n",
              "asia    Apr  wednesday\n",
              "        Aug  wednesday\n",
              "        Dec  wednesday\n",
              "        Feb  wednesday\n",
              "        Jan  wednesday\n",
              "        Jul  wednesday\n",
              "        Jun  wednesday\n",
              "        Mar  wednesday\n",
              "        May  wednesday\n",
              "        Nov  wednesday\n",
              "        Oct  wednesday\n",
              "        Sep  wednesday\n",
              "euorpe  Apr  wednesday\n",
              "        Aug  wednesday\n",
              "        Dec  wednesday\n",
              "        Feb    tuesday\n",
              "        Jul  wednesday\n",
              "        Jun  wednesday\n",
              "        Mar  wednesday\n",
              "        May  wednesday\n",
              "        Nov  wednesday\n",
              "        Oct  wednesday\n",
              "        Sep  wednesday"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>x30</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x24</th>\n",
              "      <th>x29</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"10\" valign=\"top\">america</th>\n",
              "      <th>Apr</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Aug</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Feb</th>\n",
              "      <td>tuesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Jul</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Jun</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Mar</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>May</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Nov</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Oct</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Sep</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"12\" valign=\"top\">asia</th>\n",
              "      <th>Apr</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Aug</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dec</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Feb</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Jan</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Jul</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Jun</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Mar</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>May</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Nov</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Oct</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Sep</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"11\" valign=\"top\">euorpe</th>\n",
              "      <th>Apr</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Aug</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dec</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Feb</th>\n",
              "      <td>tuesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Jul</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Jun</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Mar</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>May</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Nov</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Oct</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Sep</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# impute day\n",
        "df['x30'] = df[['x24','x29','x30']].groupby(by=['x24','x29']).transform(lambda grp: grp.fillna(pd.Series.mode(grp)[0]))"
      ],
      "metadata": {
        "id": "HVQavir8IV9m"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.loc[:,df.columns != 'y']\n",
        "y = df['y'].values.flatten()\n",
        "\n",
        "df_num = X.select_dtypes(exclude='object')\n",
        "num_cols = df_num.columns\n",
        "\n",
        "# impute numeric with median\n",
        "for i in num_cols:\n",
        "    df_num[i] = df_num[i].transform(lambda x: x.fillna(pd.Series.mode(x)[0]))\n",
        "df_num.info()\n",
        "\n",
        "df_cat = X.select_dtypes(include='object')\n",
        "cat_cols = df_cat.columns\n",
        "\n",
        "# scale numerical data\n",
        "scaler = StandardScaler()\n",
        "df_num = scaler.fit_transform(df_num)\n",
        "\n",
        "#one hot encode cat data\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "df_cat = encoder.fit_transform(df_cat)"
      ],
      "metadata": {
        "id": "w8Uy9xG3Iern",
        "outputId": "013b2cef-16ec-4e53-f70c-471ccc4c70ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 160000 entries, 0 to 159999\n",
            "Data columns (total 47 columns):\n",
            " #   Column  Non-Null Count   Dtype  \n",
            "---  ------  --------------   -----  \n",
            " 0   x0      160000 non-null  float64\n",
            " 1   x1      160000 non-null  float64\n",
            " 2   x2      160000 non-null  float64\n",
            " 3   x3      160000 non-null  float64\n",
            " 4   x4      160000 non-null  float64\n",
            " 5   x5      160000 non-null  float64\n",
            " 6   x6      160000 non-null  float64\n",
            " 7   x7      160000 non-null  float64\n",
            " 8   x8      160000 non-null  float64\n",
            " 9   x9      160000 non-null  float64\n",
            " 10  x10     160000 non-null  float64\n",
            " 11  x11     160000 non-null  float64\n",
            " 12  x12     160000 non-null  float64\n",
            " 13  x13     160000 non-null  float64\n",
            " 14  x14     160000 non-null  float64\n",
            " 15  x15     160000 non-null  float64\n",
            " 16  x16     160000 non-null  float64\n",
            " 17  x17     160000 non-null  float64\n",
            " 18  x18     160000 non-null  float64\n",
            " 19  x19     160000 non-null  float64\n",
            " 20  x20     160000 non-null  float64\n",
            " 21  x21     160000 non-null  float64\n",
            " 22  x22     160000 non-null  float64\n",
            " 23  x23     160000 non-null  float64\n",
            " 24  x25     160000 non-null  float64\n",
            " 25  x26     160000 non-null  float64\n",
            " 26  x27     160000 non-null  float64\n",
            " 27  x28     160000 non-null  float64\n",
            " 28  x31     160000 non-null  float64\n",
            " 29  x32     160000 non-null  float64\n",
            " 30  x33     160000 non-null  float64\n",
            " 31  x34     160000 non-null  float64\n",
            " 32  x35     160000 non-null  float64\n",
            " 33  x36     160000 non-null  float64\n",
            " 34  x37     160000 non-null  float64\n",
            " 35  x38     160000 non-null  float64\n",
            " 36  x39     160000 non-null  float64\n",
            " 37  x40     160000 non-null  float64\n",
            " 38  x41     160000 non-null  float64\n",
            " 39  x42     160000 non-null  float64\n",
            " 40  x43     160000 non-null  float64\n",
            " 41  x44     160000 non-null  float64\n",
            " 42  x45     160000 non-null  float64\n",
            " 43  x46     160000 non-null  float64\n",
            " 44  x47     160000 non-null  float64\n",
            " 45  x48     160000 non-null  float64\n",
            " 46  x49     160000 non-null  float64\n",
            "dtypes: float64(47)\n",
            "memory usage: 57.4 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.hstack([df_num,df_cat])"
      ],
      "metadata": {
        "id": "R4oaw8CgIkXV"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['y'].value_counts() # not balanced"
      ],
      "metadata": {
        "id": "xbzdACUdImVy",
        "outputId": "2af4c1db-c53e-4562-f7c1-7fac18a5478d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "y\n",
              "0    95803\n",
              "1    64197\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#add y plot"
      ],
      "metadata": {
        "id": "vUXTcWUZIoot"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## define cost function\n",
        "def cost_func(y_pred,y_true):\n",
        "    diff = y_pred - y_true\n",
        "    x = 0\n",
        "    for i in diff:\n",
        "        #false pos (y_pred -y_true == 1)\n",
        "        if i == 1:\n",
        "            x += 100\n",
        "        #false neg (y_pred -y_true ==-1)\n",
        "        elif i == -1:\n",
        "            x += 150\n",
        "    return(x)"
      ],
      "metadata": {
        "id": "HRJz__nu_bxm"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "skf = StratifiedKFold(n_splits=5,random_state=807,shuffle=True)\n",
        "skf.get_n_splits(X,y)"
      ],
      "metadata": {
        "id": "qIgBnSnHIqjU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d23b095a-0acc-491f-b4c4-e145f5393188"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression"
      ],
      "metadata": {
        "id": "gpPJ9Gb95HXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logR = LogisticRegression(random_state=807,penalty='elasticnet',class_weight='balanced',solver='saga')\n",
        "params = {'C':[0.00001,0.0001,0.001,0.01,0.1,1,2,5,10],'l1_ratio':[0,0.001,0.01,0.1,0.25,0.5,0.75,0.9,0.99,1],'max_iter':[25,50,75]}\n",
        "\n",
        "lr_clf = GridSearchCV(estimator=logR,param_grid=params,n_jobs=-1,cv=skf)\n",
        "lr_clf.fit(X,y)"
      ],
      "metadata": {
        "id": "AcpquSzJO6SF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_params = lr_clf.best_params_\n",
        "lr_params"
      ],
      "metadata": {
        "id": "Eb5e2xOvO8pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "{'C': 0.001, 'l1_ratio': 0.5, 'max_iter': 50}"
      ],
      "metadata": {
        "id": "gfllGKF4pUMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = LogisticRegression(random_state=807,penalty='elasticnet',class_weight='balanced',solver='saga')\n",
        "model1.set_params(**lr_params)\n",
        "model1.fit(X,y)\n",
        "\n",
        "preds_m1 = cross_val_predict(model1,X,y,cv=skf)"
      ],
      "metadata": {
        "id": "RHUkzlw2PATu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cr_m1 = classification_report(y,preds_m1,output_dict=True)\n",
        "cr_m1\n"
      ],
      "metadata": {
        "id": "3katCHeBPQB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ConfusionMatrixDisplay.from_predictions(y,preds_m1)\n"
      ],
      "metadata": {
        "id": "ent8XHdUPayB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cost_m1 = cost_func(preds_m1,y)\n",
        "cost_m1"
      ],
      "metadata": {
        "id": "zawbBlwk_fbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest"
      ],
      "metadata": {
        "id": "gWPF0EgN5Mw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier(random_state=807,criterion='gini',class_weight='balanced')\n",
        "params = {'n_estimators':[200,250,500],'max_features':['sqrt','log2',20]}\n",
        "rf_clf = GridSearchCV(rf,param_grid=params,cv=skf,n_jobs = -1)"
      ],
      "metadata": {
        "id": "4HWotr_VPhhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_clf.fit(X,y)"
      ],
      "metadata": {
        "id": "soQp5nQ8PkUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_params = rf_clf.best_params_\n",
        "rf_params"
      ],
      "metadata": {
        "id": "Hz6owkMTPlOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = RandomForestClassifier(random_state=807,criterion='gini',class_weight='balanced')\n",
        "model2.set_params(**rf_params)\n",
        "preds_m2 = cross_val_predict(model2,X,y,cv=skf)"
      ],
      "metadata": {
        "id": "hvIbks5bPnfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cr_m2 = classification_report(y,preds_m2,output_dict=True)\n",
        "cr_m2"
      ],
      "metadata": {
        "id": "Aaw8aFI7PpCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ConfusionMatrixDisplay.from_predictions(y,preds_m2)"
      ],
      "metadata": {
        "id": "BK6zSOo4Pq_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cost_m2 = cost_func(preds_m2,y)\n",
        "cost_m2\n"
      ],
      "metadata": {
        "id": "XqmhfmAx9Ge2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SVC"
      ],
      "metadata": {
        "id": "_6cqxOG75RGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svc = NuSVC(random_state=807,class_weight='balanced',cache_size = 8192)\n",
        "params = {'nu':[0.0001,0.01, 0.1, 0.25, 0.5, 0.75, 0.99,1],'kernel':['linear','rbf','sigmoid']}\n",
        "svc_clf = GridSearchCV(svc,param_grid=params,cv=skf,n_jobs=-1)"
      ],
      "metadata": {
        "id": "zYBX6GGsPwEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svc_clf.fit(X,y)"
      ],
      "metadata": {
        "id": "EjZRywoCP6hm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svc_params = svc_clf.best_params_\n",
        "svc_params"
      ],
      "metadata": {
        "id": "yYiNL9AlP98A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3 = NuSVC(random_state=807,class_weight='balanced',cache_size = 2048)\n",
        "model3.set_params(**svc_params)\n",
        "preds_m3 = cross_val_predict(model3,X,y,cv=skf)"
      ],
      "metadata": {
        "id": "_58pwDIoQGHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cr_m3 = classification_report(y,preds_m3,output_dict=True)\n",
        "cr_m3"
      ],
      "metadata": {
        "id": "5AVAi2z8QPOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ConfusionMatrixDisplay.from_predictions(y,preds_m3)"
      ],
      "metadata": {
        "id": "j5nGHzRUQUOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cost_m3 = cost_func(preds_m3,y)\n",
        "cost_m3"
      ],
      "metadata": {
        "id": "sefhH4BT-Rd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "8946dca4-81fe-4bfb-be64-a384b53f9eb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cost_m3 \u001b[38;5;241m=\u001b[39m cost(\u001b[43mpreds_m3\u001b[49m,y)\n\u001b[1;32m      2\u001b[0m cost_m3\n",
            "\u001b[0;31mNameError\u001b[0m: name 'preds_m3' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Neural Network"
      ],
      "metadata": {
        "id": "qA5qTJxP5amW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#sets binary values for predictions\n",
        "def cost(y_true,y_pred):\n",
        "\n",
        "  bin_p = K.switch(K.greater_equal(y_pred,0.5),K.constant(1,shape=y_pred.shape),\n",
        "                   K.constant(0,shape=y_pred.shape))\n",
        "  diff = bin_p-y_true\n",
        "\n",
        "  error = K.switch(\n",
        "      K.equal(diff,1),K.constant(100,shape=y_pred.shape),\n",
        "      K.switch(\n",
        "          K.equal(diff,-1),K.constant(150,shape=y_pred.shape),\n",
        "          K.constant(0,shape=y_pred.shape)\n",
        "      )\n",
        "  )\n",
        "  return(K.sum(error))"
      ],
      "metadata": {
        "id": "z2vUglkTkzxm"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "now = datetime.datetime.now"
      ],
      "metadata": {
        "id": "b04x7Mf5dBDd"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "es = EarlyStopping(monitor='val_cost', mode='min',patience=10,restore_best_weights=True,start_from_epoch=40)\n",
        "fold_results = {}\n",
        "t_tot = now()\n",
        "for i, (train_index, test_index) in enumerate(skf.split(X,y)):\n",
        "  # building model within loop to reset weights each time\n",
        "  tf.keras.backend.clear_session()\n",
        "  tf.keras.utils.set_random_seed(807)\n",
        "  model4 = tf.keras.Sequential()\n",
        "  model4.add(tf.keras.Input(shape=(X.shape[1],)))\n",
        "  model4.add(tf.keras.layers.Dense(64, activation='relu',name=\"Relu1\"))\n",
        "  model4.add(tf.keras.layers.Dense(64, activation='relu', name=\"Relu2\"))\n",
        "  model4.add(tf.keras.layers.Dropout(0.4))\n",
        "  model4.add(tf.keras.layers.Dense(1, activation='sigmoid', name=\"Activation\"))\n",
        "  model4.compile(optimizer='adam',\n",
        "                loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "                metrics = [tf.keras.metrics.AUC(from_logits=False), 'accuracy',cost])\n",
        "  t_fold = now()\n",
        "  print('x_train shape:', X[train_index].shape)\n",
        "  model4.fit(X[train_index],y[train_index],epochs=100,batch_size=64,validation_split=0.2,callbacks=[es])\n",
        "  fold_results.update({i:{'predictions':model4.predict(X[test_index]).flatten(),'index':test_index,'y_true':y[test_index]}})\n",
        "  print('fold train/predict time: %s' % (now()-t_fold))\n",
        "print('total train/predict time: %s' % (now()-t_tot))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEjPIqNtVPF1",
        "outputId": "516fc5ea-c1e1-4b61-c20e-d1f8568c9078"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-07-30 03:51:28.251440: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-07-30 03:51:28.257785: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-07-30 03:51:28.258232: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (128000, 67)\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-07-30 03:51:28.259947: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-07-30 03:51:28.260394: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-07-30 03:51:28.260783: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-07-30 03:51:28.854240: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-07-30 03:51:28.854579: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-07-30 03:51:28.854832: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-07-30 03:51:28.855103: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3326 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050 Ti with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
            "2023-07-30 03:51:29.996372: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7f620f366190 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2023-07-30 03:51:29.996395: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce GTX 1050 Ti with Max-Q Design, Compute Capability 6.1\n",
            "2023-07-30 03:51:29.999885: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "2023-07-30 03:51:30.153608: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
            "2023-07-30 03:51:30.294362: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1600/1600 [==============================] - 7s 3ms/step - loss: 0.3797 - auc: 0.9067 - accuracy: 0.8289 - cost: 1409.5000 - val_loss: 0.2840 - val_auc: 0.9493 - val_accuracy: 0.8816 - val_cost: 975.0000\n",
            "Epoch 2/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.2641 - auc: 0.9560 - accuracy: 0.8947 - cost: 853.5938 - val_loss: 0.2302 - val_auc: 0.9665 - val_accuracy: 0.9097 - val_cost: 721.0000\n",
            "Epoch 3/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.2245 - auc: 0.9681 - accuracy: 0.9131 - cost: 704.6250 - val_loss: 0.1985 - val_auc: 0.9749 - val_accuracy: 0.9253 - val_cost: 602.2500\n",
            "Epoch 4/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1992 - auc: 0.9748 - accuracy: 0.9264 - cost: 597.9375 - val_loss: 0.1783 - val_auc: 0.9797 - val_accuracy: 0.9354 - val_cost: 520.2500\n",
            "Epoch 5/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1821 - auc: 0.9786 - accuracy: 0.9347 - cost: 531.9375 - val_loss: 0.1677 - val_auc: 0.9815 - val_accuracy: 0.9396 - val_cost: 484.7500\n",
            "Epoch 6/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1707 - auc: 0.9811 - accuracy: 0.9387 - cost: 499.3750 - val_loss: 0.1584 - val_auc: 0.9834 - val_accuracy: 0.9459 - val_cost: 434.0000\n",
            "Epoch 7/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1626 - auc: 0.9826 - accuracy: 0.9421 - cost: 470.4062 - val_loss: 0.1559 - val_auc: 0.9840 - val_accuracy: 0.9445 - val_cost: 456.0000\n",
            "Epoch 8/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1571 - auc: 0.9836 - accuracy: 0.9443 - cost: 453.0625 - val_loss: 0.1504 - val_auc: 0.9849 - val_accuracy: 0.9483 - val_cost: 430.5000\n",
            "Epoch 9/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1515 - auc: 0.9847 - accuracy: 0.9477 - cost: 427.0312 - val_loss: 0.1474 - val_auc: 0.9854 - val_accuracy: 0.9500 - val_cost: 396.6250\n",
            "Epoch 10/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1463 - auc: 0.9856 - accuracy: 0.9505 - cost: 403.9688 - val_loss: 0.1458 - val_auc: 0.9858 - val_accuracy: 0.9516 - val_cost: 393.7500\n",
            "Epoch 11/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1437 - auc: 0.9861 - accuracy: 0.9505 - cost: 404.4062 - val_loss: 0.1390 - val_auc: 0.9866 - val_accuracy: 0.9534 - val_cost: 385.5000\n",
            "Epoch 12/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1404 - auc: 0.9866 - accuracy: 0.9524 - cost: 389.1875 - val_loss: 0.1384 - val_auc: 0.9870 - val_accuracy: 0.9541 - val_cost: 382.8750\n",
            "Epoch 13/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1375 - auc: 0.9872 - accuracy: 0.9542 - cost: 375.0625 - val_loss: 0.1403 - val_auc: 0.9864 - val_accuracy: 0.9534 - val_cost: 378.0000\n",
            "Epoch 14/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1329 - auc: 0.9878 - accuracy: 0.9560 - cost: 361.9062 - val_loss: 0.1329 - val_auc: 0.9875 - val_accuracy: 0.9573 - val_cost: 345.2500\n",
            "Epoch 15/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1317 - auc: 0.9879 - accuracy: 0.9560 - cost: 361.1875 - val_loss: 0.1343 - val_auc: 0.9876 - val_accuracy: 0.9575 - val_cost: 335.8750\n",
            "Epoch 16/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1280 - auc: 0.9885 - accuracy: 0.9573 - cost: 350.0000 - val_loss: 0.1293 - val_auc: 0.9882 - val_accuracy: 0.9588 - val_cost: 340.1250\n",
            "Epoch 17/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1259 - auc: 0.9888 - accuracy: 0.9585 - cost: 341.8438 - val_loss: 0.1286 - val_auc: 0.9883 - val_accuracy: 0.9589 - val_cost: 339.2500\n",
            "Epoch 18/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1252 - auc: 0.9888 - accuracy: 0.9593 - cost: 333.7500 - val_loss: 0.1303 - val_auc: 0.9880 - val_accuracy: 0.9586 - val_cost: 326.7500\n",
            "Epoch 19/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1231 - auc: 0.9891 - accuracy: 0.9593 - cost: 334.6875 - val_loss: 0.1264 - val_auc: 0.9884 - val_accuracy: 0.9621 - val_cost: 306.7500\n",
            "Epoch 20/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1196 - auc: 0.9896 - accuracy: 0.9602 - cost: 326.6562 - val_loss: 0.1233 - val_auc: 0.9890 - val_accuracy: 0.9633 - val_cost: 294.2500\n",
            "Epoch 21/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1199 - auc: 0.9896 - accuracy: 0.9612 - cost: 318.3438 - val_loss: 0.1235 - val_auc: 0.9889 - val_accuracy: 0.9625 - val_cost: 301.1250\n",
            "Epoch 22/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1173 - auc: 0.9900 - accuracy: 0.9616 - cost: 316.0000 - val_loss: 0.1245 - val_auc: 0.9888 - val_accuracy: 0.9623 - val_cost: 301.1250\n",
            "Epoch 23/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1154 - auc: 0.9902 - accuracy: 0.9625 - cost: 308.1250 - val_loss: 0.1211 - val_auc: 0.9893 - val_accuracy: 0.9634 - val_cost: 291.7500\n",
            "Epoch 24/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1125 - auc: 0.9906 - accuracy: 0.9636 - cost: 299.8438 - val_loss: 0.1196 - val_auc: 0.9893 - val_accuracy: 0.9629 - val_cost: 302.7500\n",
            "Epoch 25/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1121 - auc: 0.9907 - accuracy: 0.9637 - cost: 299.8438 - val_loss: 0.1187 - val_auc: 0.9894 - val_accuracy: 0.9639 - val_cost: 291.5000\n",
            "Epoch 26/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1118 - auc: 0.9905 - accuracy: 0.9642 - cost: 295.1875 - val_loss: 0.1195 - val_auc: 0.9896 - val_accuracy: 0.9638 - val_cost: 288.6250\n",
            "Epoch 27/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1096 - auc: 0.9908 - accuracy: 0.9652 - cost: 286.7500 - val_loss: 0.1180 - val_auc: 0.9897 - val_accuracy: 0.9645 - val_cost: 294.7500\n",
            "Epoch 28/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1093 - auc: 0.9910 - accuracy: 0.9658 - cost: 282.1875 - val_loss: 0.1168 - val_auc: 0.9898 - val_accuracy: 0.9651 - val_cost: 282.7500\n",
            "Epoch 29/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1076 - auc: 0.9911 - accuracy: 0.9656 - cost: 282.7500 - val_loss: 0.1166 - val_auc: 0.9896 - val_accuracy: 0.9657 - val_cost: 273.7500\n",
            "Epoch 30/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1067 - auc: 0.9913 - accuracy: 0.9661 - cost: 279.4688 - val_loss: 0.1180 - val_auc: 0.9895 - val_accuracy: 0.9650 - val_cost: 292.3750\n",
            "Epoch 31/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1061 - auc: 0.9914 - accuracy: 0.9660 - cost: 280.9062 - val_loss: 0.1167 - val_auc: 0.9896 - val_accuracy: 0.9665 - val_cost: 279.2500\n",
            "Epoch 32/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1043 - auc: 0.9916 - accuracy: 0.9675 - cost: 268.0938 - val_loss: 0.1159 - val_auc: 0.9898 - val_accuracy: 0.9652 - val_cost: 291.5000\n",
            "Epoch 33/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1039 - auc: 0.9917 - accuracy: 0.9674 - cost: 268.3438 - val_loss: 0.1164 - val_auc: 0.9894 - val_accuracy: 0.9662 - val_cost: 272.2500\n",
            "Epoch 34/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1031 - auc: 0.9916 - accuracy: 0.9677 - cost: 267.5625 - val_loss: 0.1173 - val_auc: 0.9897 - val_accuracy: 0.9656 - val_cost: 277.0000\n",
            "Epoch 35/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1011 - auc: 0.9920 - accuracy: 0.9687 - cost: 257.6562 - val_loss: 0.1149 - val_auc: 0.9895 - val_accuracy: 0.9665 - val_cost: 270.7500\n",
            "Epoch 36/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1018 - auc: 0.9919 - accuracy: 0.9681 - cost: 263.1875 - val_loss: 0.1153 - val_auc: 0.9899 - val_accuracy: 0.9655 - val_cost: 278.2500\n",
            "Epoch 37/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1008 - auc: 0.9919 - accuracy: 0.9687 - cost: 259.0312 - val_loss: 0.1135 - val_auc: 0.9901 - val_accuracy: 0.9679 - val_cost: 260.8750\n",
            "Epoch 38/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1003 - auc: 0.9921 - accuracy: 0.9690 - cost: 256.2812 - val_loss: 0.1150 - val_auc: 0.9898 - val_accuracy: 0.9666 - val_cost: 265.0000\n",
            "Epoch 39/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0984 - auc: 0.9923 - accuracy: 0.9692 - cost: 254.3750 - val_loss: 0.1140 - val_auc: 0.9896 - val_accuracy: 0.9673 - val_cost: 267.7500\n",
            "Epoch 40/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0990 - auc: 0.9921 - accuracy: 0.9694 - cost: 252.6562 - val_loss: 0.1193 - val_auc: 0.9896 - val_accuracy: 0.9652 - val_cost: 268.7500\n",
            "Epoch 41/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0989 - auc: 0.9922 - accuracy: 0.9694 - cost: 252.8750 - val_loss: 0.1141 - val_auc: 0.9898 - val_accuracy: 0.9672 - val_cost: 268.2500\n",
            "Epoch 42/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0973 - auc: 0.9924 - accuracy: 0.9699 - cost: 249.1875 - val_loss: 0.1131 - val_auc: 0.9902 - val_accuracy: 0.9670 - val_cost: 266.5000\n",
            "Epoch 43/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0967 - auc: 0.9923 - accuracy: 0.9705 - cost: 243.3125 - val_loss: 0.1154 - val_auc: 0.9899 - val_accuracy: 0.9668 - val_cost: 269.1250\n",
            "Epoch 44/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0958 - auc: 0.9925 - accuracy: 0.9698 - cost: 248.8125 - val_loss: 0.1111 - val_auc: 0.9903 - val_accuracy: 0.9682 - val_cost: 262.2500\n",
            "Epoch 45/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0962 - auc: 0.9925 - accuracy: 0.9705 - cost: 244.0938 - val_loss: 0.1141 - val_auc: 0.9900 - val_accuracy: 0.9673 - val_cost: 260.0000\n",
            "Epoch 46/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0960 - auc: 0.9925 - accuracy: 0.9708 - cost: 241.0625 - val_loss: 0.1129 - val_auc: 0.9902 - val_accuracy: 0.9680 - val_cost: 263.1250\n",
            "Epoch 47/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0949 - auc: 0.9927 - accuracy: 0.9709 - cost: 239.5938 - val_loss: 0.1156 - val_auc: 0.9896 - val_accuracy: 0.9673 - val_cost: 268.8750\n",
            "Epoch 48/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0946 - auc: 0.9927 - accuracy: 0.9712 - cost: 238.6875 - val_loss: 0.1127 - val_auc: 0.9903 - val_accuracy: 0.9680 - val_cost: 259.7500\n",
            "Epoch 49/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0932 - auc: 0.9928 - accuracy: 0.9711 - cost: 238.1562 - val_loss: 0.1120 - val_auc: 0.9903 - val_accuracy: 0.9675 - val_cost: 266.8750\n",
            "Epoch 50/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0940 - auc: 0.9927 - accuracy: 0.9717 - cost: 234.3125 - val_loss: 0.1131 - val_auc: 0.9902 - val_accuracy: 0.9677 - val_cost: 258.6250\n",
            "Epoch 51/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0915 - auc: 0.9929 - accuracy: 0.9724 - cost: 228.0312 - val_loss: 0.1131 - val_auc: 0.9903 - val_accuracy: 0.9675 - val_cost: 263.6250\n",
            "Epoch 52/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0918 - auc: 0.9929 - accuracy: 0.9721 - cost: 229.6875 - val_loss: 0.1117 - val_auc: 0.9902 - val_accuracy: 0.9682 - val_cost: 260.0000\n",
            "Epoch 53/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0934 - auc: 0.9928 - accuracy: 0.9713 - cost: 237.5625 - val_loss: 0.1117 - val_auc: 0.9901 - val_accuracy: 0.9682 - val_cost: 258.0000\n",
            "Epoch 54/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0922 - auc: 0.9929 - accuracy: 0.9724 - cost: 227.7188 - val_loss: 0.1121 - val_auc: 0.9903 - val_accuracy: 0.9675 - val_cost: 272.0000\n",
            "Epoch 55/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0918 - auc: 0.9931 - accuracy: 0.9719 - cost: 232.3750 - val_loss: 0.1114 - val_auc: 0.9904 - val_accuracy: 0.9676 - val_cost: 261.1250\n",
            "Epoch 56/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0901 - auc: 0.9932 - accuracy: 0.9727 - cost: 225.3750 - val_loss: 0.1092 - val_auc: 0.9907 - val_accuracy: 0.9680 - val_cost: 256.3750\n",
            "Epoch 57/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0907 - auc: 0.9932 - accuracy: 0.9726 - cost: 226.3438 - val_loss: 0.1098 - val_auc: 0.9907 - val_accuracy: 0.9694 - val_cost: 246.0000\n",
            "Epoch 58/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0910 - auc: 0.9930 - accuracy: 0.9729 - cost: 224.3125 - val_loss: 0.1112 - val_auc: 0.9903 - val_accuracy: 0.9677 - val_cost: 265.5000\n",
            "Epoch 59/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0889 - auc: 0.9933 - accuracy: 0.9728 - cost: 224.0625 - val_loss: 0.1106 - val_auc: 0.9903 - val_accuracy: 0.9683 - val_cost: 262.5000\n",
            "Epoch 60/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0898 - auc: 0.9932 - accuracy: 0.9723 - cost: 229.9375 - val_loss: 0.1110 - val_auc: 0.9902 - val_accuracy: 0.9688 - val_cost: 250.7500\n",
            "Epoch 61/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0888 - auc: 0.9933 - accuracy: 0.9730 - cost: 223.3438 - val_loss: 0.1159 - val_auc: 0.9898 - val_accuracy: 0.9676 - val_cost: 261.5000\n",
            "Epoch 62/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0892 - auc: 0.9933 - accuracy: 0.9728 - cost: 224.9062 - val_loss: 0.1130 - val_auc: 0.9901 - val_accuracy: 0.9682 - val_cost: 257.6250\n",
            "Epoch 63/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0875 - auc: 0.9936 - accuracy: 0.9733 - cost: 222.0938 - val_loss: 0.1130 - val_auc: 0.9901 - val_accuracy: 0.9681 - val_cost: 257.1250\n",
            "Epoch 64/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0876 - auc: 0.9935 - accuracy: 0.9730 - cost: 222.5312 - val_loss: 0.1139 - val_auc: 0.9901 - val_accuracy: 0.9682 - val_cost: 266.1250\n",
            "Epoch 65/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0888 - auc: 0.9934 - accuracy: 0.9729 - cost: 224.5938 - val_loss: 0.1128 - val_auc: 0.9899 - val_accuracy: 0.9680 - val_cost: 263.1250\n",
            "Epoch 66/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0880 - auc: 0.9932 - accuracy: 0.9732 - cost: 221.1562 - val_loss: 0.1148 - val_auc: 0.9901 - val_accuracy: 0.9676 - val_cost: 264.3750\n",
            "Epoch 67/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0878 - auc: 0.9934 - accuracy: 0.9736 - cost: 218.2188 - val_loss: 0.1151 - val_auc: 0.9902 - val_accuracy: 0.9688 - val_cost: 243.5000\n",
            "Epoch 68/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0868 - auc: 0.9937 - accuracy: 0.9735 - cost: 218.4375 - val_loss: 0.1139 - val_auc: 0.9900 - val_accuracy: 0.9684 - val_cost: 260.5000\n",
            "Epoch 69/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0859 - auc: 0.9936 - accuracy: 0.9741 - cost: 213.9688 - val_loss: 0.1126 - val_auc: 0.9900 - val_accuracy: 0.9680 - val_cost: 259.3750\n",
            "Epoch 70/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0860 - auc: 0.9937 - accuracy: 0.9742 - cost: 214.2188 - val_loss: 0.1146 - val_auc: 0.9900 - val_accuracy: 0.9680 - val_cost: 258.8750\n",
            "Epoch 71/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0863 - auc: 0.9936 - accuracy: 0.9738 - cost: 216.4062 - val_loss: 0.1164 - val_auc: 0.9899 - val_accuracy: 0.9672 - val_cost: 258.3750\n",
            "Epoch 72/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0866 - auc: 0.9936 - accuracy: 0.9738 - cost: 216.4688 - val_loss: 0.1123 - val_auc: 0.9902 - val_accuracy: 0.9687 - val_cost: 253.1250\n",
            "Epoch 73/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0847 - auc: 0.9938 - accuracy: 0.9744 - cost: 211.5625 - val_loss: 0.1153 - val_auc: 0.9901 - val_accuracy: 0.9698 - val_cost: 239.0000\n",
            "Epoch 74/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0865 - auc: 0.9935 - accuracy: 0.9739 - cost: 215.8438 - val_loss: 0.1124 - val_auc: 0.9901 - val_accuracy: 0.9687 - val_cost: 252.5000\n",
            "Epoch 75/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0848 - auc: 0.9940 - accuracy: 0.9747 - cost: 209.3438 - val_loss: 0.1153 - val_auc: 0.9897 - val_accuracy: 0.9670 - val_cost: 270.1250\n",
            "Epoch 76/100\n",
            "1600/1600 [==============================] - 4s 3ms/step - loss: 0.0848 - auc: 0.9939 - accuracy: 0.9747 - cost: 209.5000 - val_loss: 0.1118 - val_auc: 0.9900 - val_accuracy: 0.9686 - val_cost: 257.6250\n",
            "Epoch 77/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0849 - auc: 0.9939 - accuracy: 0.9742 - cost: 212.9688 - val_loss: 0.1119 - val_auc: 0.9902 - val_accuracy: 0.9702 - val_cost: 247.1250\n",
            "Epoch 78/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0855 - auc: 0.9936 - accuracy: 0.9744 - cost: 211.4062 - val_loss: 0.1110 - val_auc: 0.9904 - val_accuracy: 0.9684 - val_cost: 262.8750\n",
            "Epoch 79/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0840 - auc: 0.9939 - accuracy: 0.9746 - cost: 209.9375 - val_loss: 0.1161 - val_auc: 0.9898 - val_accuracy: 0.9680 - val_cost: 261.3750\n",
            "Epoch 80/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0841 - auc: 0.9939 - accuracy: 0.9748 - cost: 208.9688 - val_loss: 0.1128 - val_auc: 0.9901 - val_accuracy: 0.9680 - val_cost: 262.8750\n",
            "Epoch 81/100\n",
            "1600/1600 [==============================] - 4s 3ms/step - loss: 0.0844 - auc: 0.9939 - accuracy: 0.9745 - cost: 211.2500 - val_loss: 0.1136 - val_auc: 0.9899 - val_accuracy: 0.9690 - val_cost: 249.5000\n",
            "Epoch 82/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0838 - auc: 0.9939 - accuracy: 0.9750 - cost: 207.5625 - val_loss: 0.1154 - val_auc: 0.9897 - val_accuracy: 0.9677 - val_cost: 259.1250\n",
            "Epoch 83/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0834 - auc: 0.9941 - accuracy: 0.9750 - cost: 206.4375 - val_loss: 0.1141 - val_auc: 0.9897 - val_accuracy: 0.9684 - val_cost: 260.0000\n",
            "1000/1000 [==============================] - 1s 983us/step\n",
            "fold train/predict time: 0:06:28.857905\n",
            "x_train shape: (128000, 67)\n",
            "Epoch 1/100\n",
            "1600/1600 [==============================] - 6s 3ms/step - loss: 0.3809 - auc: 0.9059 - accuracy: 0.8286 - cost: 1410.5625 - val_loss: 0.2826 - val_auc: 0.9492 - val_accuracy: 0.8830 - val_cost: 944.2500\n",
            "Epoch 2/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.2631 - auc: 0.9563 - accuracy: 0.8960 - cost: 842.7500 - val_loss: 0.2274 - val_auc: 0.9672 - val_accuracy: 0.9096 - val_cost: 734.1250\n",
            "Epoch 3/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.2209 - auc: 0.9691 - accuracy: 0.9150 - cost: 691.5938 - val_loss: 0.1984 - val_auc: 0.9752 - val_accuracy: 0.9246 - val_cost: 631.1250\n",
            "Epoch 4/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1945 - auc: 0.9758 - accuracy: 0.9291 - cost: 578.5000 - val_loss: 0.1806 - val_auc: 0.9795 - val_accuracy: 0.9328 - val_cost: 565.2500\n",
            "Epoch 5/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1771 - auc: 0.9796 - accuracy: 0.9362 - cost: 520.9062 - val_loss: 0.1679 - val_auc: 0.9819 - val_accuracy: 0.9401 - val_cost: 472.8750\n",
            "Epoch 6/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1666 - auc: 0.9818 - accuracy: 0.9405 - cost: 485.1875 - val_loss: 0.1607 - val_auc: 0.9832 - val_accuracy: 0.9425 - val_cost: 486.1250\n",
            "Epoch 7/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1584 - auc: 0.9833 - accuracy: 0.9448 - cost: 451.7188 - val_loss: 0.1498 - val_auc: 0.9849 - val_accuracy: 0.9475 - val_cost: 418.2500\n",
            "Epoch 8/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1517 - auc: 0.9845 - accuracy: 0.9479 - cost: 425.1250 - val_loss: 0.1431 - val_auc: 0.9858 - val_accuracy: 0.9523 - val_cost: 383.7500\n",
            "Epoch 9/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1465 - auc: 0.9855 - accuracy: 0.9497 - cost: 410.9688 - val_loss: 0.1379 - val_auc: 0.9867 - val_accuracy: 0.9538 - val_cost: 373.8750\n",
            "Epoch 10/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1403 - auc: 0.9863 - accuracy: 0.9530 - cost: 383.9375 - val_loss: 0.1362 - val_auc: 0.9869 - val_accuracy: 0.9546 - val_cost: 378.1250\n",
            "Epoch 11/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1371 - auc: 0.9869 - accuracy: 0.9547 - cost: 370.3438 - val_loss: 0.1322 - val_auc: 0.9875 - val_accuracy: 0.9567 - val_cost: 354.5000\n",
            "Epoch 12/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1318 - auc: 0.9878 - accuracy: 0.9558 - cost: 360.6875 - val_loss: 0.1304 - val_auc: 0.9878 - val_accuracy: 0.9582 - val_cost: 339.0000\n",
            "Epoch 13/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1283 - auc: 0.9883 - accuracy: 0.9576 - cost: 345.8438 - val_loss: 0.1291 - val_auc: 0.9881 - val_accuracy: 0.9578 - val_cost: 334.7500\n",
            "Epoch 14/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1256 - auc: 0.9887 - accuracy: 0.9595 - cost: 330.8438 - val_loss: 0.1287 - val_auc: 0.9880 - val_accuracy: 0.9590 - val_cost: 334.1250\n",
            "Epoch 15/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1233 - auc: 0.9890 - accuracy: 0.9604 - cost: 323.2188 - val_loss: 0.1274 - val_auc: 0.9883 - val_accuracy: 0.9595 - val_cost: 324.0000\n",
            "Epoch 16/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1203 - auc: 0.9894 - accuracy: 0.9613 - cost: 316.5938 - val_loss: 0.1246 - val_auc: 0.9885 - val_accuracy: 0.9601 - val_cost: 328.8750\n",
            "Epoch 17/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1192 - auc: 0.9896 - accuracy: 0.9614 - cost: 315.8125 - val_loss: 0.1209 - val_auc: 0.9891 - val_accuracy: 0.9625 - val_cost: 304.5000\n",
            "Epoch 18/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1170 - auc: 0.9899 - accuracy: 0.9634 - cost: 298.4062 - val_loss: 0.1246 - val_auc: 0.9885 - val_accuracy: 0.9604 - val_cost: 320.3750\n",
            "Epoch 19/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1157 - auc: 0.9900 - accuracy: 0.9636 - cost: 297.8125 - val_loss: 0.1227 - val_auc: 0.9891 - val_accuracy: 0.9618 - val_cost: 302.5000\n",
            "Epoch 20/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1129 - auc: 0.9904 - accuracy: 0.9645 - cost: 290.7500 - val_loss: 0.1184 - val_auc: 0.9895 - val_accuracy: 0.9628 - val_cost: 301.1250\n",
            "Epoch 21/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1125 - auc: 0.9905 - accuracy: 0.9646 - cost: 289.9375 - val_loss: 0.1192 - val_auc: 0.9892 - val_accuracy: 0.9633 - val_cost: 291.5000\n",
            "Epoch 22/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1105 - auc: 0.9908 - accuracy: 0.9650 - cost: 286.9375 - val_loss: 0.1184 - val_auc: 0.9890 - val_accuracy: 0.9639 - val_cost: 292.3750\n",
            "Epoch 23/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1093 - auc: 0.9908 - accuracy: 0.9658 - cost: 280.7500 - val_loss: 0.1166 - val_auc: 0.9897 - val_accuracy: 0.9649 - val_cost: 282.2500\n",
            "Epoch 24/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1076 - auc: 0.9911 - accuracy: 0.9667 - cost: 273.8125 - val_loss: 0.1151 - val_auc: 0.9899 - val_accuracy: 0.9656 - val_cost: 276.1250\n",
            "Epoch 25/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1063 - auc: 0.9912 - accuracy: 0.9668 - cost: 272.0625 - val_loss: 0.1166 - val_auc: 0.9899 - val_accuracy: 0.9658 - val_cost: 272.6250\n",
            "Epoch 26/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1050 - auc: 0.9915 - accuracy: 0.9672 - cost: 268.9375 - val_loss: 0.1151 - val_auc: 0.9898 - val_accuracy: 0.9666 - val_cost: 275.6250\n",
            "Epoch 27/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1045 - auc: 0.9915 - accuracy: 0.9671 - cost: 270.4375 - val_loss: 0.1163 - val_auc: 0.9896 - val_accuracy: 0.9655 - val_cost: 286.0000\n",
            "Epoch 28/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1024 - auc: 0.9917 - accuracy: 0.9682 - cost: 260.8750 - val_loss: 0.1171 - val_auc: 0.9896 - val_accuracy: 0.9657 - val_cost: 276.7500\n",
            "Epoch 29/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1035 - auc: 0.9916 - accuracy: 0.9678 - cost: 264.5938 - val_loss: 0.1136 - val_auc: 0.9901 - val_accuracy: 0.9658 - val_cost: 277.0000\n",
            "Epoch 30/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1011 - auc: 0.9919 - accuracy: 0.9683 - cost: 259.4062 - val_loss: 0.1129 - val_auc: 0.9903 - val_accuracy: 0.9659 - val_cost: 271.1250\n",
            "Epoch 31/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1003 - auc: 0.9920 - accuracy: 0.9693 - cost: 251.7812 - val_loss: 0.1171 - val_auc: 0.9895 - val_accuracy: 0.9652 - val_cost: 293.3750\n",
            "Epoch 32/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1002 - auc: 0.9920 - accuracy: 0.9692 - cost: 253.1562 - val_loss: 0.1126 - val_auc: 0.9902 - val_accuracy: 0.9669 - val_cost: 263.2500\n",
            "Epoch 33/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1000 - auc: 0.9920 - accuracy: 0.9696 - cost: 249.5000 - val_loss: 0.1128 - val_auc: 0.9902 - val_accuracy: 0.9671 - val_cost: 263.0000\n",
            "Epoch 34/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0984 - auc: 0.9922 - accuracy: 0.9695 - cost: 250.3438 - val_loss: 0.1125 - val_auc: 0.9900 - val_accuracy: 0.9670 - val_cost: 269.0000\n",
            "Epoch 35/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0983 - auc: 0.9923 - accuracy: 0.9692 - cost: 252.3438 - val_loss: 0.1130 - val_auc: 0.9902 - val_accuracy: 0.9673 - val_cost: 259.8750\n",
            "Epoch 36/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0976 - auc: 0.9924 - accuracy: 0.9702 - cost: 245.4375 - val_loss: 0.1109 - val_auc: 0.9905 - val_accuracy: 0.9683 - val_cost: 253.8750\n",
            "Epoch 37/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0964 - auc: 0.9925 - accuracy: 0.9702 - cost: 244.8438 - val_loss: 0.1129 - val_auc: 0.9903 - val_accuracy: 0.9672 - val_cost: 261.3750\n",
            "Epoch 38/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0969 - auc: 0.9923 - accuracy: 0.9711 - cost: 237.2500 - val_loss: 0.1115 - val_auc: 0.9903 - val_accuracy: 0.9664 - val_cost: 264.3750\n",
            "Epoch 39/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0948 - auc: 0.9926 - accuracy: 0.9710 - cost: 238.3438 - val_loss: 0.1124 - val_auc: 0.9903 - val_accuracy: 0.9668 - val_cost: 266.6250\n",
            "Epoch 40/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0945 - auc: 0.9925 - accuracy: 0.9723 - cost: 227.0625 - val_loss: 0.1102 - val_auc: 0.9902 - val_accuracy: 0.9680 - val_cost: 260.3750\n",
            "Epoch 41/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0944 - auc: 0.9927 - accuracy: 0.9712 - cost: 237.2500 - val_loss: 0.1130 - val_auc: 0.9900 - val_accuracy: 0.9677 - val_cost: 254.1250\n",
            "Epoch 42/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0953 - auc: 0.9924 - accuracy: 0.9715 - cost: 233.3438 - val_loss: 0.1128 - val_auc: 0.9901 - val_accuracy: 0.9680 - val_cost: 261.2500\n",
            "Epoch 43/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0943 - auc: 0.9927 - accuracy: 0.9716 - cost: 233.2188 - val_loss: 0.1125 - val_auc: 0.9904 - val_accuracy: 0.9681 - val_cost: 251.1250\n",
            "Epoch 44/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0933 - auc: 0.9928 - accuracy: 0.9723 - cost: 226.9062 - val_loss: 0.1144 - val_auc: 0.9899 - val_accuracy: 0.9675 - val_cost: 255.7500\n",
            "Epoch 45/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0937 - auc: 0.9926 - accuracy: 0.9723 - cost: 228.4688 - val_loss: 0.1101 - val_auc: 0.9904 - val_accuracy: 0.9680 - val_cost: 256.0000\n",
            "Epoch 46/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0923 - auc: 0.9928 - accuracy: 0.9724 - cost: 226.6875 - val_loss: 0.1111 - val_auc: 0.9903 - val_accuracy: 0.9672 - val_cost: 265.7500\n",
            "Epoch 47/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0925 - auc: 0.9928 - accuracy: 0.9724 - cost: 227.0312 - val_loss: 0.1094 - val_auc: 0.9907 - val_accuracy: 0.9682 - val_cost: 252.5000\n",
            "Epoch 48/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0923 - auc: 0.9929 - accuracy: 0.9724 - cost: 227.8750 - val_loss: 0.1123 - val_auc: 0.9901 - val_accuracy: 0.9687 - val_cost: 254.2500\n",
            "Epoch 49/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0912 - auc: 0.9930 - accuracy: 0.9724 - cost: 226.4062 - val_loss: 0.1125 - val_auc: 0.9902 - val_accuracy: 0.9674 - val_cost: 263.3750\n",
            "Epoch 50/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0903 - auc: 0.9930 - accuracy: 0.9732 - cost: 220.0625 - val_loss: 0.1143 - val_auc: 0.9900 - val_accuracy: 0.9681 - val_cost: 261.3750\n",
            "Epoch 51/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0901 - auc: 0.9932 - accuracy: 0.9731 - cost: 221.7188 - val_loss: 0.1118 - val_auc: 0.9904 - val_accuracy: 0.9672 - val_cost: 265.8750\n",
            "Epoch 52/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0901 - auc: 0.9931 - accuracy: 0.9733 - cost: 219.0000 - val_loss: 0.1120 - val_auc: 0.9902 - val_accuracy: 0.9679 - val_cost: 264.8750\n",
            "Epoch 53/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0899 - auc: 0.9932 - accuracy: 0.9730 - cost: 221.5312 - val_loss: 0.1133 - val_auc: 0.9900 - val_accuracy: 0.9682 - val_cost: 262.7500\n",
            "1000/1000 [==============================] - 1s 991us/step\n",
            "fold train/predict time: 0:04:05.549003\n",
            "x_train shape: (128000, 67)\n",
            "Epoch 1/100\n",
            "1600/1600 [==============================] - 6s 3ms/step - loss: 0.3786 - auc: 0.9072 - accuracy: 0.8287 - cost: 1412.4375 - val_loss: 0.2821 - val_auc: 0.9499 - val_accuracy: 0.8854 - val_cost: 913.5000\n",
            "Epoch 2/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.2644 - auc: 0.9560 - accuracy: 0.8940 - cost: 858.6562 - val_loss: 0.2322 - val_auc: 0.9661 - val_accuracy: 0.9098 - val_cost: 740.6250\n",
            "Epoch 3/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.2239 - auc: 0.9683 - accuracy: 0.9143 - cost: 698.1562 - val_loss: 0.1968 - val_auc: 0.9754 - val_accuracy: 0.9261 - val_cost: 587.6250\n",
            "Epoch 4/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1976 - auc: 0.9752 - accuracy: 0.9264 - cost: 598.9062 - val_loss: 0.1774 - val_auc: 0.9800 - val_accuracy: 0.9354 - val_cost: 513.8750\n",
            "Epoch 5/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1804 - auc: 0.9790 - accuracy: 0.9346 - cost: 532.5625 - val_loss: 0.1652 - val_auc: 0.9824 - val_accuracy: 0.9401 - val_cost: 493.1250\n",
            "Epoch 6/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1683 - auc: 0.9816 - accuracy: 0.9405 - cost: 486.6562 - val_loss: 0.1564 - val_auc: 0.9841 - val_accuracy: 0.9448 - val_cost: 433.3750\n",
            "Epoch 7/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1591 - auc: 0.9832 - accuracy: 0.9446 - cost: 452.6250 - val_loss: 0.1460 - val_auc: 0.9857 - val_accuracy: 0.9494 - val_cost: 409.1250\n",
            "Epoch 8/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1527 - auc: 0.9844 - accuracy: 0.9473 - cost: 430.4375 - val_loss: 0.1428 - val_auc: 0.9862 - val_accuracy: 0.9507 - val_cost: 403.6250\n",
            "Epoch 9/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1473 - auc: 0.9854 - accuracy: 0.9498 - cost: 411.4062 - val_loss: 0.1411 - val_auc: 0.9866 - val_accuracy: 0.9524 - val_cost: 401.1250\n",
            "Epoch 10/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1417 - auc: 0.9863 - accuracy: 0.9529 - cost: 385.3750 - val_loss: 0.1375 - val_auc: 0.9870 - val_accuracy: 0.9551 - val_cost: 356.1250\n",
            "Epoch 11/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1366 - auc: 0.9871 - accuracy: 0.9538 - cost: 378.2812 - val_loss: 0.1331 - val_auc: 0.9877 - val_accuracy: 0.9584 - val_cost: 327.6250\n",
            "Epoch 12/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1338 - auc: 0.9876 - accuracy: 0.9558 - cost: 362.0625 - val_loss: 0.1301 - val_auc: 0.9881 - val_accuracy: 0.9581 - val_cost: 330.8750\n",
            "Epoch 13/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1290 - auc: 0.9882 - accuracy: 0.9576 - cost: 347.9375 - val_loss: 0.1283 - val_auc: 0.9881 - val_accuracy: 0.9600 - val_cost: 318.7500\n",
            "Epoch 14/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1261 - auc: 0.9885 - accuracy: 0.9584 - cost: 341.8125 - val_loss: 0.1268 - val_auc: 0.9888 - val_accuracy: 0.9600 - val_cost: 308.5000\n",
            "Epoch 15/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1239 - auc: 0.9888 - accuracy: 0.9602 - cost: 326.5625 - val_loss: 0.1253 - val_auc: 0.9887 - val_accuracy: 0.9618 - val_cost: 302.2500\n",
            "Epoch 16/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1208 - auc: 0.9894 - accuracy: 0.9605 - cost: 323.8750 - val_loss: 0.1257 - val_auc: 0.9888 - val_accuracy: 0.9597 - val_cost: 338.2500\n",
            "Epoch 17/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1193 - auc: 0.9896 - accuracy: 0.9621 - cost: 311.4062 - val_loss: 0.1181 - val_auc: 0.9894 - val_accuracy: 0.9633 - val_cost: 295.2500\n",
            "Epoch 18/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1181 - auc: 0.9895 - accuracy: 0.9624 - cost: 309.6250 - val_loss: 0.1214 - val_auc: 0.9891 - val_accuracy: 0.9621 - val_cost: 303.2500\n",
            "Epoch 19/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1156 - auc: 0.9900 - accuracy: 0.9636 - cost: 298.8750 - val_loss: 0.1199 - val_auc: 0.9892 - val_accuracy: 0.9634 - val_cost: 292.1250\n",
            "Epoch 20/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1148 - auc: 0.9901 - accuracy: 0.9633 - cost: 302.2188 - val_loss: 0.1203 - val_auc: 0.9890 - val_accuracy: 0.9633 - val_cost: 296.2500\n",
            "Epoch 21/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1125 - auc: 0.9904 - accuracy: 0.9642 - cost: 294.8438 - val_loss: 0.1163 - val_auc: 0.9898 - val_accuracy: 0.9644 - val_cost: 291.8750\n",
            "Epoch 22/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1114 - auc: 0.9905 - accuracy: 0.9650 - cost: 288.5312 - val_loss: 0.1181 - val_auc: 0.9895 - val_accuracy: 0.9644 - val_cost: 286.1250\n",
            "Epoch 23/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1099 - auc: 0.9908 - accuracy: 0.9650 - cost: 287.9688 - val_loss: 0.1153 - val_auc: 0.9899 - val_accuracy: 0.9641 - val_cost: 291.3750\n",
            "Epoch 24/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1077 - auc: 0.9910 - accuracy: 0.9667 - cost: 274.9375 - val_loss: 0.1157 - val_auc: 0.9900 - val_accuracy: 0.9648 - val_cost: 285.1250\n",
            "Epoch 25/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1073 - auc: 0.9911 - accuracy: 0.9665 - cost: 275.6875 - val_loss: 0.1146 - val_auc: 0.9902 - val_accuracy: 0.9649 - val_cost: 286.3750\n",
            "Epoch 26/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1057 - auc: 0.9911 - accuracy: 0.9671 - cost: 271.5312 - val_loss: 0.1141 - val_auc: 0.9899 - val_accuracy: 0.9647 - val_cost: 285.0000\n",
            "Epoch 27/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1054 - auc: 0.9914 - accuracy: 0.9675 - cost: 267.6562 - val_loss: 0.1174 - val_auc: 0.9900 - val_accuracy: 0.9641 - val_cost: 281.8750\n",
            "Epoch 28/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1052 - auc: 0.9913 - accuracy: 0.9677 - cost: 265.1875 - val_loss: 0.1145 - val_auc: 0.9900 - val_accuracy: 0.9666 - val_cost: 264.0000\n",
            "Epoch 29/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1039 - auc: 0.9914 - accuracy: 0.9681 - cost: 263.0000 - val_loss: 0.1134 - val_auc: 0.9903 - val_accuracy: 0.9645 - val_cost: 290.7500\n",
            "Epoch 30/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1023 - auc: 0.9918 - accuracy: 0.9681 - cost: 263.2812 - val_loss: 0.1167 - val_auc: 0.9901 - val_accuracy: 0.9641 - val_cost: 305.7500\n",
            "Epoch 31/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1009 - auc: 0.9918 - accuracy: 0.9688 - cost: 256.5312 - val_loss: 0.1146 - val_auc: 0.9899 - val_accuracy: 0.9665 - val_cost: 270.3750\n",
            "Epoch 32/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1013 - auc: 0.9919 - accuracy: 0.9685 - cost: 259.6875 - val_loss: 0.1123 - val_auc: 0.9902 - val_accuracy: 0.9674 - val_cost: 261.0000\n",
            "Epoch 33/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1003 - auc: 0.9918 - accuracy: 0.9689 - cost: 256.3750 - val_loss: 0.1154 - val_auc: 0.9901 - val_accuracy: 0.9659 - val_cost: 268.2500\n",
            "Epoch 34/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0994 - auc: 0.9921 - accuracy: 0.9689 - cost: 256.3750 - val_loss: 0.1156 - val_auc: 0.9900 - val_accuracy: 0.9663 - val_cost: 278.3750\n",
            "Epoch 35/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0997 - auc: 0.9921 - accuracy: 0.9692 - cost: 253.9688 - val_loss: 0.1140 - val_auc: 0.9900 - val_accuracy: 0.9661 - val_cost: 275.6250\n",
            "Epoch 36/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0985 - auc: 0.9921 - accuracy: 0.9701 - cost: 247.0312 - val_loss: 0.1112 - val_auc: 0.9906 - val_accuracy: 0.9659 - val_cost: 274.3750\n",
            "Epoch 37/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0968 - auc: 0.9924 - accuracy: 0.9704 - cost: 243.9375 - val_loss: 0.1118 - val_auc: 0.9902 - val_accuracy: 0.9672 - val_cost: 269.8750\n",
            "Epoch 38/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0966 - auc: 0.9923 - accuracy: 0.9710 - cost: 239.5312 - val_loss: 0.1119 - val_auc: 0.9902 - val_accuracy: 0.9666 - val_cost: 269.2500\n",
            "Epoch 39/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0958 - auc: 0.9925 - accuracy: 0.9706 - cost: 243.1562 - val_loss: 0.1146 - val_auc: 0.9899 - val_accuracy: 0.9648 - val_cost: 288.5000\n",
            "Epoch 40/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0958 - auc: 0.9925 - accuracy: 0.9711 - cost: 237.5625 - val_loss: 0.1140 - val_auc: 0.9900 - val_accuracy: 0.9674 - val_cost: 264.5000\n",
            "Epoch 41/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0966 - auc: 0.9923 - accuracy: 0.9705 - cost: 242.0000 - val_loss: 0.1100 - val_auc: 0.9905 - val_accuracy: 0.9674 - val_cost: 264.5000\n",
            "Epoch 42/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0949 - auc: 0.9925 - accuracy: 0.9709 - cost: 240.4062 - val_loss: 0.1124 - val_auc: 0.9900 - val_accuracy: 0.9680 - val_cost: 261.0000\n",
            "Epoch 43/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0955 - auc: 0.9925 - accuracy: 0.9713 - cost: 236.9375 - val_loss: 0.1124 - val_auc: 0.9904 - val_accuracy: 0.9670 - val_cost: 263.8750\n",
            "Epoch 44/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0947 - auc: 0.9925 - accuracy: 0.9712 - cost: 238.7812 - val_loss: 0.1123 - val_auc: 0.9902 - val_accuracy: 0.9679 - val_cost: 259.3750\n",
            "Epoch 45/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0935 - auc: 0.9928 - accuracy: 0.9716 - cost: 235.2188 - val_loss: 0.1100 - val_auc: 0.9904 - val_accuracy: 0.9682 - val_cost: 263.5000\n",
            "Epoch 46/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0918 - auc: 0.9929 - accuracy: 0.9719 - cost: 232.0938 - val_loss: 0.1124 - val_auc: 0.9899 - val_accuracy: 0.9673 - val_cost: 268.5000\n",
            "Epoch 47/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0919 - auc: 0.9929 - accuracy: 0.9718 - cost: 232.6562 - val_loss: 0.1134 - val_auc: 0.9899 - val_accuracy: 0.9667 - val_cost: 267.7500\n",
            "Epoch 48/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0920 - auc: 0.9929 - accuracy: 0.9726 - cost: 225.8125 - val_loss: 0.1118 - val_auc: 0.9901 - val_accuracy: 0.9672 - val_cost: 262.1250\n",
            "Epoch 49/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0906 - auc: 0.9930 - accuracy: 0.9727 - cost: 224.5625 - val_loss: 0.1106 - val_auc: 0.9903 - val_accuracy: 0.9679 - val_cost: 261.2500\n",
            "Epoch 50/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0902 - auc: 0.9931 - accuracy: 0.9728 - cost: 224.4062 - val_loss: 0.1120 - val_auc: 0.9902 - val_accuracy: 0.9675 - val_cost: 269.7500\n",
            "Epoch 51/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0911 - auc: 0.9930 - accuracy: 0.9727 - cost: 225.9375 - val_loss: 0.1127 - val_auc: 0.9903 - val_accuracy: 0.9682 - val_cost: 254.3750\n",
            "Epoch 52/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0897 - auc: 0.9932 - accuracy: 0.9733 - cost: 220.9375 - val_loss: 0.1141 - val_auc: 0.9897 - val_accuracy: 0.9676 - val_cost: 268.7500\n",
            "Epoch 53/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0894 - auc: 0.9932 - accuracy: 0.9731 - cost: 221.0625 - val_loss: 0.1152 - val_auc: 0.9900 - val_accuracy: 0.9679 - val_cost: 265.6250\n",
            "Epoch 54/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0891 - auc: 0.9932 - accuracy: 0.9729 - cost: 223.5000 - val_loss: 0.1100 - val_auc: 0.9903 - val_accuracy: 0.9693 - val_cost: 246.7500\n",
            "Epoch 55/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0891 - auc: 0.9933 - accuracy: 0.9731 - cost: 222.1875 - val_loss: 0.1094 - val_auc: 0.9905 - val_accuracy: 0.9689 - val_cost: 247.8750\n",
            "Epoch 56/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0887 - auc: 0.9933 - accuracy: 0.9737 - cost: 216.9688 - val_loss: 0.1117 - val_auc: 0.9904 - val_accuracy: 0.9679 - val_cost: 261.7500\n",
            "Epoch 57/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0874 - auc: 0.9934 - accuracy: 0.9737 - cost: 217.2500 - val_loss: 0.1118 - val_auc: 0.9905 - val_accuracy: 0.9685 - val_cost: 255.1250\n",
            "Epoch 58/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0874 - auc: 0.9934 - accuracy: 0.9741 - cost: 214.0312 - val_loss: 0.1114 - val_auc: 0.9905 - val_accuracy: 0.9678 - val_cost: 267.7500\n",
            "Epoch 59/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0880 - auc: 0.9934 - accuracy: 0.9732 - cost: 221.5312 - val_loss: 0.1119 - val_auc: 0.9905 - val_accuracy: 0.9693 - val_cost: 241.7500\n",
            "Epoch 60/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0871 - auc: 0.9935 - accuracy: 0.9736 - cost: 218.1250 - val_loss: 0.1087 - val_auc: 0.9908 - val_accuracy: 0.9697 - val_cost: 241.2500\n",
            "Epoch 61/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0883 - auc: 0.9933 - accuracy: 0.9740 - cost: 215.3750 - val_loss: 0.1070 - val_auc: 0.9908 - val_accuracy: 0.9696 - val_cost: 250.5000\n",
            "Epoch 62/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0874 - auc: 0.9935 - accuracy: 0.9735 - cost: 219.0312 - val_loss: 0.1099 - val_auc: 0.9905 - val_accuracy: 0.9691 - val_cost: 247.5000\n",
            "Epoch 63/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0852 - auc: 0.9937 - accuracy: 0.9747 - cost: 208.5625 - val_loss: 0.1099 - val_auc: 0.9904 - val_accuracy: 0.9688 - val_cost: 252.6250\n",
            "Epoch 64/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0859 - auc: 0.9937 - accuracy: 0.9741 - cost: 213.4688 - val_loss: 0.1137 - val_auc: 0.9903 - val_accuracy: 0.9688 - val_cost: 251.8750\n",
            "Epoch 65/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0859 - auc: 0.9938 - accuracy: 0.9748 - cost: 208.5625 - val_loss: 0.1121 - val_auc: 0.9903 - val_accuracy: 0.9687 - val_cost: 248.5000\n",
            "Epoch 66/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0867 - auc: 0.9935 - accuracy: 0.9741 - cost: 214.3750 - val_loss: 0.1120 - val_auc: 0.9902 - val_accuracy: 0.9677 - val_cost: 269.6250\n",
            "Epoch 67/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0858 - auc: 0.9936 - accuracy: 0.9746 - cost: 210.3125 - val_loss: 0.1091 - val_auc: 0.9905 - val_accuracy: 0.9693 - val_cost: 248.6250\n",
            "Epoch 68/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0857 - auc: 0.9936 - accuracy: 0.9745 - cost: 211.1875 - val_loss: 0.1105 - val_auc: 0.9906 - val_accuracy: 0.9687 - val_cost: 253.7500\n",
            "Epoch 69/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0854 - auc: 0.9936 - accuracy: 0.9744 - cost: 211.5938 - val_loss: 0.1106 - val_auc: 0.9905 - val_accuracy: 0.9687 - val_cost: 253.5000\n",
            "Epoch 70/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0848 - auc: 0.9938 - accuracy: 0.9749 - cost: 208.3438 - val_loss: 0.1118 - val_auc: 0.9905 - val_accuracy: 0.9689 - val_cost: 250.3750\n",
            "1000/1000 [==============================] - 1s 965us/step\n",
            "fold train/predict time: 0:05:23.527754\n",
            "x_train shape: (128000, 67)\n",
            "Epoch 1/100\n",
            "1600/1600 [==============================] - 6s 3ms/step - loss: 0.3798 - auc: 0.9068 - accuracy: 0.8294 - cost: 1402.6250 - val_loss: 0.2823 - val_auc: 0.9503 - val_accuracy: 0.8815 - val_cost: 990.3750\n",
            "Epoch 2/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.2636 - auc: 0.9560 - accuracy: 0.8943 - cost: 855.1250 - val_loss: 0.2291 - val_auc: 0.9681 - val_accuracy: 0.9100 - val_cost: 696.6250\n",
            "Epoch 3/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.2199 - auc: 0.9692 - accuracy: 0.9164 - cost: 677.0312 - val_loss: 0.1963 - val_auc: 0.9755 - val_accuracy: 0.9275 - val_cost: 578.5000\n",
            "Epoch 4/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1936 - auc: 0.9759 - accuracy: 0.9287 - cost: 577.9688 - val_loss: 0.1738 - val_auc: 0.9804 - val_accuracy: 0.9368 - val_cost: 513.5000\n",
            "Epoch 5/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1744 - auc: 0.9802 - accuracy: 0.9370 - cost: 512.1250 - val_loss: 0.1623 - val_auc: 0.9827 - val_accuracy: 0.9426 - val_cost: 466.6250\n",
            "Epoch 6/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1621 - auc: 0.9826 - accuracy: 0.9422 - cost: 470.0938 - val_loss: 0.1512 - val_auc: 0.9848 - val_accuracy: 0.9469 - val_cost: 433.6250\n",
            "Epoch 7/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1541 - auc: 0.9840 - accuracy: 0.9463 - cost: 436.8125 - val_loss: 0.1448 - val_auc: 0.9860 - val_accuracy: 0.9512 - val_cost: 383.1250\n",
            "Epoch 8/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1448 - auc: 0.9857 - accuracy: 0.9500 - cost: 405.9062 - val_loss: 0.1375 - val_auc: 0.9868 - val_accuracy: 0.9530 - val_cost: 382.2500\n",
            "Epoch 9/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1404 - auc: 0.9864 - accuracy: 0.9522 - cost: 388.0000 - val_loss: 0.1345 - val_auc: 0.9873 - val_accuracy: 0.9555 - val_cost: 363.3750\n",
            "Epoch 10/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1343 - auc: 0.9874 - accuracy: 0.9550 - cost: 366.9062 - val_loss: 0.1328 - val_auc: 0.9878 - val_accuracy: 0.9558 - val_cost: 357.5000\n",
            "Epoch 11/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1306 - auc: 0.9879 - accuracy: 0.9565 - cost: 354.5312 - val_loss: 0.1304 - val_auc: 0.9879 - val_accuracy: 0.9567 - val_cost: 360.1250\n",
            "Epoch 12/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1284 - auc: 0.9882 - accuracy: 0.9581 - cost: 341.6562 - val_loss: 0.1251 - val_auc: 0.9889 - val_accuracy: 0.9603 - val_cost: 315.8750\n",
            "Epoch 13/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1247 - auc: 0.9888 - accuracy: 0.9596 - cost: 329.8750 - val_loss: 0.1256 - val_auc: 0.9888 - val_accuracy: 0.9593 - val_cost: 318.6250\n",
            "Epoch 14/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1214 - auc: 0.9893 - accuracy: 0.9603 - cost: 324.7188 - val_loss: 0.1214 - val_auc: 0.9893 - val_accuracy: 0.9613 - val_cost: 317.2500\n",
            "Epoch 15/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1171 - auc: 0.9897 - accuracy: 0.9626 - cost: 304.6562 - val_loss: 0.1200 - val_auc: 0.9893 - val_accuracy: 0.9620 - val_cost: 306.6250\n",
            "Epoch 16/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1157 - auc: 0.9901 - accuracy: 0.9628 - cost: 304.3125 - val_loss: 0.1203 - val_auc: 0.9893 - val_accuracy: 0.9623 - val_cost: 310.6250\n",
            "Epoch 17/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1134 - auc: 0.9903 - accuracy: 0.9638 - cost: 296.5312 - val_loss: 0.1201 - val_auc: 0.9892 - val_accuracy: 0.9624 - val_cost: 298.5000\n",
            "Epoch 18/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1116 - auc: 0.9905 - accuracy: 0.9653 - cost: 283.0938 - val_loss: 0.1181 - val_auc: 0.9898 - val_accuracy: 0.9633 - val_cost: 289.1250\n",
            "Epoch 19/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1111 - auc: 0.9905 - accuracy: 0.9657 - cost: 281.1875 - val_loss: 0.1170 - val_auc: 0.9897 - val_accuracy: 0.9641 - val_cost: 292.8750\n",
            "Epoch 20/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1097 - auc: 0.9907 - accuracy: 0.9654 - cost: 282.7188 - val_loss: 0.1194 - val_auc: 0.9896 - val_accuracy: 0.9638 - val_cost: 283.3750\n",
            "Epoch 21/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1073 - auc: 0.9910 - accuracy: 0.9659 - cost: 278.5938 - val_loss: 0.1182 - val_auc: 0.9897 - val_accuracy: 0.9634 - val_cost: 283.6250\n",
            "Epoch 22/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1062 - auc: 0.9912 - accuracy: 0.9666 - cost: 273.3125 - val_loss: 0.1196 - val_auc: 0.9899 - val_accuracy: 0.9635 - val_cost: 279.1250\n",
            "Epoch 23/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1065 - auc: 0.9913 - accuracy: 0.9670 - cost: 268.7500 - val_loss: 0.1154 - val_auc: 0.9899 - val_accuracy: 0.9646 - val_cost: 292.6250\n",
            "Epoch 24/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1041 - auc: 0.9915 - accuracy: 0.9677 - cost: 264.4375 - val_loss: 0.1172 - val_auc: 0.9897 - val_accuracy: 0.9642 - val_cost: 274.8750\n",
            "Epoch 25/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1033 - auc: 0.9914 - accuracy: 0.9680 - cost: 262.2500 - val_loss: 0.1150 - val_auc: 0.9896 - val_accuracy: 0.9663 - val_cost: 278.7500\n",
            "Epoch 26/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1022 - auc: 0.9916 - accuracy: 0.9683 - cost: 260.0625 - val_loss: 0.1128 - val_auc: 0.9901 - val_accuracy: 0.9653 - val_cost: 278.6250\n",
            "Epoch 27/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1001 - auc: 0.9918 - accuracy: 0.9696 - cost: 249.3125 - val_loss: 0.1128 - val_auc: 0.9902 - val_accuracy: 0.9666 - val_cost: 268.7500\n",
            "Epoch 28/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1005 - auc: 0.9919 - accuracy: 0.9689 - cost: 254.8750 - val_loss: 0.1109 - val_auc: 0.9905 - val_accuracy: 0.9665 - val_cost: 274.7500\n",
            "Epoch 29/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0981 - auc: 0.9921 - accuracy: 0.9698 - cost: 247.9375 - val_loss: 0.1099 - val_auc: 0.9904 - val_accuracy: 0.9666 - val_cost: 268.3750\n",
            "Epoch 30/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0989 - auc: 0.9921 - accuracy: 0.9695 - cost: 250.4062 - val_loss: 0.1131 - val_auc: 0.9902 - val_accuracy: 0.9656 - val_cost: 281.2500\n",
            "Epoch 31/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0973 - auc: 0.9922 - accuracy: 0.9699 - cost: 246.8750 - val_loss: 0.1137 - val_auc: 0.9900 - val_accuracy: 0.9667 - val_cost: 273.7500\n",
            "Epoch 32/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0970 - auc: 0.9923 - accuracy: 0.9704 - cost: 243.0938 - val_loss: 0.1126 - val_auc: 0.9904 - val_accuracy: 0.9671 - val_cost: 263.6250\n",
            "Epoch 33/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0965 - auc: 0.9923 - accuracy: 0.9706 - cost: 240.9688 - val_loss: 0.1107 - val_auc: 0.9902 - val_accuracy: 0.9673 - val_cost: 269.3750\n",
            "Epoch 34/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0952 - auc: 0.9926 - accuracy: 0.9707 - cost: 241.0938 - val_loss: 0.1101 - val_auc: 0.9904 - val_accuracy: 0.9684 - val_cost: 257.8750\n",
            "Epoch 35/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0952 - auc: 0.9925 - accuracy: 0.9708 - cost: 239.9062 - val_loss: 0.1136 - val_auc: 0.9898 - val_accuracy: 0.9672 - val_cost: 270.5000\n",
            "Epoch 36/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0953 - auc: 0.9925 - accuracy: 0.9714 - cost: 234.1875 - val_loss: 0.1134 - val_auc: 0.9901 - val_accuracy: 0.9663 - val_cost: 272.2500\n",
            "Epoch 37/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0952 - auc: 0.9924 - accuracy: 0.9713 - cost: 235.5625 - val_loss: 0.1122 - val_auc: 0.9903 - val_accuracy: 0.9664 - val_cost: 270.5000\n",
            "Epoch 38/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0935 - auc: 0.9927 - accuracy: 0.9714 - cost: 234.2500 - val_loss: 0.1114 - val_auc: 0.9904 - val_accuracy: 0.9677 - val_cost: 254.5000\n",
            "Epoch 39/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0937 - auc: 0.9927 - accuracy: 0.9722 - cost: 228.6250 - val_loss: 0.1108 - val_auc: 0.9903 - val_accuracy: 0.9673 - val_cost: 263.8750\n",
            "Epoch 40/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0940 - auc: 0.9928 - accuracy: 0.9712 - cost: 237.0625 - val_loss: 0.1095 - val_auc: 0.9904 - val_accuracy: 0.9688 - val_cost: 250.8750\n",
            "Epoch 41/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0925 - auc: 0.9927 - accuracy: 0.9725 - cost: 225.5000 - val_loss: 0.1093 - val_auc: 0.9906 - val_accuracy: 0.9683 - val_cost: 259.0000\n",
            "Epoch 42/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0920 - auc: 0.9929 - accuracy: 0.9723 - cost: 227.0938 - val_loss: 0.1108 - val_auc: 0.9902 - val_accuracy: 0.9677 - val_cost: 260.0000\n",
            "Epoch 43/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0930 - auc: 0.9928 - accuracy: 0.9723 - cost: 228.0312 - val_loss: 0.1118 - val_auc: 0.9901 - val_accuracy: 0.9667 - val_cost: 270.7500\n",
            "Epoch 44/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0915 - auc: 0.9931 - accuracy: 0.9723 - cost: 227.6250 - val_loss: 0.1139 - val_auc: 0.9898 - val_accuracy: 0.9683 - val_cost: 255.5000\n",
            "Epoch 45/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0916 - auc: 0.9929 - accuracy: 0.9726 - cost: 225.1250 - val_loss: 0.1116 - val_auc: 0.9901 - val_accuracy: 0.9668 - val_cost: 269.1250\n",
            "Epoch 46/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0895 - auc: 0.9932 - accuracy: 0.9729 - cost: 222.7188 - val_loss: 0.1116 - val_auc: 0.9902 - val_accuracy: 0.9684 - val_cost: 260.1250\n",
            "Epoch 47/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0902 - auc: 0.9931 - accuracy: 0.9731 - cost: 220.4062 - val_loss: 0.1100 - val_auc: 0.9902 - val_accuracy: 0.9682 - val_cost: 258.7500\n",
            "Epoch 48/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0899 - auc: 0.9932 - accuracy: 0.9726 - cost: 224.7500 - val_loss: 0.1108 - val_auc: 0.9901 - val_accuracy: 0.9681 - val_cost: 260.7500\n",
            "Epoch 49/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0877 - auc: 0.9935 - accuracy: 0.9736 - cost: 216.1875 - val_loss: 0.1105 - val_auc: 0.9903 - val_accuracy: 0.9682 - val_cost: 257.5000\n",
            "Epoch 50/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0890 - auc: 0.9932 - accuracy: 0.9735 - cost: 217.5625 - val_loss: 0.1092 - val_auc: 0.9905 - val_accuracy: 0.9687 - val_cost: 251.7500\n",
            "Epoch 51/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0880 - auc: 0.9933 - accuracy: 0.9741 - cost: 213.3438 - val_loss: 0.1092 - val_auc: 0.9902 - val_accuracy: 0.9690 - val_cost: 245.6250\n",
            "Epoch 52/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0880 - auc: 0.9935 - accuracy: 0.9736 - cost: 217.6562 - val_loss: 0.1114 - val_auc: 0.9901 - val_accuracy: 0.9681 - val_cost: 260.3750\n",
            "Epoch 53/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0882 - auc: 0.9933 - accuracy: 0.9740 - cost: 214.6875 - val_loss: 0.1086 - val_auc: 0.9903 - val_accuracy: 0.9680 - val_cost: 265.0000\n",
            "Epoch 54/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0889 - auc: 0.9933 - accuracy: 0.9735 - cost: 218.6562 - val_loss: 0.1104 - val_auc: 0.9898 - val_accuracy: 0.9690 - val_cost: 255.8750\n",
            "Epoch 55/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0885 - auc: 0.9933 - accuracy: 0.9736 - cost: 218.1250 - val_loss: 0.1129 - val_auc: 0.9901 - val_accuracy: 0.9684 - val_cost: 251.5000\n",
            "Epoch 56/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0870 - auc: 0.9935 - accuracy: 0.9739 - cost: 214.7188 - val_loss: 0.1101 - val_auc: 0.9904 - val_accuracy: 0.9688 - val_cost: 255.7500\n",
            "Epoch 57/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0859 - auc: 0.9936 - accuracy: 0.9746 - cost: 209.4062 - val_loss: 0.1107 - val_auc: 0.9901 - val_accuracy: 0.9686 - val_cost: 251.7500\n",
            "Epoch 58/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0863 - auc: 0.9936 - accuracy: 0.9742 - cost: 212.2188 - val_loss: 0.1087 - val_auc: 0.9904 - val_accuracy: 0.9685 - val_cost: 254.5000\n",
            "Epoch 59/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0860 - auc: 0.9935 - accuracy: 0.9743 - cost: 211.4688 - val_loss: 0.1105 - val_auc: 0.9901 - val_accuracy: 0.9684 - val_cost: 258.3750\n",
            "Epoch 60/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0860 - auc: 0.9937 - accuracy: 0.9746 - cost: 208.4688 - val_loss: 0.1110 - val_auc: 0.9899 - val_accuracy: 0.9690 - val_cost: 253.2500\n",
            "Epoch 61/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0855 - auc: 0.9937 - accuracy: 0.9744 - cost: 210.6875 - val_loss: 0.1140 - val_auc: 0.9898 - val_accuracy: 0.9692 - val_cost: 242.1250\n",
            "Epoch 62/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0846 - auc: 0.9938 - accuracy: 0.9746 - cost: 209.1875 - val_loss: 0.1120 - val_auc: 0.9902 - val_accuracy: 0.9686 - val_cost: 249.6250\n",
            "Epoch 63/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0862 - auc: 0.9936 - accuracy: 0.9744 - cost: 210.6250 - val_loss: 0.1124 - val_auc: 0.9900 - val_accuracy: 0.9679 - val_cost: 265.3750\n",
            "Epoch 64/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0838 - auc: 0.9938 - accuracy: 0.9754 - cost: 203.2500 - val_loss: 0.1137 - val_auc: 0.9900 - val_accuracy: 0.9687 - val_cost: 254.5000\n",
            "Epoch 65/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0828 - auc: 0.9941 - accuracy: 0.9753 - cost: 203.3125 - val_loss: 0.1094 - val_auc: 0.9906 - val_accuracy: 0.9702 - val_cost: 241.6250\n",
            "Epoch 66/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0846 - auc: 0.9938 - accuracy: 0.9746 - cost: 209.6562 - val_loss: 0.1119 - val_auc: 0.9902 - val_accuracy: 0.9695 - val_cost: 240.5000\n",
            "Epoch 67/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0834 - auc: 0.9939 - accuracy: 0.9753 - cost: 203.4375 - val_loss: 0.1082 - val_auc: 0.9906 - val_accuracy: 0.9699 - val_cost: 244.1250\n",
            "Epoch 68/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0844 - auc: 0.9937 - accuracy: 0.9752 - cost: 203.7812 - val_loss: 0.1115 - val_auc: 0.9904 - val_accuracy: 0.9690 - val_cost: 240.6250\n",
            "Epoch 69/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0826 - auc: 0.9940 - accuracy: 0.9751 - cost: 204.9062 - val_loss: 0.1137 - val_auc: 0.9902 - val_accuracy: 0.9679 - val_cost: 261.2500\n",
            "Epoch 70/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0831 - auc: 0.9940 - accuracy: 0.9753 - cost: 203.1875 - val_loss: 0.1104 - val_auc: 0.9902 - val_accuracy: 0.9697 - val_cost: 240.2500\n",
            "Epoch 71/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0823 - auc: 0.9940 - accuracy: 0.9761 - cost: 196.7188 - val_loss: 0.1095 - val_auc: 0.9905 - val_accuracy: 0.9695 - val_cost: 244.1250\n",
            "Epoch 72/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0832 - auc: 0.9938 - accuracy: 0.9757 - cost: 199.1875 - val_loss: 0.1079 - val_auc: 0.9905 - val_accuracy: 0.9689 - val_cost: 251.3750\n",
            "Epoch 73/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0826 - auc: 0.9939 - accuracy: 0.9754 - cost: 203.3438 - val_loss: 0.1102 - val_auc: 0.9900 - val_accuracy: 0.9689 - val_cost: 255.5000\n",
            "Epoch 74/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0818 - auc: 0.9942 - accuracy: 0.9755 - cost: 201.3125 - val_loss: 0.1091 - val_auc: 0.9905 - val_accuracy: 0.9694 - val_cost: 248.3750\n",
            "Epoch 75/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0821 - auc: 0.9940 - accuracy: 0.9759 - cost: 198.3438 - val_loss: 0.1128 - val_auc: 0.9898 - val_accuracy: 0.9688 - val_cost: 249.8750\n",
            "Epoch 76/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0812 - auc: 0.9941 - accuracy: 0.9756 - cost: 201.4062 - val_loss: 0.1119 - val_auc: 0.9901 - val_accuracy: 0.9688 - val_cost: 250.5000\n",
            "Epoch 77/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0799 - auc: 0.9942 - accuracy: 0.9761 - cost: 196.9375 - val_loss: 0.1134 - val_auc: 0.9899 - val_accuracy: 0.9682 - val_cost: 254.0000\n",
            "Epoch 78/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0819 - auc: 0.9941 - accuracy: 0.9758 - cost: 199.9688 - val_loss: 0.1148 - val_auc: 0.9898 - val_accuracy: 0.9673 - val_cost: 259.7500\n",
            "Epoch 79/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0818 - auc: 0.9939 - accuracy: 0.9763 - cost: 196.0625 - val_loss: 0.1099 - val_auc: 0.9905 - val_accuracy: 0.9690 - val_cost: 254.7500\n",
            "Epoch 80/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0814 - auc: 0.9941 - accuracy: 0.9763 - cost: 196.1250 - val_loss: 0.1090 - val_auc: 0.9907 - val_accuracy: 0.9706 - val_cost: 240.6250\n",
            "1000/1000 [==============================] - 1s 956us/step\n",
            "fold train/predict time: 0:06:08.935634\n",
            "x_train shape: (128000, 67)\n",
            "Epoch 1/100\n",
            "1600/1600 [==============================] - 6s 3ms/step - loss: 0.3810 - auc: 0.9062 - accuracy: 0.8298 - cost: 1404.8750 - val_loss: 0.2841 - val_auc: 0.9504 - val_accuracy: 0.8835 - val_cost: 912.0000\n",
            "Epoch 2/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.2653 - auc: 0.9556 - accuracy: 0.8937 - cost: 859.6562 - val_loss: 0.2307 - val_auc: 0.9667 - val_accuracy: 0.9092 - val_cost: 742.1250\n",
            "Epoch 3/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.2229 - auc: 0.9685 - accuracy: 0.9152 - cost: 688.1875 - val_loss: 0.1985 - val_auc: 0.9749 - val_accuracy: 0.9247 - val_cost: 610.5000\n",
            "Epoch 4/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1978 - auc: 0.9749 - accuracy: 0.9261 - cost: 600.5312 - val_loss: 0.1789 - val_auc: 0.9794 - val_accuracy: 0.9332 - val_cost: 540.2500\n",
            "Epoch 5/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1796 - auc: 0.9791 - accuracy: 0.9345 - cost: 533.4062 - val_loss: 0.1681 - val_auc: 0.9817 - val_accuracy: 0.9407 - val_cost: 473.1250\n",
            "Epoch 6/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1672 - auc: 0.9817 - accuracy: 0.9402 - cost: 486.9375 - val_loss: 0.1593 - val_auc: 0.9834 - val_accuracy: 0.9436 - val_cost: 475.1250\n",
            "Epoch 7/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1579 - auc: 0.9836 - accuracy: 0.9443 - cost: 453.1562 - val_loss: 0.1536 - val_auc: 0.9843 - val_accuracy: 0.9461 - val_cost: 434.7500\n",
            "Epoch 8/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1507 - auc: 0.9847 - accuracy: 0.9478 - cost: 425.2188 - val_loss: 0.1483 - val_auc: 0.9852 - val_accuracy: 0.9486 - val_cost: 416.8750\n",
            "Epoch 9/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1470 - auc: 0.9854 - accuracy: 0.9499 - cost: 409.0938 - val_loss: 0.1443 - val_auc: 0.9858 - val_accuracy: 0.9511 - val_cost: 400.1250\n",
            "Epoch 10/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1415 - auc: 0.9863 - accuracy: 0.9519 - cost: 392.8125 - val_loss: 0.1430 - val_auc: 0.9860 - val_accuracy: 0.9510 - val_cost: 401.8750\n",
            "Epoch 11/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1376 - auc: 0.9870 - accuracy: 0.9530 - cost: 383.2500 - val_loss: 0.1402 - val_auc: 0.9867 - val_accuracy: 0.9520 - val_cost: 405.3750\n",
            "Epoch 12/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1339 - auc: 0.9875 - accuracy: 0.9553 - cost: 365.8438 - val_loss: 0.1392 - val_auc: 0.9870 - val_accuracy: 0.9530 - val_cost: 363.8750\n",
            "Epoch 13/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1306 - auc: 0.9879 - accuracy: 0.9569 - cost: 352.4375 - val_loss: 0.1348 - val_auc: 0.9874 - val_accuracy: 0.9565 - val_cost: 346.8750\n",
            "Epoch 14/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1290 - auc: 0.9882 - accuracy: 0.9575 - cost: 346.8125 - val_loss: 0.1322 - val_auc: 0.9878 - val_accuracy: 0.9572 - val_cost: 342.6250\n",
            "Epoch 15/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1249 - auc: 0.9889 - accuracy: 0.9589 - cost: 336.5938 - val_loss: 0.1315 - val_auc: 0.9878 - val_accuracy: 0.9582 - val_cost: 342.1250\n",
            "Epoch 16/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1220 - auc: 0.9892 - accuracy: 0.9602 - cost: 324.5625 - val_loss: 0.1316 - val_auc: 0.9878 - val_accuracy: 0.9579 - val_cost: 336.0000\n",
            "Epoch 17/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1209 - auc: 0.9894 - accuracy: 0.9605 - cost: 324.1875 - val_loss: 0.1273 - val_auc: 0.9883 - val_accuracy: 0.9593 - val_cost: 330.2500\n",
            "Epoch 18/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1202 - auc: 0.9895 - accuracy: 0.9613 - cost: 317.7500 - val_loss: 0.1266 - val_auc: 0.9883 - val_accuracy: 0.9610 - val_cost: 312.0000\n",
            "Epoch 19/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1173 - auc: 0.9899 - accuracy: 0.9621 - cost: 310.7812 - val_loss: 0.1269 - val_auc: 0.9885 - val_accuracy: 0.9600 - val_cost: 322.1250\n",
            "Epoch 20/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1157 - auc: 0.9901 - accuracy: 0.9625 - cost: 307.4688 - val_loss: 0.1247 - val_auc: 0.9888 - val_accuracy: 0.9604 - val_cost: 321.2500\n",
            "Epoch 21/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1130 - auc: 0.9905 - accuracy: 0.9631 - cost: 302.0000 - val_loss: 0.1260 - val_auc: 0.9885 - val_accuracy: 0.9603 - val_cost: 320.0000\n",
            "Epoch 22/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1129 - auc: 0.9905 - accuracy: 0.9639 - cost: 296.4062 - val_loss: 0.1245 - val_auc: 0.9885 - val_accuracy: 0.9613 - val_cost: 322.0000\n",
            "Epoch 23/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1131 - auc: 0.9904 - accuracy: 0.9640 - cost: 296.4375 - val_loss: 0.1291 - val_auc: 0.9883 - val_accuracy: 0.9604 - val_cost: 333.0000\n",
            "Epoch 24/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1099 - auc: 0.9909 - accuracy: 0.9653 - cost: 284.9062 - val_loss: 0.1265 - val_auc: 0.9885 - val_accuracy: 0.9612 - val_cost: 321.6250\n",
            "Epoch 25/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1095 - auc: 0.9909 - accuracy: 0.9655 - cost: 283.8438 - val_loss: 0.1241 - val_auc: 0.9885 - val_accuracy: 0.9619 - val_cost: 310.3750\n",
            "Epoch 26/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1089 - auc: 0.9910 - accuracy: 0.9653 - cost: 284.3125 - val_loss: 0.1208 - val_auc: 0.9891 - val_accuracy: 0.9627 - val_cost: 304.5000\n",
            "Epoch 27/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1067 - auc: 0.9910 - accuracy: 0.9663 - cost: 277.3750 - val_loss: 0.1217 - val_auc: 0.9890 - val_accuracy: 0.9627 - val_cost: 298.1250\n",
            "Epoch 28/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1067 - auc: 0.9912 - accuracy: 0.9666 - cost: 275.1250 - val_loss: 0.1269 - val_auc: 0.9884 - val_accuracy: 0.9620 - val_cost: 306.2500\n",
            "Epoch 29/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1070 - auc: 0.9913 - accuracy: 0.9665 - cost: 276.3750 - val_loss: 0.1239 - val_auc: 0.9887 - val_accuracy: 0.9621 - val_cost: 302.7500\n",
            "Epoch 30/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1047 - auc: 0.9914 - accuracy: 0.9672 - cost: 269.8125 - val_loss: 0.1267 - val_auc: 0.9884 - val_accuracy: 0.9627 - val_cost: 304.8750\n",
            "Epoch 31/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1044 - auc: 0.9914 - accuracy: 0.9680 - cost: 262.8750 - val_loss: 0.1253 - val_auc: 0.9889 - val_accuracy: 0.9632 - val_cost: 286.1250\n",
            "Epoch 32/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1028 - auc: 0.9917 - accuracy: 0.9677 - cost: 265.6250 - val_loss: 0.1193 - val_auc: 0.9893 - val_accuracy: 0.9646 - val_cost: 291.3750\n",
            "Epoch 33/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1027 - auc: 0.9918 - accuracy: 0.9680 - cost: 263.4062 - val_loss: 0.1220 - val_auc: 0.9890 - val_accuracy: 0.9635 - val_cost: 294.3750\n",
            "Epoch 34/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1019 - auc: 0.9918 - accuracy: 0.9681 - cost: 262.5312 - val_loss: 0.1200 - val_auc: 0.9891 - val_accuracy: 0.9641 - val_cost: 298.1250\n",
            "Epoch 35/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1002 - auc: 0.9921 - accuracy: 0.9681 - cost: 262.6562 - val_loss: 0.1254 - val_auc: 0.9889 - val_accuracy: 0.9635 - val_cost: 282.3750\n",
            "Epoch 36/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0998 - auc: 0.9921 - accuracy: 0.9687 - cost: 257.3438 - val_loss: 0.1241 - val_auc: 0.9890 - val_accuracy: 0.9625 - val_cost: 310.8750\n",
            "Epoch 37/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0995 - auc: 0.9921 - accuracy: 0.9688 - cost: 257.1250 - val_loss: 0.1231 - val_auc: 0.9891 - val_accuracy: 0.9633 - val_cost: 297.8750\n",
            "Epoch 38/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0977 - auc: 0.9923 - accuracy: 0.9700 - cost: 247.3125 - val_loss: 0.1190 - val_auc: 0.9892 - val_accuracy: 0.9654 - val_cost: 282.1250\n",
            "Epoch 39/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0969 - auc: 0.9925 - accuracy: 0.9698 - cost: 248.4375 - val_loss: 0.1258 - val_auc: 0.9890 - val_accuracy: 0.9623 - val_cost: 320.2500\n",
            "Epoch 40/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0977 - auc: 0.9923 - accuracy: 0.9699 - cost: 247.5938 - val_loss: 0.1252 - val_auc: 0.9892 - val_accuracy: 0.9641 - val_cost: 280.5000\n",
            "Epoch 41/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0969 - auc: 0.9924 - accuracy: 0.9699 - cost: 248.5000 - val_loss: 0.1222 - val_auc: 0.9893 - val_accuracy: 0.9646 - val_cost: 293.5000\n",
            "Epoch 42/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0974 - auc: 0.9922 - accuracy: 0.9702 - cost: 246.0938 - val_loss: 0.1206 - val_auc: 0.9894 - val_accuracy: 0.9642 - val_cost: 288.7500\n",
            "Epoch 43/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0955 - auc: 0.9926 - accuracy: 0.9706 - cost: 242.4062 - val_loss: 0.1223 - val_auc: 0.9893 - val_accuracy: 0.9645 - val_cost: 299.5000\n",
            "Epoch 44/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0945 - auc: 0.9927 - accuracy: 0.9711 - cost: 238.5312 - val_loss: 0.1213 - val_auc: 0.9891 - val_accuracy: 0.9650 - val_cost: 286.1250\n",
            "Epoch 45/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0954 - auc: 0.9925 - accuracy: 0.9713 - cost: 237.0938 - val_loss: 0.1180 - val_auc: 0.9895 - val_accuracy: 0.9655 - val_cost: 278.2500\n",
            "Epoch 46/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0947 - auc: 0.9927 - accuracy: 0.9707 - cost: 241.4375 - val_loss: 0.1182 - val_auc: 0.9895 - val_accuracy: 0.9660 - val_cost: 271.0000\n",
            "Epoch 47/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0946 - auc: 0.9927 - accuracy: 0.9703 - cost: 244.5312 - val_loss: 0.1194 - val_auc: 0.9895 - val_accuracy: 0.9659 - val_cost: 276.5000\n",
            "Epoch 48/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0943 - auc: 0.9927 - accuracy: 0.9708 - cost: 240.7188 - val_loss: 0.1183 - val_auc: 0.9895 - val_accuracy: 0.9658 - val_cost: 278.1250\n",
            "Epoch 49/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0925 - auc: 0.9930 - accuracy: 0.9715 - cost: 235.4062 - val_loss: 0.1240 - val_auc: 0.9897 - val_accuracy: 0.9641 - val_cost: 294.3750\n",
            "Epoch 50/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0924 - auc: 0.9928 - accuracy: 0.9719 - cost: 231.9062 - val_loss: 0.1195 - val_auc: 0.9898 - val_accuracy: 0.9661 - val_cost: 273.0000\n",
            "Epoch 51/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0926 - auc: 0.9930 - accuracy: 0.9716 - cost: 233.3438 - val_loss: 0.1227 - val_auc: 0.9893 - val_accuracy: 0.9654 - val_cost: 280.7500\n",
            "Epoch 52/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0913 - auc: 0.9931 - accuracy: 0.9723 - cost: 228.1250 - val_loss: 0.1204 - val_auc: 0.9891 - val_accuracy: 0.9648 - val_cost: 289.8750\n",
            "Epoch 53/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0907 - auc: 0.9931 - accuracy: 0.9727 - cost: 225.0000 - val_loss: 0.1197 - val_auc: 0.9895 - val_accuracy: 0.9654 - val_cost: 276.1250\n",
            "Epoch 54/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0921 - auc: 0.9928 - accuracy: 0.9720 - cost: 230.6875 - val_loss: 0.1174 - val_auc: 0.9899 - val_accuracy: 0.9663 - val_cost: 278.2500\n",
            "Epoch 55/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0898 - auc: 0.9932 - accuracy: 0.9728 - cost: 224.2812 - val_loss: 0.1198 - val_auc: 0.9893 - val_accuracy: 0.9659 - val_cost: 276.2500\n",
            "Epoch 56/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0910 - auc: 0.9930 - accuracy: 0.9728 - cost: 224.7812 - val_loss: 0.1179 - val_auc: 0.9900 - val_accuracy: 0.9671 - val_cost: 269.0000\n",
            "Epoch 57/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0884 - auc: 0.9933 - accuracy: 0.9731 - cost: 220.6250 - val_loss: 0.1169 - val_auc: 0.9898 - val_accuracy: 0.9668 - val_cost: 271.7500\n",
            "Epoch 58/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0877 - auc: 0.9934 - accuracy: 0.9734 - cost: 218.5312 - val_loss: 0.1171 - val_auc: 0.9897 - val_accuracy: 0.9671 - val_cost: 267.2500\n",
            "Epoch 59/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0886 - auc: 0.9933 - accuracy: 0.9730 - cost: 222.0938 - val_loss: 0.1185 - val_auc: 0.9893 - val_accuracy: 0.9661 - val_cost: 279.0000\n",
            "Epoch 60/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0884 - auc: 0.9934 - accuracy: 0.9731 - cost: 221.6250 - val_loss: 0.1195 - val_auc: 0.9896 - val_accuracy: 0.9660 - val_cost: 269.3750\n",
            "Epoch 61/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0886 - auc: 0.9932 - accuracy: 0.9730 - cost: 222.9375 - val_loss: 0.1184 - val_auc: 0.9896 - val_accuracy: 0.9657 - val_cost: 276.0000\n",
            "Epoch 62/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0883 - auc: 0.9934 - accuracy: 0.9732 - cost: 220.8438 - val_loss: 0.1240 - val_auc: 0.9891 - val_accuracy: 0.9648 - val_cost: 285.5000\n",
            "Epoch 63/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0876 - auc: 0.9934 - accuracy: 0.9735 - cost: 217.9062 - val_loss: 0.1204 - val_auc: 0.9898 - val_accuracy: 0.9652 - val_cost: 287.2500\n",
            "Epoch 64/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0884 - auc: 0.9933 - accuracy: 0.9729 - cost: 223.3750 - val_loss: 0.1182 - val_auc: 0.9898 - val_accuracy: 0.9667 - val_cost: 273.6250\n",
            "Epoch 65/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0866 - auc: 0.9936 - accuracy: 0.9736 - cost: 218.3125 - val_loss: 0.1198 - val_auc: 0.9897 - val_accuracy: 0.9658 - val_cost: 273.5000\n",
            "Epoch 66/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0885 - auc: 0.9933 - accuracy: 0.9730 - cost: 222.9688 - val_loss: 0.1185 - val_auc: 0.9898 - val_accuracy: 0.9669 - val_cost: 265.7500\n",
            "Epoch 67/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0865 - auc: 0.9938 - accuracy: 0.9738 - cost: 216.0000 - val_loss: 0.1203 - val_auc: 0.9898 - val_accuracy: 0.9662 - val_cost: 273.3750\n",
            "Epoch 68/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0863 - auc: 0.9936 - accuracy: 0.9744 - cost: 211.2812 - val_loss: 0.1211 - val_auc: 0.9895 - val_accuracy: 0.9679 - val_cost: 265.3750\n",
            "Epoch 69/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0859 - auc: 0.9937 - accuracy: 0.9738 - cost: 216.4688 - val_loss: 0.1219 - val_auc: 0.9896 - val_accuracy: 0.9658 - val_cost: 276.2500\n",
            "Epoch 70/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0860 - auc: 0.9936 - accuracy: 0.9738 - cost: 215.5312 - val_loss: 0.1208 - val_auc: 0.9894 - val_accuracy: 0.9661 - val_cost: 270.2500\n",
            "Epoch 71/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0858 - auc: 0.9937 - accuracy: 0.9747 - cost: 209.8438 - val_loss: 0.1242 - val_auc: 0.9890 - val_accuracy: 0.9651 - val_cost: 280.8750\n",
            "Epoch 72/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0855 - auc: 0.9937 - accuracy: 0.9745 - cost: 209.9062 - val_loss: 0.1243 - val_auc: 0.9893 - val_accuracy: 0.9650 - val_cost: 282.8750\n",
            "Epoch 73/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0845 - auc: 0.9938 - accuracy: 0.9748 - cost: 207.7188 - val_loss: 0.1222 - val_auc: 0.9893 - val_accuracy: 0.9659 - val_cost: 270.1250\n",
            "Epoch 74/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0844 - auc: 0.9937 - accuracy: 0.9749 - cost: 206.5000 - val_loss: 0.1215 - val_auc: 0.9896 - val_accuracy: 0.9665 - val_cost: 266.5000\n",
            "Epoch 75/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0855 - auc: 0.9937 - accuracy: 0.9740 - cost: 215.3750 - val_loss: 0.1196 - val_auc: 0.9897 - val_accuracy: 0.9668 - val_cost: 266.0000\n",
            "Epoch 76/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0840 - auc: 0.9937 - accuracy: 0.9750 - cost: 206.8125 - val_loss: 0.1211 - val_auc: 0.9897 - val_accuracy: 0.9667 - val_cost: 273.8750\n",
            "Epoch 77/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0846 - auc: 0.9937 - accuracy: 0.9742 - cost: 211.8750 - val_loss: 0.1211 - val_auc: 0.9896 - val_accuracy: 0.9663 - val_cost: 268.8750\n",
            "Epoch 78/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0843 - auc: 0.9939 - accuracy: 0.9748 - cost: 208.3750 - val_loss: 0.1249 - val_auc: 0.9894 - val_accuracy: 0.9659 - val_cost: 278.8750\n",
            "1000/1000 [==============================] - 1s 937us/step\n",
            "fold train/predict time: 0:06:01.382580\n",
            "total train/predict time: 0:28:09.327746\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m4_results = fold_results"
      ],
      "metadata": {
        "id": "fszFGfDwmD7r"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds_m4 = np.zeros(len(y))\n",
        "for i in fold_results.keys():\n",
        "  for j in range(len(fold_results.get(i).get('predictions'))):\n",
        "    idx = fold_results.get(i).get('index')[j]\n",
        "    preds_m4[idx] = np.round(fold_results.get(i).get('predictions')[j],0)\n",
        "m4_cost = cost_func(y,preds_m4)\n",
        "m4_cost"
      ],
      "metadata": {
        "id": "WMZkaNGNeKT0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16d4193c-ca83-4b45-c28a-c214fd80db00"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "628350"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds_new = np.zeros(len(y))\n",
        "for i in m4_results.keys():\n",
        "  for j in range(len(m4_results.get(i).get('predictions'))):\n",
        "    idx = m4_results.get(i).get('index')[j]\n",
        "    if m4_results.get(i).get('predictions')[j] > 0.6:\n",
        "      preds_new[idx] = 1\n",
        "    else:\n",
        "      preds_new[idx] = 0\n",
        "m4_cost_t = cost_func(y,preds_new)\n",
        "m4_cost_t"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3X7K7DWxmHit",
        "outputId": "3c310187-f349-43a0-9766-f6d4b952e338"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "614800"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model4.summary()\n"
      ],
      "metadata": {
        "id": "7jg2oQFvH2QS",
        "outputId": "806201ac-b2b3-4647-9b37-fd9f1228f94d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " Relu1 (Dense)               (None, 64)                4352      \n",
            "                                                                 \n",
            " Relu2 (Dense)               (None, 64)                4160      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " Activation (Dense)          (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8,577\n",
            "Trainable params: 8,577\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "es = EarlyStopping(monitor='val_cost', mode='min',patience=15,restore_best_weights=True,start_from_epoch=40)\n",
        "fold_results = {}\n",
        "t_tot = now()\n",
        "for i, (train_index, test_index) in enumerate(skf.split(X,y)):\n",
        "  # building model within loop to reset weights each time\n",
        "  tf.keras.backend.clear_session()\n",
        "  tf.keras.utils.set_random_seed(807)\n",
        "  #adamM5 = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "  model5 = tf.keras.Sequential()\n",
        "  model5.add(tf.keras.Input(shape=(X.shape[1],)))\n",
        "  model5.add(tf.keras.layers.Dense(64, activation='relu',name=\"Relu1\"))\n",
        "  model5.add(tf.keras.layers.Dense(64, activation='relu', name=\"Relu2\"))\n",
        "  #model5.add(tf.keras.layers.Dense(64, activation='relu', name=\"Relu3\"))\n",
        "  model5.add(tf.keras.layers.Dropout(0.4))\n",
        "  model5.add(tf.keras.layers.Dense(1, activation='sigmoid', name=\"Activation\"))\n",
        "  model5.compile(optimizer='adam',\n",
        "                loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "                metrics = [tf.keras.metrics.AUC(from_logits=False), 'accuracy',cost])\n",
        "  t_fold = now()\n",
        "  print('x_train shape:', X[train_index].shape)\n",
        "  model5.fit(X[train_index],y[train_index],epochs=100,batch_size=64,validation_split=0.2,callbacks=[es])\n",
        "  fold_results.update({i:{'predictions':model5.predict(X[test_index]).flatten(),'index':test_index,'y_true':y[test_index]}})\n",
        "  print('fold train/predict time: %s' % (now()-t_fold))\n",
        "print('total train/predict time: %s' % (now()-t_tot))"
      ],
      "metadata": {
        "id": "DVNOkJr3oYGb",
        "outputId": "b9191588-123d-47ed-c500-2cd375ab2c0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (128000, 67)\n",
            "Epoch 1/100\n",
            "1600/1600 [==============================] - 6s 3ms/step - loss: 0.3797 - auc: 0.9067 - accuracy: 0.8288 - cost: 1409.9375 - val_loss: 0.2842 - val_auc: 0.9492 - val_accuracy: 0.8815 - val_cost: 976.8750\n",
            "Epoch 2/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.2643 - auc: 0.9559 - accuracy: 0.8949 - cost: 852.9375 - val_loss: 0.2301 - val_auc: 0.9666 - val_accuracy: 0.9103 - val_cost: 716.6250\n",
            "Epoch 3/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.2248 - auc: 0.9680 - accuracy: 0.9130 - cost: 705.7812 - val_loss: 0.1989 - val_auc: 0.9748 - val_accuracy: 0.9248 - val_cost: 604.2500\n",
            "Epoch 4/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1995 - auc: 0.9747 - accuracy: 0.9264 - cost: 597.4688 - val_loss: 0.1786 - val_auc: 0.9797 - val_accuracy: 0.9346 - val_cost: 523.3750\n",
            "Epoch 5/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1824 - auc: 0.9785 - accuracy: 0.9340 - cost: 536.5625 - val_loss: 0.1676 - val_auc: 0.9816 - val_accuracy: 0.9396 - val_cost: 485.3750\n",
            "Epoch 6/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1706 - auc: 0.9810 - accuracy: 0.9391 - cost: 495.9688 - val_loss: 0.1583 - val_auc: 0.9834 - val_accuracy: 0.9443 - val_cost: 444.3750\n",
            "Epoch 7/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1628 - auc: 0.9826 - accuracy: 0.9421 - cost: 471.0938 - val_loss: 0.1557 - val_auc: 0.9839 - val_accuracy: 0.9442 - val_cost: 458.2500\n",
            "Epoch 8/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1565 - auc: 0.9837 - accuracy: 0.9448 - cost: 448.8438 - val_loss: 0.1483 - val_auc: 0.9851 - val_accuracy: 0.9487 - val_cost: 423.1250\n",
            "Epoch 9/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1514 - auc: 0.9847 - accuracy: 0.9479 - cost: 425.2188 - val_loss: 0.1460 - val_auc: 0.9857 - val_accuracy: 0.9504 - val_cost: 390.0000\n",
            "Epoch 10/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1458 - auc: 0.9857 - accuracy: 0.9502 - cost: 405.6250 - val_loss: 0.1424 - val_auc: 0.9862 - val_accuracy: 0.9526 - val_cost: 386.2500\n",
            "Epoch 11/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1434 - auc: 0.9861 - accuracy: 0.9508 - cost: 401.5312 - val_loss: 0.1385 - val_auc: 0.9866 - val_accuracy: 0.9548 - val_cost: 371.1250\n",
            "Epoch 12/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1396 - auc: 0.9868 - accuracy: 0.9530 - cost: 384.1875 - val_loss: 0.1358 - val_auc: 0.9873 - val_accuracy: 0.9559 - val_cost: 367.1250\n",
            "Epoch 13/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1367 - auc: 0.9873 - accuracy: 0.9549 - cost: 369.5938 - val_loss: 0.1358 - val_auc: 0.9871 - val_accuracy: 0.9550 - val_cost: 366.6250\n",
            "Epoch 14/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1315 - auc: 0.9880 - accuracy: 0.9567 - cost: 355.8750 - val_loss: 0.1301 - val_auc: 0.9878 - val_accuracy: 0.9580 - val_cost: 337.8750\n",
            "Epoch 15/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1303 - auc: 0.9880 - accuracy: 0.9569 - cost: 353.8125 - val_loss: 0.1320 - val_auc: 0.9877 - val_accuracy: 0.9582 - val_cost: 331.0000\n",
            "Epoch 16/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1270 - auc: 0.9886 - accuracy: 0.9576 - cost: 348.0000 - val_loss: 0.1256 - val_auc: 0.9885 - val_accuracy: 0.9602 - val_cost: 330.3750\n",
            "Epoch 17/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1240 - auc: 0.9890 - accuracy: 0.9587 - cost: 339.6875 - val_loss: 0.1263 - val_auc: 0.9885 - val_accuracy: 0.9602 - val_cost: 329.6250\n",
            "Epoch 18/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1234 - auc: 0.9891 - accuracy: 0.9597 - cost: 331.1875 - val_loss: 0.1271 - val_auc: 0.9884 - val_accuracy: 0.9606 - val_cost: 311.8750\n",
            "Epoch 19/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1208 - auc: 0.9894 - accuracy: 0.9606 - cost: 323.6562 - val_loss: 0.1226 - val_auc: 0.9888 - val_accuracy: 0.9630 - val_cost: 301.7500\n",
            "Epoch 20/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1169 - auc: 0.9899 - accuracy: 0.9623 - cost: 310.6875 - val_loss: 0.1212 - val_auc: 0.9891 - val_accuracy: 0.9632 - val_cost: 294.7500\n",
            "Epoch 21/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1177 - auc: 0.9899 - accuracy: 0.9622 - cost: 309.8438 - val_loss: 0.1213 - val_auc: 0.9889 - val_accuracy: 0.9623 - val_cost: 302.3750\n",
            "Epoch 22/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1142 - auc: 0.9904 - accuracy: 0.9631 - cost: 303.7188 - val_loss: 0.1221 - val_auc: 0.9888 - val_accuracy: 0.9628 - val_cost: 301.0000\n",
            "Epoch 23/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1128 - auc: 0.9905 - accuracy: 0.9638 - cost: 297.0000 - val_loss: 0.1191 - val_auc: 0.9895 - val_accuracy: 0.9639 - val_cost: 286.3750\n",
            "Epoch 24/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1108 - auc: 0.9908 - accuracy: 0.9648 - cost: 289.9375 - val_loss: 0.1195 - val_auc: 0.9892 - val_accuracy: 0.9631 - val_cost: 302.7500\n",
            "Epoch 25/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1104 - auc: 0.9907 - accuracy: 0.9650 - cost: 288.4688 - val_loss: 0.1183 - val_auc: 0.9894 - val_accuracy: 0.9639 - val_cost: 294.8750\n",
            "Epoch 26/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1104 - auc: 0.9908 - accuracy: 0.9648 - cost: 289.5625 - val_loss: 0.1176 - val_auc: 0.9896 - val_accuracy: 0.9643 - val_cost: 288.0000\n",
            "Epoch 27/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1064 - auc: 0.9912 - accuracy: 0.9661 - cost: 279.1875 - val_loss: 0.1167 - val_auc: 0.9896 - val_accuracy: 0.9639 - val_cost: 296.8750\n",
            "Epoch 28/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1074 - auc: 0.9912 - accuracy: 0.9662 - cost: 278.5938 - val_loss: 0.1155 - val_auc: 0.9897 - val_accuracy: 0.9648 - val_cost: 289.6250\n",
            "Epoch 29/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1065 - auc: 0.9913 - accuracy: 0.9664 - cost: 276.0938 - val_loss: 0.1151 - val_auc: 0.9897 - val_accuracy: 0.9661 - val_cost: 270.3750\n",
            "Epoch 30/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1044 - auc: 0.9916 - accuracy: 0.9669 - cost: 272.4688 - val_loss: 0.1143 - val_auc: 0.9896 - val_accuracy: 0.9651 - val_cost: 291.3750\n",
            "Epoch 31/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1034 - auc: 0.9916 - accuracy: 0.9674 - cost: 269.2188 - val_loss: 0.1174 - val_auc: 0.9894 - val_accuracy: 0.9663 - val_cost: 281.8750\n",
            "Epoch 32/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1027 - auc: 0.9917 - accuracy: 0.9682 - cost: 262.0000 - val_loss: 0.1135 - val_auc: 0.9899 - val_accuracy: 0.9666 - val_cost: 278.1250\n",
            "Epoch 33/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1018 - auc: 0.9918 - accuracy: 0.9683 - cost: 261.5000 - val_loss: 0.1150 - val_auc: 0.9895 - val_accuracy: 0.9665 - val_cost: 270.3750\n",
            "Epoch 34/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1013 - auc: 0.9917 - accuracy: 0.9686 - cost: 259.0938 - val_loss: 0.1183 - val_auc: 0.9894 - val_accuracy: 0.9652 - val_cost: 277.5000\n",
            "Epoch 35/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0998 - auc: 0.9920 - accuracy: 0.9687 - cost: 257.7500 - val_loss: 0.1157 - val_auc: 0.9895 - val_accuracy: 0.9664 - val_cost: 270.7500\n",
            "Epoch 36/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1002 - auc: 0.9920 - accuracy: 0.9689 - cost: 256.5312 - val_loss: 0.1152 - val_auc: 0.9896 - val_accuracy: 0.9667 - val_cost: 270.7500\n",
            "Epoch 37/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0997 - auc: 0.9921 - accuracy: 0.9686 - cost: 259.6562 - val_loss: 0.1139 - val_auc: 0.9897 - val_accuracy: 0.9665 - val_cost: 272.1250\n",
            "Epoch 38/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0987 - auc: 0.9922 - accuracy: 0.9693 - cost: 253.5938 - val_loss: 0.1128 - val_auc: 0.9901 - val_accuracy: 0.9664 - val_cost: 272.3750\n",
            "Epoch 39/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0983 - auc: 0.9921 - accuracy: 0.9701 - cost: 246.8750 - val_loss: 0.1140 - val_auc: 0.9899 - val_accuracy: 0.9668 - val_cost: 269.5000\n",
            "Epoch 40/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0974 - auc: 0.9923 - accuracy: 0.9696 - cost: 250.1562 - val_loss: 0.1177 - val_auc: 0.9895 - val_accuracy: 0.9649 - val_cost: 277.5000\n",
            "Epoch 41/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0977 - auc: 0.9922 - accuracy: 0.9699 - cost: 248.4375 - val_loss: 0.1133 - val_auc: 0.9897 - val_accuracy: 0.9679 - val_cost: 262.5000\n",
            "Epoch 42/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0965 - auc: 0.9924 - accuracy: 0.9707 - cost: 241.9062 - val_loss: 0.1133 - val_auc: 0.9899 - val_accuracy: 0.9677 - val_cost: 260.3750\n",
            "Epoch 43/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0948 - auc: 0.9925 - accuracy: 0.9714 - cost: 236.7188 - val_loss: 0.1164 - val_auc: 0.9896 - val_accuracy: 0.9664 - val_cost: 271.1250\n",
            "Epoch 44/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0945 - auc: 0.9924 - accuracy: 0.9709 - cost: 240.1562 - val_loss: 0.1119 - val_auc: 0.9901 - val_accuracy: 0.9680 - val_cost: 259.5000\n",
            "Epoch 45/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0951 - auc: 0.9926 - accuracy: 0.9706 - cost: 242.9375 - val_loss: 0.1113 - val_auc: 0.9904 - val_accuracy: 0.9675 - val_cost: 263.1250\n",
            "Epoch 46/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0941 - auc: 0.9927 - accuracy: 0.9716 - cost: 234.0625 - val_loss: 0.1111 - val_auc: 0.9902 - val_accuracy: 0.9676 - val_cost: 266.8750\n",
            "Epoch 47/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0943 - auc: 0.9927 - accuracy: 0.9712 - cost: 238.1875 - val_loss: 0.1119 - val_auc: 0.9900 - val_accuracy: 0.9676 - val_cost: 265.0000\n",
            "Epoch 48/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0940 - auc: 0.9927 - accuracy: 0.9711 - cost: 238.5000 - val_loss: 0.1122 - val_auc: 0.9904 - val_accuracy: 0.9673 - val_cost: 268.5000\n",
            "Epoch 49/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0925 - auc: 0.9928 - accuracy: 0.9718 - cost: 232.1562 - val_loss: 0.1120 - val_auc: 0.9899 - val_accuracy: 0.9679 - val_cost: 264.3750\n",
            "Epoch 50/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0929 - auc: 0.9928 - accuracy: 0.9717 - cost: 233.6250 - val_loss: 0.1135 - val_auc: 0.9901 - val_accuracy: 0.9681 - val_cost: 257.1250\n",
            "Epoch 51/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0915 - auc: 0.9929 - accuracy: 0.9723 - cost: 228.7188 - val_loss: 0.1124 - val_auc: 0.9902 - val_accuracy: 0.9684 - val_cost: 254.6250\n",
            "Epoch 52/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0912 - auc: 0.9930 - accuracy: 0.9726 - cost: 226.2812 - val_loss: 0.1104 - val_auc: 0.9901 - val_accuracy: 0.9683 - val_cost: 257.6250\n",
            "Epoch 53/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0917 - auc: 0.9929 - accuracy: 0.9718 - cost: 233.0625 - val_loss: 0.1135 - val_auc: 0.9898 - val_accuracy: 0.9667 - val_cost: 273.7500\n",
            "Epoch 54/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0911 - auc: 0.9929 - accuracy: 0.9726 - cost: 225.7500 - val_loss: 0.1122 - val_auc: 0.9899 - val_accuracy: 0.9673 - val_cost: 270.6250\n",
            "Epoch 55/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0905 - auc: 0.9930 - accuracy: 0.9722 - cost: 230.0938 - val_loss: 0.1119 - val_auc: 0.9903 - val_accuracy: 0.9677 - val_cost: 266.2500\n",
            "Epoch 56/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0895 - auc: 0.9932 - accuracy: 0.9729 - cost: 224.2812 - val_loss: 0.1100 - val_auc: 0.9903 - val_accuracy: 0.9690 - val_cost: 249.0000\n",
            "Epoch 57/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0888 - auc: 0.9933 - accuracy: 0.9732 - cost: 222.0938 - val_loss: 0.1089 - val_auc: 0.9906 - val_accuracy: 0.9685 - val_cost: 255.5000\n",
            "Epoch 58/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0900 - auc: 0.9930 - accuracy: 0.9733 - cost: 220.4062 - val_loss: 0.1106 - val_auc: 0.9903 - val_accuracy: 0.9674 - val_cost: 271.5000\n",
            "Epoch 59/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0884 - auc: 0.9933 - accuracy: 0.9730 - cost: 222.8125 - val_loss: 0.1108 - val_auc: 0.9903 - val_accuracy: 0.9681 - val_cost: 266.3750\n",
            "Epoch 60/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0889 - auc: 0.9932 - accuracy: 0.9727 - cost: 226.4062 - val_loss: 0.1124 - val_auc: 0.9902 - val_accuracy: 0.9684 - val_cost: 252.5000\n",
            "Epoch 61/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0883 - auc: 0.9932 - accuracy: 0.9734 - cost: 219.5938 - val_loss: 0.1138 - val_auc: 0.9900 - val_accuracy: 0.9683 - val_cost: 256.6250\n",
            "Epoch 62/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0886 - auc: 0.9933 - accuracy: 0.9732 - cost: 221.4062 - val_loss: 0.1129 - val_auc: 0.9904 - val_accuracy: 0.9687 - val_cost: 255.5000\n",
            "Epoch 63/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0870 - auc: 0.9935 - accuracy: 0.9736 - cost: 219.1875 - val_loss: 0.1126 - val_auc: 0.9899 - val_accuracy: 0.9671 - val_cost: 270.3750\n",
            "Epoch 64/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0859 - auc: 0.9936 - accuracy: 0.9743 - cost: 211.8125 - val_loss: 0.1116 - val_auc: 0.9902 - val_accuracy: 0.9683 - val_cost: 260.1250\n",
            "Epoch 65/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0878 - auc: 0.9932 - accuracy: 0.9736 - cost: 218.1875 - val_loss: 0.1127 - val_auc: 0.9901 - val_accuracy: 0.9673 - val_cost: 266.5000\n",
            "Epoch 66/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0858 - auc: 0.9935 - accuracy: 0.9734 - cost: 219.0312 - val_loss: 0.1138 - val_auc: 0.9902 - val_accuracy: 0.9681 - val_cost: 261.1250\n",
            "Epoch 67/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0870 - auc: 0.9933 - accuracy: 0.9737 - cost: 218.3125 - val_loss: 0.1137 - val_auc: 0.9903 - val_accuracy: 0.9686 - val_cost: 245.3750\n",
            "Epoch 68/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0857 - auc: 0.9936 - accuracy: 0.9744 - cost: 212.2812 - val_loss: 0.1131 - val_auc: 0.9901 - val_accuracy: 0.9674 - val_cost: 270.2500\n",
            "Epoch 69/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0862 - auc: 0.9935 - accuracy: 0.9740 - cost: 214.0312 - val_loss: 0.1119 - val_auc: 0.9902 - val_accuracy: 0.9683 - val_cost: 261.2500\n",
            "Epoch 70/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0856 - auc: 0.9936 - accuracy: 0.9748 - cost: 208.6562 - val_loss: 0.1150 - val_auc: 0.9899 - val_accuracy: 0.9674 - val_cost: 260.8750\n",
            "Epoch 71/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0861 - auc: 0.9934 - accuracy: 0.9745 - cost: 211.5312 - val_loss: 0.1133 - val_auc: 0.9900 - val_accuracy: 0.9683 - val_cost: 253.3750\n",
            "Epoch 72/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0849 - auc: 0.9937 - accuracy: 0.9748 - cost: 209.3125 - val_loss: 0.1114 - val_auc: 0.9902 - val_accuracy: 0.9680 - val_cost: 259.7500\n",
            "Epoch 73/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0834 - auc: 0.9938 - accuracy: 0.9753 - cost: 204.8125 - val_loss: 0.1146 - val_auc: 0.9901 - val_accuracy: 0.9694 - val_cost: 244.6250\n",
            "Epoch 74/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0860 - auc: 0.9935 - accuracy: 0.9747 - cost: 209.2500 - val_loss: 0.1130 - val_auc: 0.9901 - val_accuracy: 0.9690 - val_cost: 248.1250\n",
            "Epoch 75/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0837 - auc: 0.9938 - accuracy: 0.9749 - cost: 208.2812 - val_loss: 0.1146 - val_auc: 0.9900 - val_accuracy: 0.9684 - val_cost: 260.0000\n",
            "Epoch 76/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0838 - auc: 0.9938 - accuracy: 0.9746 - cost: 210.0000 - val_loss: 0.1118 - val_auc: 0.9904 - val_accuracy: 0.9685 - val_cost: 254.3750\n",
            "Epoch 77/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0840 - auc: 0.9939 - accuracy: 0.9747 - cost: 209.4688 - val_loss: 0.1156 - val_auc: 0.9897 - val_accuracy: 0.9690 - val_cost: 251.7500\n",
            "Epoch 78/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0846 - auc: 0.9937 - accuracy: 0.9745 - cost: 211.0000 - val_loss: 0.1135 - val_auc: 0.9900 - val_accuracy: 0.9690 - val_cost: 254.2500\n",
            "Epoch 79/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0832 - auc: 0.9939 - accuracy: 0.9754 - cost: 203.6875 - val_loss: 0.1148 - val_auc: 0.9898 - val_accuracy: 0.9679 - val_cost: 263.7500\n",
            "Epoch 80/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0837 - auc: 0.9937 - accuracy: 0.9751 - cost: 206.8438 - val_loss: 0.1144 - val_auc: 0.9898 - val_accuracy: 0.9677 - val_cost: 269.6250\n",
            "Epoch 81/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0837 - auc: 0.9939 - accuracy: 0.9753 - cost: 205.1875 - val_loss: 0.1111 - val_auc: 0.9902 - val_accuracy: 0.9692 - val_cost: 249.6250\n",
            "Epoch 82/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0824 - auc: 0.9940 - accuracy: 0.9750 - cost: 207.5312 - val_loss: 0.1127 - val_auc: 0.9900 - val_accuracy: 0.9684 - val_cost: 252.1250\n",
            "Epoch 83/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0836 - auc: 0.9937 - accuracy: 0.9754 - cost: 204.2188 - val_loss: 0.1127 - val_auc: 0.9901 - val_accuracy: 0.9692 - val_cost: 255.0000\n",
            "Epoch 84/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0814 - auc: 0.9940 - accuracy: 0.9754 - cost: 204.2500 - val_loss: 0.1147 - val_auc: 0.9900 - val_accuracy: 0.9690 - val_cost: 246.0000\n",
            "Epoch 85/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0816 - auc: 0.9940 - accuracy: 0.9754 - cost: 204.2812 - val_loss: 0.1169 - val_auc: 0.9898 - val_accuracy: 0.9678 - val_cost: 263.0000\n",
            "Epoch 86/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0823 - auc: 0.9939 - accuracy: 0.9756 - cost: 202.3750 - val_loss: 0.1147 - val_auc: 0.9899 - val_accuracy: 0.9676 - val_cost: 264.0000\n",
            "Epoch 87/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0834 - auc: 0.9938 - accuracy: 0.9751 - cost: 205.7500 - val_loss: 0.1131 - val_auc: 0.9898 - val_accuracy: 0.9686 - val_cost: 261.3750\n",
            "Epoch 88/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0831 - auc: 0.9938 - accuracy: 0.9753 - cost: 204.5312 - val_loss: 0.1133 - val_auc: 0.9903 - val_accuracy: 0.9685 - val_cost: 251.7500\n",
            "1000/1000 [==============================] - 1s 958us/step\n",
            "fold train/predict time: 0:06:54.335705\n",
            "x_train shape: (128000, 67)\n",
            "Epoch 1/100\n",
            "1600/1600 [==============================] - 6s 3ms/step - loss: 0.3809 - auc: 0.9059 - accuracy: 0.8286 - cost: 1410.5625 - val_loss: 0.2826 - val_auc: 0.9492 - val_accuracy: 0.8830 - val_cost: 944.2500\n",
            "Epoch 2/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.2631 - auc: 0.9563 - accuracy: 0.8960 - cost: 842.2500 - val_loss: 0.2275 - val_auc: 0.9671 - val_accuracy: 0.9093 - val_cost: 734.5000\n",
            "Epoch 3/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.2207 - auc: 0.9691 - accuracy: 0.9150 - cost: 691.1250 - val_loss: 0.1985 - val_auc: 0.9751 - val_accuracy: 0.9238 - val_cost: 636.2500\n",
            "Epoch 4/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1946 - auc: 0.9758 - accuracy: 0.9289 - cost: 580.3125 - val_loss: 0.1791 - val_auc: 0.9797 - val_accuracy: 0.9329 - val_cost: 562.0000\n",
            "Epoch 5/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1773 - auc: 0.9795 - accuracy: 0.9360 - cost: 521.9375 - val_loss: 0.1670 - val_auc: 0.9820 - val_accuracy: 0.9408 - val_cost: 466.5000\n",
            "Epoch 6/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1667 - auc: 0.9818 - accuracy: 0.9405 - cost: 484.5625 - val_loss: 0.1605 - val_auc: 0.9833 - val_accuracy: 0.9428 - val_cost: 485.5000\n",
            "Epoch 7/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1581 - auc: 0.9833 - accuracy: 0.9448 - cost: 450.6875 - val_loss: 0.1497 - val_auc: 0.9850 - val_accuracy: 0.9481 - val_cost: 409.6250\n",
            "Epoch 8/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1518 - auc: 0.9845 - accuracy: 0.9482 - cost: 422.1562 - val_loss: 0.1443 - val_auc: 0.9856 - val_accuracy: 0.9521 - val_cost: 383.5000\n",
            "Epoch 9/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1470 - auc: 0.9854 - accuracy: 0.9497 - cost: 409.6875 - val_loss: 0.1391 - val_auc: 0.9865 - val_accuracy: 0.9526 - val_cost: 379.6250\n",
            "Epoch 10/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1406 - auc: 0.9864 - accuracy: 0.9522 - cost: 390.5938 - val_loss: 0.1364 - val_auc: 0.9870 - val_accuracy: 0.9548 - val_cost: 373.2500\n",
            "Epoch 11/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1373 - auc: 0.9869 - accuracy: 0.9545 - cost: 371.4375 - val_loss: 0.1328 - val_auc: 0.9875 - val_accuracy: 0.9560 - val_cost: 356.2500\n",
            "Epoch 12/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1318 - auc: 0.9878 - accuracy: 0.9560 - cost: 359.3125 - val_loss: 0.1312 - val_auc: 0.9876 - val_accuracy: 0.9575 - val_cost: 343.3750\n",
            "Epoch 13/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1291 - auc: 0.9883 - accuracy: 0.9577 - cost: 344.8125 - val_loss: 0.1297 - val_auc: 0.9879 - val_accuracy: 0.9573 - val_cost: 339.8750\n",
            "Epoch 14/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1267 - auc: 0.9886 - accuracy: 0.9584 - cost: 340.1250 - val_loss: 0.1309 - val_auc: 0.9878 - val_accuracy: 0.9582 - val_cost: 344.7500\n",
            "Epoch 15/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1243 - auc: 0.9889 - accuracy: 0.9593 - cost: 331.6562 - val_loss: 0.1276 - val_auc: 0.9883 - val_accuracy: 0.9593 - val_cost: 327.5000\n",
            "Epoch 16/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1209 - auc: 0.9894 - accuracy: 0.9607 - cost: 322.1250 - val_loss: 0.1265 - val_auc: 0.9884 - val_accuracy: 0.9605 - val_cost: 321.5000\n",
            "Epoch 17/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1196 - auc: 0.9895 - accuracy: 0.9611 - cost: 318.2812 - val_loss: 0.1225 - val_auc: 0.9889 - val_accuracy: 0.9618 - val_cost: 309.6250\n",
            "Epoch 18/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1171 - auc: 0.9898 - accuracy: 0.9632 - cost: 300.4688 - val_loss: 0.1254 - val_auc: 0.9884 - val_accuracy: 0.9606 - val_cost: 319.5000\n",
            "Epoch 19/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1161 - auc: 0.9900 - accuracy: 0.9636 - cost: 298.5000 - val_loss: 0.1248 - val_auc: 0.9888 - val_accuracy: 0.9613 - val_cost: 308.1250\n",
            "Epoch 20/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1129 - auc: 0.9904 - accuracy: 0.9645 - cost: 289.9062 - val_loss: 0.1209 - val_auc: 0.9890 - val_accuracy: 0.9628 - val_cost: 306.3750\n",
            "Epoch 21/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1128 - auc: 0.9905 - accuracy: 0.9643 - cost: 292.4062 - val_loss: 0.1222 - val_auc: 0.9887 - val_accuracy: 0.9618 - val_cost: 303.2500\n",
            "Epoch 22/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1114 - auc: 0.9907 - accuracy: 0.9649 - cost: 287.4375 - val_loss: 0.1195 - val_auc: 0.9891 - val_accuracy: 0.9631 - val_cost: 298.2500\n",
            "Epoch 23/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1092 - auc: 0.9908 - accuracy: 0.9660 - cost: 279.2500 - val_loss: 0.1193 - val_auc: 0.9894 - val_accuracy: 0.9638 - val_cost: 291.2500\n",
            "Epoch 24/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1079 - auc: 0.9910 - accuracy: 0.9659 - cost: 279.0938 - val_loss: 0.1162 - val_auc: 0.9897 - val_accuracy: 0.9652 - val_cost: 283.0000\n",
            "Epoch 25/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1068 - auc: 0.9911 - accuracy: 0.9667 - cost: 272.4688 - val_loss: 0.1175 - val_auc: 0.9895 - val_accuracy: 0.9647 - val_cost: 284.3750\n",
            "Epoch 26/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1056 - auc: 0.9913 - accuracy: 0.9668 - cost: 270.7812 - val_loss: 0.1166 - val_auc: 0.9897 - val_accuracy: 0.9653 - val_cost: 288.3750\n",
            "Epoch 27/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1047 - auc: 0.9913 - accuracy: 0.9671 - cost: 269.8438 - val_loss: 0.1182 - val_auc: 0.9893 - val_accuracy: 0.9644 - val_cost: 296.8750\n",
            "Epoch 28/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1023 - auc: 0.9916 - accuracy: 0.9685 - cost: 258.5938 - val_loss: 0.1195 - val_auc: 0.9892 - val_accuracy: 0.9639 - val_cost: 293.1250\n",
            "Epoch 29/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1038 - auc: 0.9914 - accuracy: 0.9682 - cost: 261.6250 - val_loss: 0.1142 - val_auc: 0.9899 - val_accuracy: 0.9662 - val_cost: 271.3750\n",
            "Epoch 30/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1007 - auc: 0.9919 - accuracy: 0.9690 - cost: 253.2188 - val_loss: 0.1177 - val_auc: 0.9897 - val_accuracy: 0.9643 - val_cost: 282.0000\n",
            "Epoch 31/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1011 - auc: 0.9919 - accuracy: 0.9687 - cost: 256.0000 - val_loss: 0.1150 - val_auc: 0.9897 - val_accuracy: 0.9652 - val_cost: 288.7500\n",
            "Epoch 32/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1003 - auc: 0.9920 - accuracy: 0.9694 - cost: 250.0312 - val_loss: 0.1140 - val_auc: 0.9895 - val_accuracy: 0.9663 - val_cost: 270.0000\n",
            "Epoch 33/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0996 - auc: 0.9920 - accuracy: 0.9694 - cost: 250.9688 - val_loss: 0.1147 - val_auc: 0.9899 - val_accuracy: 0.9664 - val_cost: 269.1250\n",
            "Epoch 34/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0991 - auc: 0.9920 - accuracy: 0.9699 - cost: 246.0000 - val_loss: 0.1168 - val_auc: 0.9893 - val_accuracy: 0.9652 - val_cost: 279.3750\n",
            "Epoch 35/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0985 - auc: 0.9921 - accuracy: 0.9698 - cost: 247.9688 - val_loss: 0.1156 - val_auc: 0.9897 - val_accuracy: 0.9664 - val_cost: 268.5000\n",
            "Epoch 36/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0982 - auc: 0.9922 - accuracy: 0.9701 - cost: 246.1250 - val_loss: 0.1140 - val_auc: 0.9901 - val_accuracy: 0.9658 - val_cost: 276.7500\n",
            "Epoch 37/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0961 - auc: 0.9926 - accuracy: 0.9704 - cost: 242.6250 - val_loss: 0.1154 - val_auc: 0.9899 - val_accuracy: 0.9670 - val_cost: 263.0000\n",
            "Epoch 38/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0975 - auc: 0.9922 - accuracy: 0.9708 - cost: 239.1562 - val_loss: 0.1145 - val_auc: 0.9895 - val_accuracy: 0.9657 - val_cost: 275.1250\n",
            "Epoch 39/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0957 - auc: 0.9924 - accuracy: 0.9712 - cost: 236.4688 - val_loss: 0.1138 - val_auc: 0.9898 - val_accuracy: 0.9664 - val_cost: 271.3750\n",
            "Epoch 40/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0945 - auc: 0.9925 - accuracy: 0.9718 - cost: 231.4688 - val_loss: 0.1119 - val_auc: 0.9899 - val_accuracy: 0.9672 - val_cost: 267.0000\n",
            "Epoch 41/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0945 - auc: 0.9927 - accuracy: 0.9716 - cost: 233.8438 - val_loss: 0.1142 - val_auc: 0.9896 - val_accuracy: 0.9663 - val_cost: 269.0000\n",
            "Epoch 42/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0967 - auc: 0.9923 - accuracy: 0.9713 - cost: 235.0938 - val_loss: 0.1128 - val_auc: 0.9901 - val_accuracy: 0.9664 - val_cost: 268.0000\n",
            "Epoch 43/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0953 - auc: 0.9927 - accuracy: 0.9707 - cost: 240.2500 - val_loss: 0.1125 - val_auc: 0.9903 - val_accuracy: 0.9673 - val_cost: 257.7500\n",
            "Epoch 44/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0940 - auc: 0.9927 - accuracy: 0.9718 - cost: 230.7188 - val_loss: 0.1173 - val_auc: 0.9894 - val_accuracy: 0.9659 - val_cost: 270.2500\n",
            "Epoch 45/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0941 - auc: 0.9926 - accuracy: 0.9714 - cost: 234.8438 - val_loss: 0.1126 - val_auc: 0.9900 - val_accuracy: 0.9674 - val_cost: 264.5000\n",
            "Epoch 46/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0929 - auc: 0.9927 - accuracy: 0.9720 - cost: 229.9062 - val_loss: 0.1142 - val_auc: 0.9898 - val_accuracy: 0.9668 - val_cost: 269.1250\n",
            "Epoch 47/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0923 - auc: 0.9928 - accuracy: 0.9721 - cost: 228.9375 - val_loss: 0.1112 - val_auc: 0.9903 - val_accuracy: 0.9682 - val_cost: 254.5000\n",
            "Epoch 48/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0921 - auc: 0.9930 - accuracy: 0.9723 - cost: 228.3438 - val_loss: 0.1133 - val_auc: 0.9899 - val_accuracy: 0.9676 - val_cost: 260.1250\n",
            "Epoch 49/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0918 - auc: 0.9930 - accuracy: 0.9723 - cost: 228.2812 - val_loss: 0.1119 - val_auc: 0.9902 - val_accuracy: 0.9680 - val_cost: 257.2500\n",
            "Epoch 50/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0910 - auc: 0.9930 - accuracy: 0.9728 - cost: 223.7188 - val_loss: 0.1141 - val_auc: 0.9899 - val_accuracy: 0.9674 - val_cost: 270.8750\n",
            "Epoch 51/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0911 - auc: 0.9931 - accuracy: 0.9727 - cost: 225.0312 - val_loss: 0.1130 - val_auc: 0.9897 - val_accuracy: 0.9691 - val_cost: 250.2500\n",
            "Epoch 52/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0898 - auc: 0.9931 - accuracy: 0.9733 - cost: 219.4688 - val_loss: 0.1144 - val_auc: 0.9896 - val_accuracy: 0.9671 - val_cost: 273.2500\n",
            "Epoch 53/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0903 - auc: 0.9930 - accuracy: 0.9738 - cost: 215.8438 - val_loss: 0.1153 - val_auc: 0.9894 - val_accuracy: 0.9673 - val_cost: 268.7500\n",
            "Epoch 54/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0895 - auc: 0.9932 - accuracy: 0.9730 - cost: 222.3750 - val_loss: 0.1141 - val_auc: 0.9900 - val_accuracy: 0.9682 - val_cost: 262.3750\n",
            "Epoch 55/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0888 - auc: 0.9934 - accuracy: 0.9737 - cost: 215.7812 - val_loss: 0.1146 - val_auc: 0.9900 - val_accuracy: 0.9683 - val_cost: 251.2500\n",
            "Epoch 56/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0880 - auc: 0.9935 - accuracy: 0.9737 - cost: 216.0625 - val_loss: 0.1149 - val_auc: 0.9898 - val_accuracy: 0.9680 - val_cost: 256.2500\n",
            "Epoch 57/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0908 - auc: 0.9930 - accuracy: 0.9731 - cost: 221.8438 - val_loss: 0.1126 - val_auc: 0.9898 - val_accuracy: 0.9682 - val_cost: 250.8750\n",
            "Epoch 58/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0878 - auc: 0.9935 - accuracy: 0.9742 - cost: 212.2500 - val_loss: 0.1128 - val_auc: 0.9899 - val_accuracy: 0.9688 - val_cost: 257.2500\n",
            "Epoch 59/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0874 - auc: 0.9935 - accuracy: 0.9736 - cost: 217.0312 - val_loss: 0.1161 - val_auc: 0.9899 - val_accuracy: 0.9681 - val_cost: 252.1250\n",
            "Epoch 60/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0882 - auc: 0.9934 - accuracy: 0.9730 - cost: 222.4688 - val_loss: 0.1122 - val_auc: 0.9899 - val_accuracy: 0.9677 - val_cost: 265.2500\n",
            "Epoch 61/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0884 - auc: 0.9932 - accuracy: 0.9739 - cost: 214.5312 - val_loss: 0.1117 - val_auc: 0.9899 - val_accuracy: 0.9686 - val_cost: 256.2500\n",
            "Epoch 62/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0871 - auc: 0.9935 - accuracy: 0.9741 - cost: 212.6250 - val_loss: 0.1135 - val_auc: 0.9898 - val_accuracy: 0.9684 - val_cost: 254.3750\n",
            "Epoch 63/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0873 - auc: 0.9935 - accuracy: 0.9734 - cost: 218.2500 - val_loss: 0.1130 - val_auc: 0.9900 - val_accuracy: 0.9674 - val_cost: 265.8750\n",
            "Epoch 64/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0875 - auc: 0.9934 - accuracy: 0.9740 - cost: 214.1562 - val_loss: 0.1111 - val_auc: 0.9901 - val_accuracy: 0.9687 - val_cost: 255.2500\n",
            "Epoch 65/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0863 - auc: 0.9935 - accuracy: 0.9746 - cost: 208.7188 - val_loss: 0.1134 - val_auc: 0.9896 - val_accuracy: 0.9686 - val_cost: 250.5000\n",
            "Epoch 66/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0852 - auc: 0.9937 - accuracy: 0.9746 - cost: 208.6250 - val_loss: 0.1120 - val_auc: 0.9901 - val_accuracy: 0.9684 - val_cost: 247.5000\n",
            "Epoch 67/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0857 - auc: 0.9937 - accuracy: 0.9739 - cost: 215.5000 - val_loss: 0.1113 - val_auc: 0.9901 - val_accuracy: 0.9684 - val_cost: 250.8750\n",
            "Epoch 68/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0857 - auc: 0.9936 - accuracy: 0.9748 - cost: 207.7500 - val_loss: 0.1123 - val_auc: 0.9901 - val_accuracy: 0.9692 - val_cost: 242.5000\n",
            "Epoch 69/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0850 - auc: 0.9937 - accuracy: 0.9749 - cost: 207.1250 - val_loss: 0.1121 - val_auc: 0.9902 - val_accuracy: 0.9689 - val_cost: 251.7500\n",
            "Epoch 70/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0850 - auc: 0.9937 - accuracy: 0.9747 - cost: 208.7812 - val_loss: 0.1133 - val_auc: 0.9898 - val_accuracy: 0.9692 - val_cost: 251.1250\n",
            "Epoch 71/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0853 - auc: 0.9937 - accuracy: 0.9745 - cost: 210.3750 - val_loss: 0.1144 - val_auc: 0.9898 - val_accuracy: 0.9687 - val_cost: 246.8750\n",
            "Epoch 72/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0840 - auc: 0.9938 - accuracy: 0.9747 - cost: 208.3750 - val_loss: 0.1124 - val_auc: 0.9900 - val_accuracy: 0.9690 - val_cost: 251.0000\n",
            "Epoch 73/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0850 - auc: 0.9937 - accuracy: 0.9751 - cost: 204.8438 - val_loss: 0.1114 - val_auc: 0.9903 - val_accuracy: 0.9687 - val_cost: 255.0000\n",
            "Epoch 74/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0834 - auc: 0.9939 - accuracy: 0.9751 - cost: 205.5000 - val_loss: 0.1123 - val_auc: 0.9901 - val_accuracy: 0.9699 - val_cost: 241.5000\n",
            "Epoch 75/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0834 - auc: 0.9940 - accuracy: 0.9754 - cost: 202.5000 - val_loss: 0.1107 - val_auc: 0.9901 - val_accuracy: 0.9692 - val_cost: 248.0000\n",
            "Epoch 76/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0831 - auc: 0.9941 - accuracy: 0.9753 - cost: 204.0625 - val_loss: 0.1120 - val_auc: 0.9902 - val_accuracy: 0.9699 - val_cost: 243.7500\n",
            "Epoch 77/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0829 - auc: 0.9940 - accuracy: 0.9752 - cost: 204.7188 - val_loss: 0.1146 - val_auc: 0.9896 - val_accuracy: 0.9685 - val_cost: 251.8750\n",
            "Epoch 78/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0838 - auc: 0.9939 - accuracy: 0.9753 - cost: 203.1562 - val_loss: 0.1146 - val_auc: 0.9899 - val_accuracy: 0.9681 - val_cost: 258.7500\n",
            "Epoch 79/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0838 - auc: 0.9939 - accuracy: 0.9752 - cost: 204.0000 - val_loss: 0.1128 - val_auc: 0.9899 - val_accuracy: 0.9684 - val_cost: 256.3750\n",
            "Epoch 80/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0834 - auc: 0.9938 - accuracy: 0.9755 - cost: 202.3125 - val_loss: 0.1149 - val_auc: 0.9899 - val_accuracy: 0.9692 - val_cost: 242.8750\n",
            "Epoch 81/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0828 - auc: 0.9940 - accuracy: 0.9758 - cost: 199.5000 - val_loss: 0.1128 - val_auc: 0.9902 - val_accuracy: 0.9691 - val_cost: 245.6250\n",
            "Epoch 82/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0811 - auc: 0.9941 - accuracy: 0.9765 - cost: 193.8438 - val_loss: 0.1148 - val_auc: 0.9896 - val_accuracy: 0.9684 - val_cost: 263.7500\n",
            "Epoch 83/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0821 - auc: 0.9941 - accuracy: 0.9752 - cost: 205.1250 - val_loss: 0.1156 - val_auc: 0.9898 - val_accuracy: 0.9688 - val_cost: 249.1250\n",
            "Epoch 84/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0824 - auc: 0.9940 - accuracy: 0.9755 - cost: 201.6875 - val_loss: 0.1136 - val_auc: 0.9900 - val_accuracy: 0.9697 - val_cost: 240.3750\n",
            "Epoch 85/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0819 - auc: 0.9940 - accuracy: 0.9759 - cost: 198.1875 - val_loss: 0.1136 - val_auc: 0.9898 - val_accuracy: 0.9688 - val_cost: 250.3750\n",
            "Epoch 86/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0818 - auc: 0.9941 - accuracy: 0.9757 - cost: 200.5625 - val_loss: 0.1144 - val_auc: 0.9895 - val_accuracy: 0.9692 - val_cost: 252.1250\n",
            "Epoch 87/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0819 - auc: 0.9941 - accuracy: 0.9763 - cost: 195.2500 - val_loss: 0.1147 - val_auc: 0.9898 - val_accuracy: 0.9691 - val_cost: 243.7500\n",
            "Epoch 88/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0816 - auc: 0.9941 - accuracy: 0.9755 - cost: 201.0938 - val_loss: 0.1130 - val_auc: 0.9900 - val_accuracy: 0.9696 - val_cost: 240.3750\n",
            "Epoch 89/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0813 - auc: 0.9942 - accuracy: 0.9757 - cost: 201.2188 - val_loss: 0.1127 - val_auc: 0.9900 - val_accuracy: 0.9698 - val_cost: 247.5000\n",
            "Epoch 90/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0812 - auc: 0.9940 - accuracy: 0.9762 - cost: 196.4062 - val_loss: 0.1158 - val_auc: 0.9896 - val_accuracy: 0.9689 - val_cost: 254.1250\n",
            "Epoch 91/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0801 - auc: 0.9942 - accuracy: 0.9763 - cost: 196.1875 - val_loss: 0.1146 - val_auc: 0.9897 - val_accuracy: 0.9694 - val_cost: 243.0000\n",
            "Epoch 92/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0814 - auc: 0.9941 - accuracy: 0.9762 - cost: 196.4062 - val_loss: 0.1141 - val_auc: 0.9895 - val_accuracy: 0.9696 - val_cost: 244.1250\n",
            "Epoch 93/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0798 - auc: 0.9944 - accuracy: 0.9765 - cost: 194.0312 - val_loss: 0.1120 - val_auc: 0.9901 - val_accuracy: 0.9696 - val_cost: 247.7500\n",
            "Epoch 94/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0807 - auc: 0.9943 - accuracy: 0.9761 - cost: 198.1250 - val_loss: 0.1117 - val_auc: 0.9901 - val_accuracy: 0.9707 - val_cost: 233.8750\n",
            "Epoch 95/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0809 - auc: 0.9941 - accuracy: 0.9766 - cost: 192.5312 - val_loss: 0.1127 - val_auc: 0.9901 - val_accuracy: 0.9691 - val_cost: 246.3750\n",
            "Epoch 96/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0795 - auc: 0.9943 - accuracy: 0.9764 - cost: 195.0625 - val_loss: 0.1119 - val_auc: 0.9900 - val_accuracy: 0.9699 - val_cost: 245.6250\n",
            "Epoch 97/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0796 - auc: 0.9944 - accuracy: 0.9762 - cost: 196.3750 - val_loss: 0.1137 - val_auc: 0.9899 - val_accuracy: 0.9684 - val_cost: 257.0000\n",
            "Epoch 98/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0794 - auc: 0.9944 - accuracy: 0.9767 - cost: 192.3438 - val_loss: 0.1117 - val_auc: 0.9899 - val_accuracy: 0.9694 - val_cost: 242.8750\n",
            "Epoch 99/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0800 - auc: 0.9943 - accuracy: 0.9762 - cost: 196.6875 - val_loss: 0.1143 - val_auc: 0.9898 - val_accuracy: 0.9695 - val_cost: 247.8750\n",
            "Epoch 100/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0803 - auc: 0.9943 - accuracy: 0.9758 - cost: 199.6250 - val_loss: 0.1137 - val_auc: 0.9900 - val_accuracy: 0.9693 - val_cost: 249.1250\n",
            "1000/1000 [==============================] - 1s 951us/step\n",
            "fold train/predict time: 0:07:43.595781\n",
            "x_train shape: (128000, 67)\n",
            "Epoch 1/100\n",
            "1600/1600 [==============================] - 6s 3ms/step - loss: 0.3786 - auc: 0.9072 - accuracy: 0.8287 - cost: 1412.4375 - val_loss: 0.2821 - val_auc: 0.9499 - val_accuracy: 0.8854 - val_cost: 913.5000\n",
            "Epoch 2/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.2643 - auc: 0.9561 - accuracy: 0.8943 - cost: 856.8438 - val_loss: 0.2318 - val_auc: 0.9662 - val_accuracy: 0.9095 - val_cost: 741.3750\n",
            "Epoch 3/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.2235 - auc: 0.9684 - accuracy: 0.9142 - cost: 698.4688 - val_loss: 0.1973 - val_auc: 0.9754 - val_accuracy: 0.9259 - val_cost: 587.0000\n",
            "Epoch 4/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1977 - auc: 0.9751 - accuracy: 0.9266 - cost: 597.6250 - val_loss: 0.1783 - val_auc: 0.9799 - val_accuracy: 0.9345 - val_cost: 520.0000\n",
            "Epoch 5/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1807 - auc: 0.9789 - accuracy: 0.9346 - cost: 533.5938 - val_loss: 0.1656 - val_auc: 0.9824 - val_accuracy: 0.9406 - val_cost: 491.2500\n",
            "Epoch 6/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1686 - auc: 0.9815 - accuracy: 0.9402 - cost: 488.5938 - val_loss: 0.1575 - val_auc: 0.9839 - val_accuracy: 0.9448 - val_cost: 436.0000\n",
            "Epoch 7/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1596 - auc: 0.9832 - accuracy: 0.9441 - cost: 456.4062 - val_loss: 0.1471 - val_auc: 0.9855 - val_accuracy: 0.9493 - val_cost: 407.1250\n",
            "Epoch 8/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1533 - auc: 0.9843 - accuracy: 0.9468 - cost: 434.9375 - val_loss: 0.1437 - val_auc: 0.9860 - val_accuracy: 0.9510 - val_cost: 396.8750\n",
            "Epoch 9/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1484 - auc: 0.9853 - accuracy: 0.9488 - cost: 419.1875 - val_loss: 0.1419 - val_auc: 0.9864 - val_accuracy: 0.9522 - val_cost: 400.8750\n",
            "Epoch 10/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1424 - auc: 0.9861 - accuracy: 0.9520 - cost: 392.9375 - val_loss: 0.1394 - val_auc: 0.9866 - val_accuracy: 0.9540 - val_cost: 369.8750\n",
            "Epoch 11/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1378 - auc: 0.9869 - accuracy: 0.9535 - cost: 380.3438 - val_loss: 0.1348 - val_auc: 0.9875 - val_accuracy: 0.9574 - val_cost: 334.2500\n",
            "Epoch 12/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1347 - auc: 0.9874 - accuracy: 0.9552 - cost: 367.2812 - val_loss: 0.1310 - val_auc: 0.9878 - val_accuracy: 0.9574 - val_cost: 339.8750\n",
            "Epoch 13/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1306 - auc: 0.9879 - accuracy: 0.9571 - cost: 352.0312 - val_loss: 0.1312 - val_auc: 0.9877 - val_accuracy: 0.9589 - val_cost: 323.0000\n",
            "Epoch 14/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1275 - auc: 0.9883 - accuracy: 0.9581 - cost: 344.0312 - val_loss: 0.1290 - val_auc: 0.9884 - val_accuracy: 0.9593 - val_cost: 313.6250\n",
            "Epoch 15/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1253 - auc: 0.9886 - accuracy: 0.9595 - cost: 331.6250 - val_loss: 0.1266 - val_auc: 0.9886 - val_accuracy: 0.9601 - val_cost: 319.2500\n",
            "Epoch 16/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1220 - auc: 0.9892 - accuracy: 0.9601 - cost: 327.2812 - val_loss: 0.1256 - val_auc: 0.9886 - val_accuracy: 0.9616 - val_cost: 318.0000\n",
            "Epoch 17/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1198 - auc: 0.9895 - accuracy: 0.9619 - cost: 312.7812 - val_loss: 0.1199 - val_auc: 0.9892 - val_accuracy: 0.9630 - val_cost: 299.2500\n",
            "Epoch 18/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1191 - auc: 0.9894 - accuracy: 0.9615 - cost: 316.3750 - val_loss: 0.1209 - val_auc: 0.9891 - val_accuracy: 0.9630 - val_cost: 296.8750\n",
            "Epoch 19/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1166 - auc: 0.9898 - accuracy: 0.9628 - cost: 306.4062 - val_loss: 0.1186 - val_auc: 0.9893 - val_accuracy: 0.9636 - val_cost: 292.3750\n",
            "Epoch 20/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1158 - auc: 0.9900 - accuracy: 0.9625 - cost: 308.4688 - val_loss: 0.1207 - val_auc: 0.9891 - val_accuracy: 0.9643 - val_cost: 282.5000\n",
            "Epoch 21/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1122 - auc: 0.9905 - accuracy: 0.9648 - cost: 289.9375 - val_loss: 0.1164 - val_auc: 0.9896 - val_accuracy: 0.9642 - val_cost: 292.3750\n",
            "Epoch 22/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1113 - auc: 0.9905 - accuracy: 0.9650 - cost: 286.3438 - val_loss: 0.1184 - val_auc: 0.9893 - val_accuracy: 0.9646 - val_cost: 289.6250\n",
            "Epoch 23/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1100 - auc: 0.9908 - accuracy: 0.9653 - cost: 286.6250 - val_loss: 0.1179 - val_auc: 0.9895 - val_accuracy: 0.9634 - val_cost: 295.7500\n",
            "Epoch 24/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1073 - auc: 0.9911 - accuracy: 0.9666 - cost: 274.4062 - val_loss: 0.1178 - val_auc: 0.9896 - val_accuracy: 0.9655 - val_cost: 279.7500\n",
            "Epoch 25/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1075 - auc: 0.9910 - accuracy: 0.9661 - cost: 279.0625 - val_loss: 0.1158 - val_auc: 0.9898 - val_accuracy: 0.9652 - val_cost: 282.7500\n",
            "Epoch 26/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1058 - auc: 0.9911 - accuracy: 0.9670 - cost: 271.2812 - val_loss: 0.1145 - val_auc: 0.9898 - val_accuracy: 0.9652 - val_cost: 282.8750\n",
            "Epoch 27/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1054 - auc: 0.9913 - accuracy: 0.9672 - cost: 269.5625 - val_loss: 0.1160 - val_auc: 0.9900 - val_accuracy: 0.9649 - val_cost: 276.0000\n",
            "Epoch 28/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1046 - auc: 0.9913 - accuracy: 0.9681 - cost: 262.7500 - val_loss: 0.1173 - val_auc: 0.9896 - val_accuracy: 0.9659 - val_cost: 271.0000\n",
            "Epoch 29/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1038 - auc: 0.9914 - accuracy: 0.9680 - cost: 263.3438 - val_loss: 0.1140 - val_auc: 0.9899 - val_accuracy: 0.9654 - val_cost: 285.1250\n",
            "Epoch 30/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1013 - auc: 0.9919 - accuracy: 0.9680 - cost: 263.6562 - val_loss: 0.1154 - val_auc: 0.9901 - val_accuracy: 0.9650 - val_cost: 293.5000\n",
            "Epoch 31/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1008 - auc: 0.9919 - accuracy: 0.9689 - cost: 256.1875 - val_loss: 0.1154 - val_auc: 0.9898 - val_accuracy: 0.9665 - val_cost: 272.6250\n",
            "Epoch 32/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1006 - auc: 0.9919 - accuracy: 0.9690 - cost: 256.1250 - val_loss: 0.1132 - val_auc: 0.9902 - val_accuracy: 0.9678 - val_cost: 261.6250\n",
            "Epoch 33/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0997 - auc: 0.9920 - accuracy: 0.9692 - cost: 253.9688 - val_loss: 0.1147 - val_auc: 0.9903 - val_accuracy: 0.9663 - val_cost: 263.7500\n",
            "Epoch 34/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0989 - auc: 0.9922 - accuracy: 0.9691 - cost: 255.0938 - val_loss: 0.1154 - val_auc: 0.9900 - val_accuracy: 0.9667 - val_cost: 275.0000\n",
            "Epoch 35/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0990 - auc: 0.9920 - accuracy: 0.9698 - cost: 248.3750 - val_loss: 0.1156 - val_auc: 0.9899 - val_accuracy: 0.9646 - val_cost: 291.8750\n",
            "Epoch 36/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0978 - auc: 0.9922 - accuracy: 0.9704 - cost: 244.4375 - val_loss: 0.1128 - val_auc: 0.9903 - val_accuracy: 0.9662 - val_cost: 271.7500\n",
            "Epoch 37/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0966 - auc: 0.9924 - accuracy: 0.9708 - cost: 240.3750 - val_loss: 0.1137 - val_auc: 0.9900 - val_accuracy: 0.9670 - val_cost: 270.1250\n",
            "Epoch 38/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0966 - auc: 0.9922 - accuracy: 0.9707 - cost: 242.2500 - val_loss: 0.1110 - val_auc: 0.9903 - val_accuracy: 0.9668 - val_cost: 268.8750\n",
            "Epoch 39/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0961 - auc: 0.9924 - accuracy: 0.9709 - cost: 239.3750 - val_loss: 0.1132 - val_auc: 0.9901 - val_accuracy: 0.9661 - val_cost: 278.1250\n",
            "Epoch 40/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0964 - auc: 0.9925 - accuracy: 0.9708 - cost: 240.2188 - val_loss: 0.1145 - val_auc: 0.9898 - val_accuracy: 0.9674 - val_cost: 265.2500\n",
            "Epoch 41/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0962 - auc: 0.9923 - accuracy: 0.9713 - cost: 235.9375 - val_loss: 0.1122 - val_auc: 0.9901 - val_accuracy: 0.9669 - val_cost: 270.5000\n",
            "Epoch 42/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0948 - auc: 0.9925 - accuracy: 0.9707 - cost: 242.5625 - val_loss: 0.1140 - val_auc: 0.9900 - val_accuracy: 0.9664 - val_cost: 273.8750\n",
            "Epoch 43/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0954 - auc: 0.9925 - accuracy: 0.9712 - cost: 238.2500 - val_loss: 0.1130 - val_auc: 0.9902 - val_accuracy: 0.9663 - val_cost: 271.6250\n",
            "Epoch 44/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0940 - auc: 0.9927 - accuracy: 0.9713 - cost: 237.0312 - val_loss: 0.1135 - val_auc: 0.9901 - val_accuracy: 0.9671 - val_cost: 267.2500\n",
            "Epoch 45/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0933 - auc: 0.9929 - accuracy: 0.9719 - cost: 231.9688 - val_loss: 0.1123 - val_auc: 0.9902 - val_accuracy: 0.9671 - val_cost: 268.1250\n",
            "Epoch 46/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0918 - auc: 0.9930 - accuracy: 0.9722 - cost: 228.9375 - val_loss: 0.1135 - val_auc: 0.9898 - val_accuracy: 0.9661 - val_cost: 280.1250\n",
            "Epoch 47/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0915 - auc: 0.9930 - accuracy: 0.9724 - cost: 227.0625 - val_loss: 0.1162 - val_auc: 0.9895 - val_accuracy: 0.9671 - val_cost: 267.2500\n",
            "Epoch 48/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0918 - auc: 0.9930 - accuracy: 0.9723 - cost: 228.3438 - val_loss: 0.1134 - val_auc: 0.9899 - val_accuracy: 0.9674 - val_cost: 265.6250\n",
            "Epoch 49/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0909 - auc: 0.9930 - accuracy: 0.9727 - cost: 224.7812 - val_loss: 0.1129 - val_auc: 0.9901 - val_accuracy: 0.9670 - val_cost: 269.3750\n",
            "Epoch 50/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0907 - auc: 0.9932 - accuracy: 0.9722 - cost: 229.8125 - val_loss: 0.1125 - val_auc: 0.9903 - val_accuracy: 0.9671 - val_cost: 265.6250\n",
            "Epoch 51/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0907 - auc: 0.9930 - accuracy: 0.9729 - cost: 223.3750 - val_loss: 0.1126 - val_auc: 0.9901 - val_accuracy: 0.9686 - val_cost: 253.1250\n",
            "Epoch 52/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0897 - auc: 0.9932 - accuracy: 0.9729 - cost: 223.1250 - val_loss: 0.1165 - val_auc: 0.9896 - val_accuracy: 0.9662 - val_cost: 284.0000\n",
            "Epoch 53/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0902 - auc: 0.9932 - accuracy: 0.9724 - cost: 227.4062 - val_loss: 0.1149 - val_auc: 0.9900 - val_accuracy: 0.9674 - val_cost: 268.3750\n",
            "Epoch 54/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0894 - auc: 0.9932 - accuracy: 0.9733 - cost: 220.7500 - val_loss: 0.1130 - val_auc: 0.9901 - val_accuracy: 0.9677 - val_cost: 262.0000\n",
            "Epoch 55/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0903 - auc: 0.9932 - accuracy: 0.9728 - cost: 224.7500 - val_loss: 0.1119 - val_auc: 0.9903 - val_accuracy: 0.9684 - val_cost: 252.6250\n",
            "Epoch 56/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0887 - auc: 0.9933 - accuracy: 0.9732 - cost: 220.9375 - val_loss: 0.1155 - val_auc: 0.9899 - val_accuracy: 0.9666 - val_cost: 268.7500\n",
            "Epoch 57/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0887 - auc: 0.9933 - accuracy: 0.9730 - cost: 224.0312 - val_loss: 0.1151 - val_auc: 0.9901 - val_accuracy: 0.9677 - val_cost: 261.3750\n",
            "Epoch 58/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0882 - auc: 0.9933 - accuracy: 0.9736 - cost: 217.8125 - val_loss: 0.1144 - val_auc: 0.9901 - val_accuracy: 0.9669 - val_cost: 272.0000\n",
            "Epoch 59/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0880 - auc: 0.9935 - accuracy: 0.9733 - cost: 219.8438 - val_loss: 0.1147 - val_auc: 0.9901 - val_accuracy: 0.9682 - val_cost: 249.3750\n",
            "Epoch 60/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0872 - auc: 0.9936 - accuracy: 0.9737 - cost: 217.6250 - val_loss: 0.1137 - val_auc: 0.9903 - val_accuracy: 0.9680 - val_cost: 250.2500\n",
            "Epoch 61/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0886 - auc: 0.9934 - accuracy: 0.9734 - cost: 219.4688 - val_loss: 0.1101 - val_auc: 0.9904 - val_accuracy: 0.9688 - val_cost: 256.3750\n",
            "Epoch 62/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0873 - auc: 0.9934 - accuracy: 0.9739 - cost: 216.0625 - val_loss: 0.1135 - val_auc: 0.9900 - val_accuracy: 0.9678 - val_cost: 256.5000\n",
            "Epoch 63/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0862 - auc: 0.9935 - accuracy: 0.9746 - cost: 210.1250 - val_loss: 0.1135 - val_auc: 0.9901 - val_accuracy: 0.9685 - val_cost: 255.1250\n",
            "Epoch 64/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0872 - auc: 0.9935 - accuracy: 0.9735 - cost: 218.6250 - val_loss: 0.1168 - val_auc: 0.9897 - val_accuracy: 0.9677 - val_cost: 260.3750\n",
            "Epoch 65/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0865 - auc: 0.9936 - accuracy: 0.9746 - cost: 209.6562 - val_loss: 0.1143 - val_auc: 0.9897 - val_accuracy: 0.9682 - val_cost: 254.8750\n",
            "Epoch 66/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0874 - auc: 0.9935 - accuracy: 0.9736 - cost: 217.6250 - val_loss: 0.1129 - val_auc: 0.9900 - val_accuracy: 0.9678 - val_cost: 266.2500\n",
            "Epoch 67/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0860 - auc: 0.9936 - accuracy: 0.9739 - cost: 216.1250 - val_loss: 0.1123 - val_auc: 0.9901 - val_accuracy: 0.9684 - val_cost: 259.8750\n",
            "Epoch 68/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0856 - auc: 0.9937 - accuracy: 0.9741 - cost: 214.3125 - val_loss: 0.1118 - val_auc: 0.9901 - val_accuracy: 0.9688 - val_cost: 254.7500\n",
            "Epoch 69/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0857 - auc: 0.9936 - accuracy: 0.9749 - cost: 208.0000 - val_loss: 0.1123 - val_auc: 0.9902 - val_accuracy: 0.9683 - val_cost: 251.3750\n",
            "Epoch 70/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0850 - auc: 0.9938 - accuracy: 0.9743 - cost: 212.8438 - val_loss: 0.1155 - val_auc: 0.9899 - val_accuracy: 0.9676 - val_cost: 265.1250\n",
            "Epoch 71/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0852 - auc: 0.9938 - accuracy: 0.9745 - cost: 210.8750 - val_loss: 0.1154 - val_auc: 0.9898 - val_accuracy: 0.9675 - val_cost: 266.2500\n",
            "Epoch 72/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0850 - auc: 0.9937 - accuracy: 0.9749 - cost: 207.0938 - val_loss: 0.1123 - val_auc: 0.9905 - val_accuracy: 0.9684 - val_cost: 250.5000\n",
            "Epoch 73/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0840 - auc: 0.9938 - accuracy: 0.9756 - cost: 202.4688 - val_loss: 0.1160 - val_auc: 0.9898 - val_accuracy: 0.9675 - val_cost: 259.2500\n",
            "Epoch 74/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0831 - auc: 0.9941 - accuracy: 0.9748 - cost: 207.6250 - val_loss: 0.1163 - val_auc: 0.9899 - val_accuracy: 0.9676 - val_cost: 261.7500\n",
            "1000/1000 [==============================] - 1s 936us/step\n",
            "fold train/predict time: 0:05:43.243178\n",
            "x_train shape: (128000, 67)\n",
            "Epoch 1/100\n",
            "1600/1600 [==============================] - 6s 3ms/step - loss: 0.3798 - auc: 0.9068 - accuracy: 0.8294 - cost: 1402.6875 - val_loss: 0.2817 - val_auc: 0.9505 - val_accuracy: 0.8818 - val_cost: 987.1250\n",
            "Epoch 2/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.2632 - auc: 0.9562 - accuracy: 0.8946 - cost: 852.8125 - val_loss: 0.2283 - val_auc: 0.9681 - val_accuracy: 0.9109 - val_cost: 694.2500\n",
            "Epoch 3/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.2200 - auc: 0.9692 - accuracy: 0.9155 - cost: 684.6875 - val_loss: 0.1955 - val_auc: 0.9757 - val_accuracy: 0.9275 - val_cost: 580.3750\n",
            "Epoch 4/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1938 - auc: 0.9758 - accuracy: 0.9286 - cost: 579.6250 - val_loss: 0.1740 - val_auc: 0.9804 - val_accuracy: 0.9370 - val_cost: 509.8750\n",
            "Epoch 5/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1747 - auc: 0.9801 - accuracy: 0.9370 - cost: 512.7500 - val_loss: 0.1620 - val_auc: 0.9827 - val_accuracy: 0.9421 - val_cost: 473.1250\n",
            "Epoch 6/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1624 - auc: 0.9826 - accuracy: 0.9422 - cost: 469.9688 - val_loss: 0.1517 - val_auc: 0.9847 - val_accuracy: 0.9469 - val_cost: 434.1250\n",
            "Epoch 7/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1545 - auc: 0.9840 - accuracy: 0.9463 - cost: 437.2188 - val_loss: 0.1454 - val_auc: 0.9859 - val_accuracy: 0.9498 - val_cost: 392.2500\n",
            "Epoch 8/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1451 - auc: 0.9857 - accuracy: 0.9505 - cost: 403.0312 - val_loss: 0.1374 - val_auc: 0.9869 - val_accuracy: 0.9528 - val_cost: 384.8750\n",
            "Epoch 9/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1407 - auc: 0.9864 - accuracy: 0.9518 - cost: 391.7812 - val_loss: 0.1353 - val_auc: 0.9873 - val_accuracy: 0.9556 - val_cost: 366.2500\n",
            "Epoch 10/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1343 - auc: 0.9875 - accuracy: 0.9548 - cost: 368.6250 - val_loss: 0.1326 - val_auc: 0.9877 - val_accuracy: 0.9569 - val_cost: 347.0000\n",
            "Epoch 11/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1304 - auc: 0.9879 - accuracy: 0.9563 - cost: 356.5938 - val_loss: 0.1306 - val_auc: 0.9882 - val_accuracy: 0.9572 - val_cost: 358.1250\n",
            "Epoch 12/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1284 - auc: 0.9882 - accuracy: 0.9582 - cost: 342.1562 - val_loss: 0.1257 - val_auc: 0.9887 - val_accuracy: 0.9603 - val_cost: 317.0000\n",
            "Epoch 13/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1247 - auc: 0.9887 - accuracy: 0.9594 - cost: 330.8438 - val_loss: 0.1242 - val_auc: 0.9888 - val_accuracy: 0.9595 - val_cost: 319.5000\n",
            "Epoch 14/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1214 - auc: 0.9892 - accuracy: 0.9609 - cost: 319.4688 - val_loss: 0.1228 - val_auc: 0.9890 - val_accuracy: 0.9604 - val_cost: 324.5000\n",
            "Epoch 15/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1169 - auc: 0.9898 - accuracy: 0.9633 - cost: 299.8438 - val_loss: 0.1191 - val_auc: 0.9897 - val_accuracy: 0.9628 - val_cost: 303.7500\n",
            "Epoch 16/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1156 - auc: 0.9901 - accuracy: 0.9630 - cost: 302.8125 - val_loss: 0.1205 - val_auc: 0.9894 - val_accuracy: 0.9620 - val_cost: 311.8750\n",
            "Epoch 17/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1127 - auc: 0.9904 - accuracy: 0.9637 - cost: 297.3125 - val_loss: 0.1191 - val_auc: 0.9893 - val_accuracy: 0.9627 - val_cost: 297.6250\n",
            "Epoch 18/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1113 - auc: 0.9906 - accuracy: 0.9656 - cost: 281.0625 - val_loss: 0.1164 - val_auc: 0.9902 - val_accuracy: 0.9636 - val_cost: 285.2500\n",
            "Epoch 19/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1102 - auc: 0.9906 - accuracy: 0.9660 - cost: 278.7812 - val_loss: 0.1184 - val_auc: 0.9899 - val_accuracy: 0.9632 - val_cost: 305.5000\n",
            "Epoch 20/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1094 - auc: 0.9908 - accuracy: 0.9658 - cost: 279.8750 - val_loss: 0.1169 - val_auc: 0.9899 - val_accuracy: 0.9637 - val_cost: 285.7500\n",
            "Epoch 21/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1066 - auc: 0.9912 - accuracy: 0.9661 - cost: 277.0625 - val_loss: 0.1190 - val_auc: 0.9897 - val_accuracy: 0.9628 - val_cost: 287.8750\n",
            "Epoch 22/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1058 - auc: 0.9912 - accuracy: 0.9668 - cost: 271.9062 - val_loss: 0.1179 - val_auc: 0.9898 - val_accuracy: 0.9644 - val_cost: 277.1250\n",
            "Epoch 23/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1061 - auc: 0.9914 - accuracy: 0.9671 - cost: 269.1562 - val_loss: 0.1161 - val_auc: 0.9902 - val_accuracy: 0.9636 - val_cost: 301.2500\n",
            "Epoch 24/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1040 - auc: 0.9915 - accuracy: 0.9675 - cost: 265.9688 - val_loss: 0.1154 - val_auc: 0.9898 - val_accuracy: 0.9646 - val_cost: 279.1250\n",
            "Epoch 25/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1034 - auc: 0.9915 - accuracy: 0.9678 - cost: 263.2812 - val_loss: 0.1153 - val_auc: 0.9897 - val_accuracy: 0.9649 - val_cost: 286.7500\n",
            "Epoch 26/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1016 - auc: 0.9917 - accuracy: 0.9685 - cost: 257.8438 - val_loss: 0.1125 - val_auc: 0.9903 - val_accuracy: 0.9662 - val_cost: 272.2500\n",
            "Epoch 27/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1005 - auc: 0.9919 - accuracy: 0.9692 - cost: 253.0938 - val_loss: 0.1143 - val_auc: 0.9900 - val_accuracy: 0.9663 - val_cost: 274.5000\n",
            "Epoch 28/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1008 - auc: 0.9920 - accuracy: 0.9687 - cost: 256.6875 - val_loss: 0.1118 - val_auc: 0.9903 - val_accuracy: 0.9670 - val_cost: 270.6250\n",
            "Epoch 29/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0983 - auc: 0.9921 - accuracy: 0.9695 - cost: 250.7500 - val_loss: 0.1137 - val_auc: 0.9898 - val_accuracy: 0.9659 - val_cost: 272.2500\n",
            "Epoch 30/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0994 - auc: 0.9922 - accuracy: 0.9698 - cost: 247.8125 - val_loss: 0.1149 - val_auc: 0.9898 - val_accuracy: 0.9663 - val_cost: 277.5000\n",
            "Epoch 31/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0977 - auc: 0.9923 - accuracy: 0.9707 - cost: 240.8750 - val_loss: 0.1150 - val_auc: 0.9899 - val_accuracy: 0.9663 - val_cost: 270.3750\n",
            "Epoch 32/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0974 - auc: 0.9923 - accuracy: 0.9704 - cost: 242.5000 - val_loss: 0.1132 - val_auc: 0.9902 - val_accuracy: 0.9673 - val_cost: 265.3750\n",
            "Epoch 33/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0962 - auc: 0.9925 - accuracy: 0.9709 - cost: 238.0000 - val_loss: 0.1129 - val_auc: 0.9900 - val_accuracy: 0.9663 - val_cost: 275.8750\n",
            "Epoch 34/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0950 - auc: 0.9926 - accuracy: 0.9716 - cost: 233.2812 - val_loss: 0.1103 - val_auc: 0.9903 - val_accuracy: 0.9677 - val_cost: 265.0000\n",
            "Epoch 35/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0954 - auc: 0.9925 - accuracy: 0.9706 - cost: 241.1875 - val_loss: 0.1154 - val_auc: 0.9896 - val_accuracy: 0.9662 - val_cost: 280.5000\n",
            "Epoch 36/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0955 - auc: 0.9924 - accuracy: 0.9712 - cost: 235.9688 - val_loss: 0.1103 - val_auc: 0.9904 - val_accuracy: 0.9668 - val_cost: 267.7500\n",
            "Epoch 37/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0950 - auc: 0.9924 - accuracy: 0.9715 - cost: 233.8438 - val_loss: 0.1134 - val_auc: 0.9898 - val_accuracy: 0.9668 - val_cost: 267.1250\n",
            "Epoch 38/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0936 - auc: 0.9927 - accuracy: 0.9721 - cost: 229.1250 - val_loss: 0.1123 - val_auc: 0.9902 - val_accuracy: 0.9675 - val_cost: 255.8750\n",
            "Epoch 39/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0939 - auc: 0.9927 - accuracy: 0.9716 - cost: 233.4062 - val_loss: 0.1134 - val_auc: 0.9896 - val_accuracy: 0.9679 - val_cost: 262.1250\n",
            "Epoch 40/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0943 - auc: 0.9927 - accuracy: 0.9715 - cost: 234.1562 - val_loss: 0.1132 - val_auc: 0.9900 - val_accuracy: 0.9677 - val_cost: 264.1250\n",
            "Epoch 41/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0927 - auc: 0.9928 - accuracy: 0.9721 - cost: 229.2188 - val_loss: 0.1121 - val_auc: 0.9899 - val_accuracy: 0.9680 - val_cost: 264.5000\n",
            "Epoch 42/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0915 - auc: 0.9930 - accuracy: 0.9727 - cost: 224.1250 - val_loss: 0.1128 - val_auc: 0.9897 - val_accuracy: 0.9669 - val_cost: 264.1250\n",
            "Epoch 43/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0933 - auc: 0.9927 - accuracy: 0.9719 - cost: 231.2812 - val_loss: 0.1127 - val_auc: 0.9900 - val_accuracy: 0.9674 - val_cost: 263.2500\n",
            "Epoch 44/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0914 - auc: 0.9931 - accuracy: 0.9724 - cost: 226.7188 - val_loss: 0.1150 - val_auc: 0.9896 - val_accuracy: 0.9673 - val_cost: 264.6250\n",
            "Epoch 45/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0916 - auc: 0.9929 - accuracy: 0.9728 - cost: 223.2500 - val_loss: 0.1130 - val_auc: 0.9899 - val_accuracy: 0.9669 - val_cost: 267.6250\n",
            "Epoch 46/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0906 - auc: 0.9930 - accuracy: 0.9727 - cost: 224.6250 - val_loss: 0.1121 - val_auc: 0.9900 - val_accuracy: 0.9680 - val_cost: 262.8750\n",
            "Epoch 47/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0908 - auc: 0.9931 - accuracy: 0.9727 - cost: 223.8750 - val_loss: 0.1119 - val_auc: 0.9899 - val_accuracy: 0.9676 - val_cost: 266.1250\n",
            "Epoch 48/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0901 - auc: 0.9931 - accuracy: 0.9732 - cost: 219.7812 - val_loss: 0.1117 - val_auc: 0.9900 - val_accuracy: 0.9682 - val_cost: 258.6250\n",
            "Epoch 49/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0886 - auc: 0.9933 - accuracy: 0.9736 - cost: 217.0000 - val_loss: 0.1116 - val_auc: 0.9899 - val_accuracy: 0.9680 - val_cost: 256.3750\n",
            "Epoch 50/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0887 - auc: 0.9931 - accuracy: 0.9733 - cost: 219.7812 - val_loss: 0.1130 - val_auc: 0.9900 - val_accuracy: 0.9680 - val_cost: 261.1250\n",
            "Epoch 51/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0879 - auc: 0.9934 - accuracy: 0.9739 - cost: 214.3750 - val_loss: 0.1119 - val_auc: 0.9900 - val_accuracy: 0.9677 - val_cost: 254.5000\n",
            "Epoch 52/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0880 - auc: 0.9936 - accuracy: 0.9733 - cost: 219.9375 - val_loss: 0.1173 - val_auc: 0.9896 - val_accuracy: 0.9671 - val_cost: 268.1250\n",
            "Epoch 53/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0876 - auc: 0.9933 - accuracy: 0.9742 - cost: 211.3750 - val_loss: 0.1113 - val_auc: 0.9901 - val_accuracy: 0.9682 - val_cost: 260.3750\n",
            "Epoch 54/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0881 - auc: 0.9932 - accuracy: 0.9736 - cost: 218.0312 - val_loss: 0.1118 - val_auc: 0.9898 - val_accuracy: 0.9677 - val_cost: 267.3750\n",
            "Epoch 55/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0886 - auc: 0.9934 - accuracy: 0.9740 - cost: 213.9688 - val_loss: 0.1149 - val_auc: 0.9897 - val_accuracy: 0.9678 - val_cost: 259.6250\n",
            "Epoch 56/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0862 - auc: 0.9937 - accuracy: 0.9742 - cost: 211.1250 - val_loss: 0.1122 - val_auc: 0.9899 - val_accuracy: 0.9687 - val_cost: 256.7500\n",
            "Epoch 57/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0864 - auc: 0.9936 - accuracy: 0.9742 - cost: 212.5312 - val_loss: 0.1148 - val_auc: 0.9900 - val_accuracy: 0.9678 - val_cost: 254.6250\n",
            "Epoch 58/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0859 - auc: 0.9937 - accuracy: 0.9742 - cost: 212.0625 - val_loss: 0.1135 - val_auc: 0.9900 - val_accuracy: 0.9677 - val_cost: 258.5000\n",
            "Epoch 59/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0857 - auc: 0.9936 - accuracy: 0.9746 - cost: 208.8438 - val_loss: 0.1140 - val_auc: 0.9897 - val_accuracy: 0.9671 - val_cost: 268.5000\n",
            "Epoch 60/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0863 - auc: 0.9936 - accuracy: 0.9742 - cost: 212.2500 - val_loss: 0.1161 - val_auc: 0.9895 - val_accuracy: 0.9677 - val_cost: 260.1250\n",
            "Epoch 61/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0857 - auc: 0.9937 - accuracy: 0.9751 - cost: 205.0312 - val_loss: 0.1131 - val_auc: 0.9898 - val_accuracy: 0.9683 - val_cost: 253.1250\n",
            "Epoch 62/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0844 - auc: 0.9938 - accuracy: 0.9747 - cost: 208.1875 - val_loss: 0.1160 - val_auc: 0.9898 - val_accuracy: 0.9680 - val_cost: 253.5000\n",
            "Epoch 63/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0869 - auc: 0.9935 - accuracy: 0.9743 - cost: 211.3125 - val_loss: 0.1151 - val_auc: 0.9895 - val_accuracy: 0.9669 - val_cost: 272.3750\n",
            "Epoch 64/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0840 - auc: 0.9938 - accuracy: 0.9750 - cost: 206.6875 - val_loss: 0.1161 - val_auc: 0.9898 - val_accuracy: 0.9689 - val_cost: 253.6250\n",
            "Epoch 65/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0830 - auc: 0.9941 - accuracy: 0.9748 - cost: 207.6250 - val_loss: 0.1155 - val_auc: 0.9897 - val_accuracy: 0.9679 - val_cost: 257.6250\n",
            "Epoch 66/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0842 - auc: 0.9939 - accuracy: 0.9751 - cost: 205.6250 - val_loss: 0.1147 - val_auc: 0.9900 - val_accuracy: 0.9676 - val_cost: 256.2500\n",
            "Epoch 67/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0831 - auc: 0.9939 - accuracy: 0.9753 - cost: 202.5000 - val_loss: 0.1122 - val_auc: 0.9900 - val_accuracy: 0.9700 - val_cost: 245.7500\n",
            "Epoch 68/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0835 - auc: 0.9938 - accuracy: 0.9754 - cost: 202.0938 - val_loss: 0.1145 - val_auc: 0.9897 - val_accuracy: 0.9680 - val_cost: 254.0000\n",
            "Epoch 69/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0829 - auc: 0.9939 - accuracy: 0.9757 - cost: 200.6875 - val_loss: 0.1150 - val_auc: 0.9899 - val_accuracy: 0.9684 - val_cost: 256.0000\n",
            "Epoch 70/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0834 - auc: 0.9940 - accuracy: 0.9755 - cost: 201.7188 - val_loss: 0.1139 - val_auc: 0.9897 - val_accuracy: 0.9682 - val_cost: 252.8750\n",
            "Epoch 71/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0826 - auc: 0.9939 - accuracy: 0.9758 - cost: 198.9062 - val_loss: 0.1137 - val_auc: 0.9901 - val_accuracy: 0.9679 - val_cost: 253.7500\n",
            "Epoch 72/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0829 - auc: 0.9940 - accuracy: 0.9751 - cost: 205.1250 - val_loss: 0.1159 - val_auc: 0.9897 - val_accuracy: 0.9678 - val_cost: 258.0000\n",
            "Epoch 73/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0828 - auc: 0.9940 - accuracy: 0.9760 - cost: 197.5625 - val_loss: 0.1137 - val_auc: 0.9897 - val_accuracy: 0.9684 - val_cost: 256.5000\n",
            "Epoch 74/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0814 - auc: 0.9942 - accuracy: 0.9757 - cost: 200.6562 - val_loss: 0.1141 - val_auc: 0.9899 - val_accuracy: 0.9680 - val_cost: 259.7500\n",
            "Epoch 75/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0833 - auc: 0.9939 - accuracy: 0.9755 - cost: 202.4062 - val_loss: 0.1149 - val_auc: 0.9896 - val_accuracy: 0.9680 - val_cost: 256.2500\n",
            "Epoch 76/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0812 - auc: 0.9943 - accuracy: 0.9757 - cost: 200.5000 - val_loss: 0.1161 - val_auc: 0.9894 - val_accuracy: 0.9684 - val_cost: 256.7500\n",
            "Epoch 77/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0812 - auc: 0.9941 - accuracy: 0.9758 - cost: 198.7812 - val_loss: 0.1183 - val_auc: 0.9892 - val_accuracy: 0.9674 - val_cost: 260.6250\n",
            "Epoch 78/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0819 - auc: 0.9941 - accuracy: 0.9756 - cost: 200.4375 - val_loss: 0.1144 - val_auc: 0.9900 - val_accuracy: 0.9686 - val_cost: 247.8750\n",
            "Epoch 79/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0818 - auc: 0.9942 - accuracy: 0.9760 - cost: 198.2500 - val_loss: 0.1162 - val_auc: 0.9899 - val_accuracy: 0.9677 - val_cost: 258.3750\n",
            "Epoch 80/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0822 - auc: 0.9941 - accuracy: 0.9756 - cost: 201.1562 - val_loss: 0.1132 - val_auc: 0.9902 - val_accuracy: 0.9695 - val_cost: 247.3750\n",
            "Epoch 81/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0813 - auc: 0.9943 - accuracy: 0.9757 - cost: 200.5938 - val_loss: 0.1155 - val_auc: 0.9899 - val_accuracy: 0.9677 - val_cost: 260.7500\n",
            "Epoch 82/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0799 - auc: 0.9944 - accuracy: 0.9764 - cost: 193.8125 - val_loss: 0.1184 - val_auc: 0.9896 - val_accuracy: 0.9675 - val_cost: 254.7500\n",
            "1000/1000 [==============================] - 1s 1ms/step\n",
            "fold train/predict time: 0:06:19.238439\n",
            "x_train shape: (128000, 67)\n",
            "Epoch 1/100\n",
            "1600/1600 [==============================] - 8s 4ms/step - loss: 0.3810 - auc: 0.9062 - accuracy: 0.8298 - cost: 1404.9375 - val_loss: 0.2841 - val_auc: 0.9504 - val_accuracy: 0.8834 - val_cost: 913.2500\n",
            "Epoch 2/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.2653 - auc: 0.9556 - accuracy: 0.8936 - cost: 860.3750 - val_loss: 0.2308 - val_auc: 0.9667 - val_accuracy: 0.9080 - val_cost: 751.0000\n",
            "Epoch 3/100\n",
            "1600/1600 [==============================] - 6s 4ms/step - loss: 0.2229 - auc: 0.9685 - accuracy: 0.9147 - cost: 690.9688 - val_loss: 0.1981 - val_auc: 0.9751 - val_accuracy: 0.9248 - val_cost: 609.7500\n",
            "Epoch 4/100\n",
            "1600/1600 [==============================] - 6s 4ms/step - loss: 0.1979 - auc: 0.9748 - accuracy: 0.9262 - cost: 599.2188 - val_loss: 0.1797 - val_auc: 0.9792 - val_accuracy: 0.9334 - val_cost: 539.3750\n",
            "Epoch 5/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1799 - auc: 0.9790 - accuracy: 0.9340 - cost: 537.3438 - val_loss: 0.1679 - val_auc: 0.9817 - val_accuracy: 0.9399 - val_cost: 482.2500\n",
            "Epoch 6/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1674 - auc: 0.9816 - accuracy: 0.9396 - cost: 492.1562 - val_loss: 0.1596 - val_auc: 0.9834 - val_accuracy: 0.9425 - val_cost: 483.1250\n",
            "Epoch 7/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1580 - auc: 0.9835 - accuracy: 0.9443 - cost: 452.8750 - val_loss: 0.1539 - val_auc: 0.9842 - val_accuracy: 0.9461 - val_cost: 434.2500\n",
            "Epoch 8/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1504 - auc: 0.9847 - accuracy: 0.9478 - cost: 424.9688 - val_loss: 0.1471 - val_auc: 0.9853 - val_accuracy: 0.9489 - val_cost: 417.0000\n",
            "Epoch 9/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1468 - auc: 0.9855 - accuracy: 0.9498 - cost: 409.8750 - val_loss: 0.1440 - val_auc: 0.9859 - val_accuracy: 0.9508 - val_cost: 405.3750\n",
            "Epoch 10/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1414 - auc: 0.9863 - accuracy: 0.9519 - cost: 392.8438 - val_loss: 0.1423 - val_auc: 0.9861 - val_accuracy: 0.9516 - val_cost: 394.2500\n",
            "Epoch 11/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1376 - auc: 0.9869 - accuracy: 0.9527 - cost: 386.0625 - val_loss: 0.1397 - val_auc: 0.9868 - val_accuracy: 0.9534 - val_cost: 394.1250\n",
            "Epoch 12/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1336 - auc: 0.9876 - accuracy: 0.9560 - cost: 359.7812 - val_loss: 0.1379 - val_auc: 0.9870 - val_accuracy: 0.9541 - val_cost: 356.2500\n",
            "Epoch 13/100\n",
            "1600/1600 [==============================] - 4s 3ms/step - loss: 0.1309 - auc: 0.9880 - accuracy: 0.9566 - cost: 354.7500 - val_loss: 0.1355 - val_auc: 0.9872 - val_accuracy: 0.9555 - val_cost: 353.3750\n",
            "Epoch 14/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1289 - auc: 0.9883 - accuracy: 0.9579 - cost: 343.2812 - val_loss: 0.1314 - val_auc: 0.9879 - val_accuracy: 0.9563 - val_cost: 351.8750\n",
            "Epoch 15/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1251 - auc: 0.9888 - accuracy: 0.9595 - cost: 331.1875 - val_loss: 0.1320 - val_auc: 0.9877 - val_accuracy: 0.9561 - val_cost: 358.0000\n",
            "Epoch 16/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1230 - auc: 0.9891 - accuracy: 0.9599 - cost: 327.9062 - val_loss: 0.1316 - val_auc: 0.9877 - val_accuracy: 0.9578 - val_cost: 336.3750\n",
            "Epoch 17/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1209 - auc: 0.9895 - accuracy: 0.9607 - cost: 321.8750 - val_loss: 0.1290 - val_auc: 0.9880 - val_accuracy: 0.9581 - val_cost: 337.7500\n",
            "Epoch 18/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1204 - auc: 0.9895 - accuracy: 0.9610 - cost: 318.8438 - val_loss: 0.1280 - val_auc: 0.9881 - val_accuracy: 0.9602 - val_cost: 317.5000\n",
            "Epoch 19/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1174 - auc: 0.9899 - accuracy: 0.9621 - cost: 310.6562 - val_loss: 0.1277 - val_auc: 0.9883 - val_accuracy: 0.9589 - val_cost: 328.8750\n",
            "Epoch 20/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1166 - auc: 0.9900 - accuracy: 0.9627 - cost: 306.0625 - val_loss: 0.1261 - val_auc: 0.9884 - val_accuracy: 0.9605 - val_cost: 322.1250\n",
            "Epoch 21/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1134 - auc: 0.9904 - accuracy: 0.9630 - cost: 302.5625 - val_loss: 0.1257 - val_auc: 0.9885 - val_accuracy: 0.9607 - val_cost: 323.3750\n",
            "Epoch 22/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1126 - auc: 0.9905 - accuracy: 0.9638 - cost: 296.9062 - val_loss: 0.1267 - val_auc: 0.9882 - val_accuracy: 0.9598 - val_cost: 337.6250\n",
            "Epoch 23/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1140 - auc: 0.9904 - accuracy: 0.9635 - cost: 299.7500 - val_loss: 0.1286 - val_auc: 0.9881 - val_accuracy: 0.9596 - val_cost: 340.1250\n",
            "Epoch 24/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1099 - auc: 0.9909 - accuracy: 0.9654 - cost: 282.9375 - val_loss: 0.1273 - val_auc: 0.9883 - val_accuracy: 0.9609 - val_cost: 328.2500\n",
            "Epoch 25/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1100 - auc: 0.9908 - accuracy: 0.9651 - cost: 286.3125 - val_loss: 0.1248 - val_auc: 0.9885 - val_accuracy: 0.9622 - val_cost: 309.2500\n",
            "Epoch 26/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1089 - auc: 0.9910 - accuracy: 0.9656 - cost: 281.9062 - val_loss: 0.1221 - val_auc: 0.9889 - val_accuracy: 0.9624 - val_cost: 307.5000\n",
            "Epoch 27/100\n",
            "1600/1600 [==============================] - 6s 4ms/step - loss: 0.1072 - auc: 0.9910 - accuracy: 0.9663 - cost: 277.4062 - val_loss: 0.1233 - val_auc: 0.9887 - val_accuracy: 0.9614 - val_cost: 312.1250\n",
            "Epoch 28/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1060 - auc: 0.9913 - accuracy: 0.9667 - cost: 273.5938 - val_loss: 0.1280 - val_auc: 0.9883 - val_accuracy: 0.9612 - val_cost: 312.3750\n",
            "Epoch 29/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1067 - auc: 0.9912 - accuracy: 0.9668 - cost: 272.8438 - val_loss: 0.1264 - val_auc: 0.9882 - val_accuracy: 0.9617 - val_cost: 306.2500\n",
            "Epoch 30/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1050 - auc: 0.9913 - accuracy: 0.9673 - cost: 269.0312 - val_loss: 0.1260 - val_auc: 0.9886 - val_accuracy: 0.9629 - val_cost: 308.1250\n",
            "Epoch 31/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1044 - auc: 0.9915 - accuracy: 0.9675 - cost: 267.8438 - val_loss: 0.1223 - val_auc: 0.9893 - val_accuracy: 0.9637 - val_cost: 288.5000\n",
            "Epoch 32/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1032 - auc: 0.9917 - accuracy: 0.9674 - cost: 267.6250 - val_loss: 0.1205 - val_auc: 0.9893 - val_accuracy: 0.9630 - val_cost: 308.1250\n",
            "Epoch 33/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1017 - auc: 0.9917 - accuracy: 0.9690 - cost: 254.0625 - val_loss: 0.1216 - val_auc: 0.9892 - val_accuracy: 0.9634 - val_cost: 293.0000\n",
            "Epoch 34/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1025 - auc: 0.9918 - accuracy: 0.9677 - cost: 265.8438 - val_loss: 0.1220 - val_auc: 0.9890 - val_accuracy: 0.9637 - val_cost: 306.3750\n",
            "Epoch 35/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1001 - auc: 0.9921 - accuracy: 0.9683 - cost: 261.2188 - val_loss: 0.1233 - val_auc: 0.9891 - val_accuracy: 0.9631 - val_cost: 289.1250\n",
            "Epoch 36/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0995 - auc: 0.9921 - accuracy: 0.9694 - cost: 251.8125 - val_loss: 0.1239 - val_auc: 0.9890 - val_accuracy: 0.9642 - val_cost: 296.6250\n",
            "Epoch 37/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0994 - auc: 0.9922 - accuracy: 0.9686 - cost: 258.1250 - val_loss: 0.1207 - val_auc: 0.9892 - val_accuracy: 0.9639 - val_cost: 291.7500\n",
            "Epoch 38/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0978 - auc: 0.9922 - accuracy: 0.9701 - cost: 246.1875 - val_loss: 0.1170 - val_auc: 0.9894 - val_accuracy: 0.9656 - val_cost: 278.0000\n",
            "Epoch 39/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0962 - auc: 0.9926 - accuracy: 0.9701 - cost: 245.2812 - val_loss: 0.1249 - val_auc: 0.9891 - val_accuracy: 0.9634 - val_cost: 305.2500\n",
            "Epoch 40/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0964 - auc: 0.9924 - accuracy: 0.9700 - cost: 247.8438 - val_loss: 0.1241 - val_auc: 0.9892 - val_accuracy: 0.9646 - val_cost: 274.0000\n",
            "Epoch 41/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0970 - auc: 0.9925 - accuracy: 0.9695 - cost: 250.9062 - val_loss: 0.1221 - val_auc: 0.9894 - val_accuracy: 0.9648 - val_cost: 285.8750\n",
            "Epoch 42/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0959 - auc: 0.9925 - accuracy: 0.9703 - cost: 245.0625 - val_loss: 0.1205 - val_auc: 0.9895 - val_accuracy: 0.9652 - val_cost: 281.5000\n",
            "Epoch 43/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0947 - auc: 0.9926 - accuracy: 0.9703 - cost: 244.3750 - val_loss: 0.1206 - val_auc: 0.9894 - val_accuracy: 0.9650 - val_cost: 292.0000\n",
            "Epoch 44/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0943 - auc: 0.9927 - accuracy: 0.9711 - cost: 237.1562 - val_loss: 0.1214 - val_auc: 0.9892 - val_accuracy: 0.9644 - val_cost: 295.7500\n",
            "Epoch 45/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0949 - auc: 0.9927 - accuracy: 0.9711 - cost: 237.9688 - val_loss: 0.1197 - val_auc: 0.9895 - val_accuracy: 0.9646 - val_cost: 286.7500\n",
            "Epoch 46/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0944 - auc: 0.9927 - accuracy: 0.9711 - cost: 238.6562 - val_loss: 0.1201 - val_auc: 0.9894 - val_accuracy: 0.9653 - val_cost: 274.0000\n",
            "Epoch 47/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0935 - auc: 0.9929 - accuracy: 0.9708 - cost: 240.3750 - val_loss: 0.1199 - val_auc: 0.9896 - val_accuracy: 0.9652 - val_cost: 276.3750\n",
            "Epoch 48/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0944 - auc: 0.9928 - accuracy: 0.9706 - cost: 242.7812 - val_loss: 0.1184 - val_auc: 0.9895 - val_accuracy: 0.9647 - val_cost: 289.6250\n",
            "Epoch 49/100\n",
            "1600/1600 [==============================] - 6s 3ms/step - loss: 0.0922 - auc: 0.9931 - accuracy: 0.9716 - cost: 233.8438 - val_loss: 0.1227 - val_auc: 0.9892 - val_accuracy: 0.9651 - val_cost: 283.8750\n",
            "Epoch 50/100\n",
            "1600/1600 [==============================] - 6s 4ms/step - loss: 0.0920 - auc: 0.9931 - accuracy: 0.9720 - cost: 231.1875 - val_loss: 0.1206 - val_auc: 0.9897 - val_accuracy: 0.9658 - val_cost: 271.6250\n",
            "Epoch 51/100\n",
            "1600/1600 [==============================] - 6s 4ms/step - loss: 0.0918 - auc: 0.9930 - accuracy: 0.9721 - cost: 230.1250 - val_loss: 0.1225 - val_auc: 0.9894 - val_accuracy: 0.9648 - val_cost: 291.0000\n",
            "Epoch 52/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0909 - auc: 0.9932 - accuracy: 0.9720 - cost: 230.6875 - val_loss: 0.1182 - val_auc: 0.9894 - val_accuracy: 0.9659 - val_cost: 282.3750\n",
            "Epoch 53/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0902 - auc: 0.9933 - accuracy: 0.9727 - cost: 225.1562 - val_loss: 0.1224 - val_auc: 0.9893 - val_accuracy: 0.9656 - val_cost: 274.5000\n",
            "Epoch 54/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0914 - auc: 0.9932 - accuracy: 0.9724 - cost: 227.4688 - val_loss: 0.1178 - val_auc: 0.9898 - val_accuracy: 0.9663 - val_cost: 276.3750\n",
            "Epoch 55/100\n",
            "1600/1600 [==============================] - 4s 3ms/step - loss: 0.0907 - auc: 0.9931 - accuracy: 0.9723 - cost: 227.7812 - val_loss: 0.1180 - val_auc: 0.9895 - val_accuracy: 0.9659 - val_cost: 279.3750\n",
            "Epoch 56/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0912 - auc: 0.9931 - accuracy: 0.9726 - cost: 224.9375 - val_loss: 0.1163 - val_auc: 0.9900 - val_accuracy: 0.9675 - val_cost: 265.0000\n",
            "Epoch 57/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0887 - auc: 0.9933 - accuracy: 0.9733 - cost: 219.6562 - val_loss: 0.1153 - val_auc: 0.9901 - val_accuracy: 0.9662 - val_cost: 280.1250\n",
            "Epoch 58/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0870 - auc: 0.9935 - accuracy: 0.9737 - cost: 216.1562 - val_loss: 0.1164 - val_auc: 0.9900 - val_accuracy: 0.9666 - val_cost: 269.0000\n",
            "Epoch 59/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0889 - auc: 0.9933 - accuracy: 0.9728 - cost: 223.0938 - val_loss: 0.1164 - val_auc: 0.9896 - val_accuracy: 0.9667 - val_cost: 273.2500\n",
            "Epoch 60/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0879 - auc: 0.9935 - accuracy: 0.9731 - cost: 221.8438 - val_loss: 0.1186 - val_auc: 0.9896 - val_accuracy: 0.9667 - val_cost: 268.6250\n",
            "Epoch 61/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0886 - auc: 0.9934 - accuracy: 0.9729 - cost: 223.2188 - val_loss: 0.1183 - val_auc: 0.9895 - val_accuracy: 0.9672 - val_cost: 262.7500\n",
            "Epoch 62/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0873 - auc: 0.9935 - accuracy: 0.9736 - cost: 217.9375 - val_loss: 0.1210 - val_auc: 0.9891 - val_accuracy: 0.9670 - val_cost: 271.7500\n",
            "Epoch 63/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0874 - auc: 0.9935 - accuracy: 0.9740 - cost: 213.4688 - val_loss: 0.1179 - val_auc: 0.9896 - val_accuracy: 0.9664 - val_cost: 273.1250\n",
            "Epoch 64/100\n",
            "1600/1600 [==============================] - 6s 4ms/step - loss: 0.0867 - auc: 0.9936 - accuracy: 0.9737 - cost: 217.7500 - val_loss: 0.1190 - val_auc: 0.9895 - val_accuracy: 0.9662 - val_cost: 278.2500\n",
            "Epoch 65/100\n",
            "1600/1600 [==============================] - 6s 4ms/step - loss: 0.0860 - auc: 0.9937 - accuracy: 0.9735 - cost: 218.3125 - val_loss: 0.1197 - val_auc: 0.9894 - val_accuracy: 0.9663 - val_cost: 274.8750\n",
            "Epoch 66/100\n",
            "1600/1600 [==============================] - 6s 4ms/step - loss: 0.0879 - auc: 0.9935 - accuracy: 0.9736 - cost: 217.7500 - val_loss: 0.1200 - val_auc: 0.9896 - val_accuracy: 0.9657 - val_cost: 278.7500\n",
            "Epoch 67/100\n",
            "1600/1600 [==============================] - 6s 4ms/step - loss: 0.0861 - auc: 0.9938 - accuracy: 0.9735 - cost: 218.6875 - val_loss: 0.1191 - val_auc: 0.9896 - val_accuracy: 0.9671 - val_cost: 271.3750\n",
            "Epoch 68/100\n",
            "1600/1600 [==============================] - 6s 4ms/step - loss: 0.0853 - auc: 0.9937 - accuracy: 0.9742 - cost: 212.8438 - val_loss: 0.1184 - val_auc: 0.9896 - val_accuracy: 0.9677 - val_cost: 265.7500\n",
            "Epoch 69/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0860 - auc: 0.9936 - accuracy: 0.9741 - cost: 213.9062 - val_loss: 0.1199 - val_auc: 0.9897 - val_accuracy: 0.9674 - val_cost: 266.6250\n",
            "Epoch 70/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0857 - auc: 0.9939 - accuracy: 0.9740 - cost: 214.1250 - val_loss: 0.1190 - val_auc: 0.9895 - val_accuracy: 0.9669 - val_cost: 263.2500\n",
            "Epoch 71/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0856 - auc: 0.9937 - accuracy: 0.9741 - cost: 213.8125 - val_loss: 0.1227 - val_auc: 0.9893 - val_accuracy: 0.9674 - val_cost: 264.3750\n",
            "Epoch 72/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0848 - auc: 0.9938 - accuracy: 0.9747 - cost: 208.7188 - val_loss: 0.1217 - val_auc: 0.9893 - val_accuracy: 0.9666 - val_cost: 269.8750\n",
            "Epoch 73/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0852 - auc: 0.9938 - accuracy: 0.9742 - cost: 212.7188 - val_loss: 0.1185 - val_auc: 0.9895 - val_accuracy: 0.9668 - val_cost: 268.6250\n",
            "Epoch 74/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0846 - auc: 0.9938 - accuracy: 0.9748 - cost: 208.1562 - val_loss: 0.1183 - val_auc: 0.9894 - val_accuracy: 0.9674 - val_cost: 264.8750\n",
            "Epoch 75/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0849 - auc: 0.9938 - accuracy: 0.9749 - cost: 207.1250 - val_loss: 0.1187 - val_auc: 0.9896 - val_accuracy: 0.9671 - val_cost: 265.1250\n",
            "Epoch 76/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0838 - auc: 0.9941 - accuracy: 0.9751 - cost: 206.3438 - val_loss: 0.1214 - val_auc: 0.9895 - val_accuracy: 0.9658 - val_cost: 279.7500\n",
            "1000/1000 [==============================] - 1s 994us/step\n",
            "fold train/predict time: 0:06:16.092511\n",
            "total train/predict time: 0:32:56.898563\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m5_results = fold_results"
      ],
      "metadata": {
        "id": "h8ySkCi9mPHh"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds_m5 = np.zeros(len(y))\n",
        "for i in fold_results.keys():\n",
        "  for j in range(len(fold_results.get(i).get('predictions'))):\n",
        "    idx = fold_results.get(i).get('index')[j]\n",
        "    preds_m5[idx] = np.round(fold_results.get(i).get('predictions')[j],0)\n",
        "\n",
        "m5_cost = cost_func(y,preds_m5)\n",
        "m5_cost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkrG6or3mRV4",
        "outputId": "c503f16c-4217-4cc7-d8a1-c685ecd05f51"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "624950"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds_new = np.zeros(len(y))\n",
        "for i in fold_results.keys():\n",
        "  for j in range(len(fold_results.get(i).get('predictions'))):\n",
        "    idx = fold_results.get(i).get('index')[j]\n",
        "    if fold_results.get(i).get('predictions')[j] > 0.6:\n",
        "      preds_new[idx] = 1\n",
        "    else:\n",
        "      preds_new[idx] = 0\n",
        "\n",
        "m5_cost_t = cost_func(y,preds_new)\n",
        "m5_cost_t"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YGSE-MSmS5Q",
        "outputId": "1635a2c3-90c3-4a57-8780-d5cc62dd6645"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "619450"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model5.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xhEJqvbmWkK",
        "outputId": "3eda5e3e-ae59-411d-b082-c23cd77561f2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " Relu1 (Dense)               (None, 64)                4352      \n",
            "                                                                 \n",
            " Relu2 (Dense)               (None, 64)                4160      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " Activation (Dense)          (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8,577\n",
            "Trainable params: 8,577\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### XGBoost"
      ],
      "metadata": {
        "id": "kf48YNxF5eEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#setting up a hyperparameter dataframe\n",
        "\n",
        "learning_rates = [0.1, 0.25, 0.35]\n",
        "max_depths = [3, 5, 10, 20]\n",
        "gamma = [0,1,3]\n",
        "lambda_ls = [1,2,3]\n",
        "alpha = [0,0.1,1]\n",
        "\n",
        "xgb_param = pd.DataFrame(list(product(learning_rates, max_depths, gamma, lambda_ls, alpha)), columns=['learning_rate', 'max_depth', 'gamma', 'lambda', 'alpha'])\n",
        "\n",
        "#randomizing the dataframe order\n",
        "xgb_param = shuffle(xgb_param)\n",
        "xgb_param = xgb_param.reset_index()\n",
        "xgb_param=xgb_param.drop(['index'], axis=1)\n",
        "xgb_param"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "crFxa8d68wBQ",
        "outputId": "f5499640-776b-4430-d734-4d2866c6ba56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     learning_rate  max_depth  gamma  lambda  alpha\n",
              "0             0.25         10      3       1    1.0\n",
              "1             0.25          3      0       1    0.1\n",
              "2             0.10          3      3       1    0.1\n",
              "3             0.35         10      3       1    0.0\n",
              "4             0.25          3      1       1    0.1\n",
              "..             ...        ...    ...     ...    ...\n",
              "319           0.25         10      0       1    0.0\n",
              "320           0.25         20      0       1    1.0\n",
              "321           0.10         10      1       2    0.0\n",
              "322           0.25         10      1       3    0.0\n",
              "323           0.25         20      1       1    0.1\n",
              "\n",
              "[324 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-3fc1fb84-2166-4548-a357-c0b744ab3a96\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>learning_rate</th>\n",
              "      <th>max_depth</th>\n",
              "      <th>gamma</th>\n",
              "      <th>lambda</th>\n",
              "      <th>alpha</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.25</td>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.25</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.10</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.35</td>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.25</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>319</th>\n",
              "      <td>0.25</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320</th>\n",
              "      <td>0.25</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>321</th>\n",
              "      <td>0.10</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>322</th>\n",
              "      <td>0.25</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>323</th>\n",
              "      <td>0.25</td>\n",
              "      <td>20</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>324 rows  5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3fc1fb84-2166-4548-a357-c0b744ab3a96')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-4f41c428-470d-4298-b387-ffaa6a7fff13\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4f41c428-470d-4298-b387-ffaa6a7fff13')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-4f41c428-470d-4298-b387-ffaa6a7fff13 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3fc1fb84-2166-4548-a357-c0b744ab3a96 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3fc1fb84-2166-4548-a357-c0b744ab3a96');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reference https://xgboost.readthedocs.io/en/stable/tutorials/custom_metric_obj.html\n",
        "from typing import Tuple\n",
        "\n",
        "\n",
        "def cost_obj(predt: np.ndarray, dtrain: xgb.DMatrix) -> Tuple[str, float]:\n",
        "    ''' Root mean squared log error metric.'''\n",
        "    y = dtrain.get_label()\n",
        "\n",
        "    #adjust to reflect customized loss based on project\n",
        "    elements = cost(predt, y)\n",
        "    return 'cost_obj', float(np.sum(elements))"
      ],
      "metadata": {
        "id": "4phrKLGB5kGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dtrain = xgb.DMatrix(X, label=y, enable_categorical=True)\n",
        "\n",
        "trials = 20\n",
        "best_params = {}\n",
        "i=0\n",
        "\n",
        "for i in range(trials):\n",
        "  #random sampling from paramdf\n",
        "  hyperparams = {'objective': 'binary:hinge',\n",
        "                 'eta': xgb_param['learning_rate'][i],\n",
        "                 'max_depth': xgb_param['max_depth'][i],\n",
        "                 'gamma': xgb_param['gamma'][i],\n",
        "                 'lambda': xgb_param['lambda'][i],\n",
        "                 'alpha': xgb_param['alpha'][i]\n",
        "                 }\n",
        "\n",
        "  print(hyperparams)\n",
        "  out=xgb.cv(params=hyperparams,\n",
        "             num_boost_round=50,\n",
        "             dtrain=dtrain,\n",
        "             nfold=10,\n",
        "             stratified=True,\n",
        "             early_stopping_rounds=5,\n",
        "             verbose_eval=1,\n",
        "             custom_metric= cost_obj\n",
        "             )\n",
        "\n",
        "  index=out.shape[0]-1\n",
        "  result=out.iloc[index,2]\n",
        "  if i< 1:\n",
        "    best_result = result\n",
        "    best_params = hyperparams\n",
        "\n",
        "  if result< best_result:\n",
        "      best_result = result\n",
        "      best_params = hyperparams\n",
        "      print('result: ' ,result)\n",
        "      print('best result: ' ,best_result)\n",
        "      print('hyperparameters: ' ,hyperparams)\n",
        "      print('best hyperparameters: ' ,best_params)\n",
        "      i+=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejnx8bTi5lsU",
        "outputId": "200d2549-b5a6-4ef2-9423-65b64d514894"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'objective': 'binary:hinge', 'eta': 0.25, 'max_depth': 10, 'gamma': 3, 'lambda': 1, 'alpha': 1.0}\n",
            "[0]\ttrain-error:0.59877+0.00000\ttrain-cost_obj:8622270.00000+45.82576\ttest-error:0.59877+0.00003\ttest-cost_obj:958030.00000+45.82576\n",
            "[1]\ttrain-error:0.59877+0.00000\ttrain-cost_obj:8622270.00000+45.82576\ttest-error:0.59877+0.00003\ttest-cost_obj:958030.00000+45.82576\n",
            "[2]\ttrain-error:0.20036+0.01317\ttrain-cost_obj:3096000.00000+171947.51816\ttest-error:0.21478+0.01163\ttest-cost_obj:372665.00000+15894.88990\n",
            "[3]\ttrain-error:0.14273+0.00238\ttrain-cost_obj:2272685.00000+30614.65376\ttest-error:0.16229+0.00340\ttest-cost_obj:290990.00000+5855.16012\n",
            "[4]\ttrain-error:0.12042+0.00147\ttrain-cost_obj:1971745.00000+21817.86481\ttest-error:0.14119+0.00346\ttest-cost_obj:260185.00000+6307.10116\n",
            "[5]\ttrain-error:0.10637+0.00104\ttrain-cost_obj:1781970.00000+17552.32463\ttest-error:0.12851+0.00245\ttest-cost_obj:242005.00000+5121.54518\n",
            "[6]\ttrain-error:0.09730+0.00109\ttrain-cost_obj:1665175.00000+18289.20788\ttest-error:0.12046+0.00252\ttest-cost_obj:231450.00000+4873.70496\n",
            "[7]\ttrain-error:0.08840+0.00115\ttrain-cost_obj:1538115.00000+19199.16730\ttest-error:0.11148+0.00297\ttest-cost_obj:217415.00000+5812.10160\n",
            "[8]\ttrain-error:0.08271+0.00144\ttrain-cost_obj:1460395.00000+24929.06386\ttest-error:0.10624+0.00326\ttest-cost_obj:209910.00000+6350.03150\n",
            "[9]\ttrain-error:0.07850+0.00141\ttrain-cost_obj:1399915.00000+23461.93779\ttest-error:0.10171+0.00350\ttest-cost_obj:202905.00000+6808.98120\n",
            "[10]\ttrain-error:0.07429+0.00144\ttrain-cost_obj:1334665.00000+26636.12256\ttest-error:0.09792+0.00299\ttest-cost_obj:196805.00000+5908.86833\n",
            "[11]\ttrain-error:0.07025+0.00081\ttrain-cost_obj:1268120.00000+16086.41041\ttest-error:0.09398+0.00278\ttest-cost_obj:189710.00000+5558.00324\n",
            "[12]\ttrain-error:0.06653+0.00104\ttrain-cost_obj:1205280.00000+20962.07528\ttest-error:0.09111+0.00292\ttest-cost_obj:184515.00000+5638.57473\n",
            "[13]\ttrain-error:0.06354+0.00218\ttrain-cost_obj:1154320.00000+41330.35325\ttest-error:0.08829+0.00270\ttest-cost_obj:179170.00000+5232.21750\n",
            "[14]\ttrain-error:0.06049+0.00250\ttrain-cost_obj:1101830.00000+46470.29804\ttest-error:0.08592+0.00287\ttest-cost_obj:174665.00000+5776.93907\n",
            "[15]\ttrain-error:0.05750+0.00274\ttrain-cost_obj:1048990.00000+50765.80936\ttest-error:0.08409+0.00311\ttest-cost_obj:171220.00000+6204.32107\n",
            "[16]\ttrain-error:0.05526+0.00275\ttrain-cost_obj:1009825.00000+50696.65793\ttest-error:0.08242+0.00311\ttest-cost_obj:168015.00000+6203.18668\n",
            "[17]\ttrain-error:0.05329+0.00294\ttrain-cost_obj:974935.00000+53656.81713\ttest-error:0.08148+0.00285\ttest-cost_obj:166285.00000+5746.21832\n",
            "[18]\ttrain-error:0.05113+0.00282\ttrain-cost_obj:936690.00000+51883.06949\ttest-error:0.07984+0.00279\ttest-cost_obj:163025.00000+5681.64809\n",
            "[19]\ttrain-error:0.04948+0.00269\ttrain-cost_obj:906905.00000+49859.56002\ttest-error:0.07885+0.00274\ttest-cost_obj:160915.00000+5792.36782\n",
            "[20]\ttrain-error:0.04810+0.00255\ttrain-cost_obj:881925.00000+47167.37882\ttest-error:0.07807+0.00279\ttest-cost_obj:159400.00000+5850.55553\n",
            "[21]\ttrain-error:0.04689+0.00266\ttrain-cost_obj:860415.00000+49539.13630\ttest-error:0.07734+0.00331\ttest-cost_obj:157975.00000+6884.62962\n",
            "[22]\ttrain-error:0.04548+0.00285\ttrain-cost_obj:834545.00000+52677.83429\ttest-error:0.07642+0.00341\ttest-cost_obj:156160.00000+7084.48304\n",
            "[23]\ttrain-error:0.04409+0.00286\ttrain-cost_obj:809175.00000+53332.89440\ttest-error:0.07556+0.00293\ttest-cost_obj:154480.00000+6210.00000\n",
            "[24]\ttrain-error:0.04278+0.00262\ttrain-cost_obj:785210.00000+48988.26798\ttest-error:0.07495+0.00293\ttest-cost_obj:153315.00000+6292.45779\n",
            "[25]\ttrain-error:0.04164+0.00261\ttrain-cost_obj:765070.00000+48927.34001\ttest-error:0.07457+0.00276\ttest-cost_obj:152530.00000+5852.61480\n",
            "[26]\ttrain-error:0.04053+0.00284\ttrain-cost_obj:744860.00000+52973.38860\ttest-error:0.07376+0.00328\ttest-cost_obj:150895.00000+6744.79244\n",
            "[27]\ttrain-error:0.03958+0.00295\ttrain-cost_obj:727570.00000+55310.23504\ttest-error:0.07331+0.00324\ttest-cost_obj:149940.00000+6762.20378\n",
            "[28]\ttrain-error:0.03851+0.00283\ttrain-cost_obj:708235.00000+52802.32026\ttest-error:0.07350+0.00314\ttest-cost_obj:150390.00000+6463.54392\n",
            "[29]\ttrain-error:0.03745+0.00264\ttrain-cost_obj:688850.00000+49173.15833\ttest-error:0.07328+0.00314\ttest-cost_obj:149970.00000+6534.22528\n",
            "[30]\ttrain-error:0.03668+0.00289\ttrain-cost_obj:674895.00000+53440.51108\ttest-error:0.07319+0.00316\ttest-cost_obj:149665.00000+6469.81646\n",
            "[31]\ttrain-error:0.03600+0.00319\ttrain-cost_obj:662435.00000+59004.18650\ttest-error:0.07302+0.00317\ttest-cost_obj:149320.00000+6536.74996\n",
            "[32]\ttrain-error:0.03520+0.00326\ttrain-cost_obj:648090.00000+60457.26507\ttest-error:0.07294+0.00340\ttest-cost_obj:149185.00000+6987.13282\n",
            "[33]\ttrain-error:0.03450+0.00335\ttrain-cost_obj:635500.00000+62230.08517\ttest-error:0.07289+0.00346\ttest-cost_obj:149060.00000+7064.76468\n",
            "[34]\ttrain-error:0.03391+0.00346\ttrain-cost_obj:624485.00000+64538.20206\ttest-error:0.07282+0.00354\ttest-cost_obj:148920.00000+7279.42992\n",
            "[35]\ttrain-error:0.03311+0.00332\ttrain-cost_obj:609960.00000+61876.00019\ttest-error:0.07271+0.00294\ttest-cost_obj:148700.00000+5920.89520\n",
            "[36]\ttrain-error:0.03251+0.00334\ttrain-cost_obj:599090.00000+62064.64694\ttest-error:0.07236+0.00276\ttest-cost_obj:147975.00000+5551.72271\n",
            "[37]\ttrain-error:0.03188+0.00331\ttrain-cost_obj:587455.00000+61316.52489\ttest-error:0.07229+0.00276\ttest-cost_obj:147910.00000+5592.39662\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_params"
      ],
      "metadata": {
        "id": "xXJEj4W_8T-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparams = { 'eta':0.35, 'max_depth': 20, 'gamma': 0, 'lambda': 1, 'alpha': 1.0, 'objective': 'binary:logistic'}\n",
        "\n",
        "XGBModel = xgb.XGBClassifier(params=hyperparams, early_stopping_rounds=5)"
      ],
      "metadata": {
        "id": "1WsS7wIP8Y1W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}