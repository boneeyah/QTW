{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "mount_file_id": "https://github.com/KendallScott/QTW/blob/main/Case%20Study%207/CaseStudy7.ipynb",
      "authorship_tag": "ABX9TyNAYzQ+xkGSr4djadHAogXF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/boneeyah/QTW/blob/main/Case%20Study%207/CaseStudy7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OZryWKHR3JAd"
      },
      "outputs": [],
      "source": [
        "#!pip install scikit-learn-intelex\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearnex import patch_sklearn\n",
        "patch_sklearn()\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import NuSVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_val_predict\n",
        "import datetime"
      ],
      "metadata": {
        "id": "HrVUsV_b8Ezp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7e98688-e682-4338-cf16-f9203983f90b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-07-30 01:46:24.919997: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-30 01:46:25.623907: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('drive')"
      ],
      "metadata": {
        "id": "T6H6L4bc8YlZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df = pd.read_csv(\"drive/My Drive/final_project(5).csv\")\n",
        "df = pd.read_csv(\"/home/migue/Downloads/final_project(5).csv\")"
      ],
      "metadata": {
        "id": "hngzaeNL-EXs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()\n",
        "#there are NAs will need handling"
      ],
      "metadata": {
        "id": "kjzOue9y_SuK",
        "outputId": "ba1b23f4-43ae-46f8-ceaf-ae5113f003db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 160000 entries, 0 to 159999\n",
            "Data columns (total 51 columns):\n",
            " #   Column  Non-Null Count   Dtype  \n",
            "---  ------  --------------   -----  \n",
            " 0   x0      159974 non-null  float64\n",
            " 1   x1      159975 non-null  float64\n",
            " 2   x2      159962 non-null  float64\n",
            " 3   x3      159963 non-null  float64\n",
            " 4   x4      159974 non-null  float64\n",
            " 5   x5      159963 non-null  float64\n",
            " 6   x6      159974 non-null  float64\n",
            " 7   x7      159973 non-null  float64\n",
            " 8   x8      159979 non-null  float64\n",
            " 9   x9      159970 non-null  float64\n",
            " 10  x10     159957 non-null  float64\n",
            " 11  x11     159970 non-null  float64\n",
            " 12  x12     159964 non-null  float64\n",
            " 13  x13     159969 non-null  float64\n",
            " 14  x14     159966 non-null  float64\n",
            " 15  x15     159965 non-null  float64\n",
            " 16  x16     159974 non-null  float64\n",
            " 17  x17     159973 non-null  float64\n",
            " 18  x18     159960 non-null  float64\n",
            " 19  x19     159965 non-null  float64\n",
            " 20  x20     159962 non-null  float64\n",
            " 21  x21     159971 non-null  float64\n",
            " 22  x22     159973 non-null  float64\n",
            " 23  x23     159953 non-null  float64\n",
            " 24  x24     159972 non-null  object \n",
            " 25  x25     159978 non-null  float64\n",
            " 26  x26     159964 non-null  float64\n",
            " 27  x27     159970 non-null  float64\n",
            " 28  x28     159965 non-null  float64\n",
            " 29  x29     159970 non-null  object \n",
            " 30  x30     159970 non-null  object \n",
            " 31  x31     159961 non-null  float64\n",
            " 32  x32     159969 non-null  object \n",
            " 33  x33     159959 non-null  float64\n",
            " 34  x34     159959 non-null  float64\n",
            " 35  x35     159970 non-null  float64\n",
            " 36  x36     159973 non-null  float64\n",
            " 37  x37     159977 non-null  object \n",
            " 38  x38     159969 non-null  float64\n",
            " 39  x39     159977 non-null  float64\n",
            " 40  x40     159964 non-null  float64\n",
            " 41  x41     159960 non-null  float64\n",
            " 42  x42     159974 non-null  float64\n",
            " 43  x43     159963 non-null  float64\n",
            " 44  x44     159960 non-null  float64\n",
            " 45  x45     159971 non-null  float64\n",
            " 46  x46     159969 non-null  float64\n",
            " 47  x47     159963 non-null  float64\n",
            " 48  x48     159968 non-null  float64\n",
            " 49  x49     159968 non-null  float64\n",
            " 50  y       160000 non-null  int64  \n",
            "dtypes: float64(45), int64(1), object(5)\n",
            "memory usage: 62.3+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in df.columns:\n",
        "    if df[i].isna().sum() > 0:\n",
        "        print('{} has {}% NAs'.format(i,(df[i].isna().sum() / len(df))*100))"
      ],
      "metadata": {
        "id": "CC3yFJ6BzzOw",
        "outputId": "643d93df-1a5c-47cc-90f3-8d3a1fb71e80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x0 has 0.01625% NAs\n",
            "x1 has 0.015625% NAs\n",
            "x2 has 0.02375% NAs\n",
            "x3 has 0.023125% NAs\n",
            "x4 has 0.01625% NAs\n",
            "x5 has 0.023125% NAs\n",
            "x6 has 0.01625% NAs\n",
            "x7 has 0.016875% NAs\n",
            "x8 has 0.013125% NAs\n",
            "x9 has 0.01875% NAs\n",
            "x10 has 0.026875% NAs\n",
            "x11 has 0.01875% NAs\n",
            "x12 has 0.0225% NAs\n",
            "x13 has 0.019375% NAs\n",
            "x14 has 0.021249999999999998% NAs\n",
            "x15 has 0.021875000000000002% NAs\n",
            "x16 has 0.01625% NAs\n",
            "x17 has 0.016875% NAs\n",
            "x18 has 0.025% NAs\n",
            "x19 has 0.021875000000000002% NAs\n",
            "x20 has 0.02375% NAs\n",
            "x21 has 0.018125% NAs\n",
            "x22 has 0.016875% NAs\n",
            "x23 has 0.029375000000000002% NAs\n",
            "x24 has 0.017499999999999998% NAs\n",
            "x25 has 0.01375% NAs\n",
            "x26 has 0.0225% NAs\n",
            "x27 has 0.01875% NAs\n",
            "x28 has 0.021875000000000002% NAs\n",
            "x29 has 0.01875% NAs\n",
            "x30 has 0.01875% NAs\n",
            "x31 has 0.024374999999999997% NAs\n",
            "x32 has 0.019375% NAs\n",
            "x33 has 0.025625000000000002% NAs\n",
            "x34 has 0.025625000000000002% NAs\n",
            "x35 has 0.01875% NAs\n",
            "x36 has 0.016875% NAs\n",
            "x37 has 0.014374999999999999% NAs\n",
            "x38 has 0.019375% NAs\n",
            "x39 has 0.014374999999999999% NAs\n",
            "x40 has 0.0225% NAs\n",
            "x41 has 0.025% NAs\n",
            "x42 has 0.01625% NAs\n",
            "x43 has 0.023125% NAs\n",
            "x44 has 0.025% NAs\n",
            "x45 has 0.018125% NAs\n",
            "x46 has 0.019375% NAs\n",
            "x47 has 0.023125% NAs\n",
            "x48 has 0.02% NAs\n",
            "x49 has 0.02% NAs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['x24'].value_counts()"
      ],
      "metadata": {
        "id": "TybuIbMLz6Ng",
        "outputId": "11cded34-cecb-4275-9164-adf752903672",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "x24\n",
              "asia       138965\n",
              "euorpe      16538\n",
              "america      4469\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.bar(x=df['x24'].value_counts().index,height=df['x24'].value_counts().values)"
      ],
      "metadata": {
        "id": "ME56MKaIz7sN",
        "outputId": "db067d3a-86c5-4b4c-e3fe-b9bf56d7de2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 3 artists>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['x29'].value_counts()"
      ],
      "metadata": {
        "id": "P7rMlCalz9WH",
        "outputId": "c920a8a3-07e4-4605-9ebd-2b062a102d1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "x29\n",
              "July       45569\n",
              "Jun        41329\n",
              "Aug        29406\n",
              "May        21939\n",
              "sept.      10819\n",
              "Apr         6761\n",
              "Oct         2407\n",
              "Mar         1231\n",
              "Nov          337\n",
              "Feb          140\n",
              "Dev           23\n",
              "January        9\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#clean up month formatting to standard 3 letters\n",
        "df['x29'].replace(to_replace=['July','sept.','Dev','January'],value=['Jul','Sep','Dec','Jan'],inplace=True)"
      ],
      "metadata": {
        "id": "gB-gkjC2z_fK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mo = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\n",
        "heights = []\n",
        "for i in mo:\n",
        "    x = df['x29'].to_list().count(i)\n",
        "    heights.append(x)\n",
        "plt.bar(x=mo,height=heights)"
      ],
      "metadata": {
        "id": "eZTNOL9P0CM3",
        "outputId": "e12529da-71c4-4f59-e619-f9df52afa81c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 12 artists>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['x30'].value_counts()"
      ],
      "metadata": {
        "id": "_1LSKgy80ENF",
        "outputId": "0bf2b8c9-f3a8-4c49-a341-b8cc3a2e8ebd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "x30\n",
              "wednesday    101535\n",
              "thurday       29429\n",
              "tuesday       27954\n",
              "friday          564\n",
              "monday          488\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# thursday is incorrect\n",
        "df['x30'].replace(to_replace = 'thurday',value='thursday')\n",
        "days = ['monday','tuesday','wednesday','thursday','friday']\n",
        "d_heights = []\n",
        "for i in days:\n",
        "    x = df['x30'].to_list().count(i)\n",
        "    d_heights.append(x)\n",
        "plt.bar(x=days,height = d_heights)"
      ],
      "metadata": {
        "id": "3hFwgArL0GAB",
        "outputId": "a9ab0788-b63a-42dc-c571-83fea3209581",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 5 artists>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['x32'].value_counts()"
      ],
      "metadata": {
        "id": "WErTHaJ70Ia2",
        "outputId": "4fbbba0d-0c00-44b4-fd22-f6a3924628a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "x32\n",
              "0.01%     40767\n",
              "-0.01%    34094\n",
              "0.0%      33923\n",
              "-0.0%     30492\n",
              "-0.02%     9924\n",
              "0.02%      7987\n",
              "-0.03%     1727\n",
              "0.03%       855\n",
              "-0.04%      138\n",
              "0.04%        55\n",
              "-0.05%        6\n",
              "0.05%         1\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['x37'].value_counts()\n"
      ],
      "metadata": {
        "id": "r98Zehw50LS1",
        "outputId": "348e7378-3e83-4908-d8db-d1b898aaefdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "x37\n",
              "$-311.26     6\n",
              "$-336.77     6\n",
              "$237.4       6\n",
              "$72.42       6\n",
              "$341.26      6\n",
              "            ..\n",
              "$-505.21     1\n",
              "$770.07      1\n",
              "$74.62       1\n",
              "$-1082.96    1\n",
              "$-1229.34    1\n",
              "Name: count, Length: 129198, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### x32 and x37 are actually numeric with string format (even though x32 has few \"levels\" or possible values)\n",
        "### x32 is in percentage, but don't think it needs to be transformed to proportion since data will be scaled later (will double check, but should make no difference)\n",
        "\n",
        "df['x32'] = df['x32'].transform(lambda x: float(str(x).replace('%','')))\n",
        "df['x37'] = df['x37'].transform(lambda x: float(str(x).replace('$','')))"
      ],
      "metadata": {
        "id": "-8HSIli70VDX"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#all significanlty less than 5%, should be no issue imputing,\n",
        "#imputing categorical features with modes\n",
        "# imputing continent with mode, then dates by mode grouped by continent\n",
        "\n",
        "print('imputing continent with mode: {}'.format(df['x24'].aggregate(pd.Series.mode)))\n",
        "df['x24'] = df['x24'].transform(lambda x: x.fillna(pd.Series.mode(x)[0]))\n",
        "\n",
        "print('imputing month with mode by cont: {}'.format(df[['x24','x29']].groupby(by='x24').aggregate(pd.Series.mode)))\n",
        "idx_29 = df.loc[df['x29'].isna(),:].index #saving NAs indices to quick check what they've been imputed with\n",
        "df['x29'] = df[['x24','x29']].groupby(by='x24').transform(lambda grp: grp.fillna(pd.Series.mode(grp)[0]))\n",
        "\n",
        "# will fix index showing on print later\n"
      ],
      "metadata": {
        "id": "qyAc5R1u209T",
        "outputId": "ebe49a36-17e9-4f1d-ceed-6265be5eb697",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "imputing continent with mode: 0    asia\n",
            "Name: x24, dtype: object\n",
            "imputing month with mode by cont:          x29\n",
            "x24         \n",
            "america  Jul\n",
            "asia     Jul\n",
            "euorpe   Jul\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[['x24','x29','x30']].groupby(by=['x24','x29']).aggregate(pd.Series.mode)\n",
        "# could just impute with wednesday since Wed is mode per month, but adding continent gives more context and different days for some months"
      ],
      "metadata": {
        "id": "2iV02zDI3Emu",
        "outputId": "a05d07af-7a15-4924-88af-f8ea72c0410f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   x30\n",
              "x24     x29           \n",
              "america Apr  wednesday\n",
              "        Aug  wednesday\n",
              "        Feb    tuesday\n",
              "        Jul  wednesday\n",
              "        Jun  wednesday\n",
              "        Mar  wednesday\n",
              "        May  wednesday\n",
              "        Nov  wednesday\n",
              "        Oct  wednesday\n",
              "        Sep  wednesday\n",
              "asia    Apr  wednesday\n",
              "        Aug  wednesday\n",
              "        Dec  wednesday\n",
              "        Feb  wednesday\n",
              "        Jan  wednesday\n",
              "        Jul  wednesday\n",
              "        Jun  wednesday\n",
              "        Mar  wednesday\n",
              "        May  wednesday\n",
              "        Nov  wednesday\n",
              "        Oct  wednesday\n",
              "        Sep  wednesday\n",
              "euorpe  Apr  wednesday\n",
              "        Aug  wednesday\n",
              "        Dec  wednesday\n",
              "        Feb    tuesday\n",
              "        Jul  wednesday\n",
              "        Jun  wednesday\n",
              "        Mar  wednesday\n",
              "        May  wednesday\n",
              "        Nov  wednesday\n",
              "        Oct  wednesday\n",
              "        Sep  wednesday"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>x30</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>x24</th>\n",
              "      <th>x29</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"10\" valign=\"top\">america</th>\n",
              "      <th>Apr</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Aug</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Feb</th>\n",
              "      <td>tuesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Jul</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Jun</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Mar</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>May</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Nov</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Oct</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Sep</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"12\" valign=\"top\">asia</th>\n",
              "      <th>Apr</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Aug</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dec</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Feb</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Jan</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Jul</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Jun</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Mar</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>May</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Nov</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Oct</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Sep</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"11\" valign=\"top\">euorpe</th>\n",
              "      <th>Apr</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Aug</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dec</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Feb</th>\n",
              "      <td>tuesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Jul</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Jun</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Mar</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>May</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Nov</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Oct</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Sep</th>\n",
              "      <td>wednesday</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# impute day\n",
        "df['x30'] = df[['x24','x29','x30']].groupby(by=['x24','x29']).transform(lambda grp: grp.fillna(pd.Series.mode(grp)[0]))"
      ],
      "metadata": {
        "id": "HVQavir8IV9m"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.loc[:,df.columns != 'y']\n",
        "y = df['y'].values.flatten()\n",
        "\n",
        "df_num = X.select_dtypes(exclude='object')\n",
        "num_cols = df_num.columns\n",
        "\n",
        "# impute numeric with median\n",
        "for i in num_cols:\n",
        "    df_num[i] = df_num[i].transform(lambda x: x.fillna(pd.Series.mode(x)[0]))\n",
        "df_num.info()\n",
        "\n",
        "df_cat = X.select_dtypes(include='object')\n",
        "cat_cols = df_cat.columns\n",
        "\n",
        "# scale numerical data\n",
        "scaler = StandardScaler()\n",
        "df_num = scaler.fit_transform(df_num)\n",
        "\n",
        "#one hot encode cat data\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "df_cat = encoder.fit_transform(df_cat)"
      ],
      "metadata": {
        "id": "w8Uy9xG3Iern",
        "outputId": "684dc7d8-eec9-4442-972a-4e7bd0ecd7d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 160000 entries, 0 to 159999\n",
            "Data columns (total 47 columns):\n",
            " #   Column  Non-Null Count   Dtype  \n",
            "---  ------  --------------   -----  \n",
            " 0   x0      160000 non-null  float64\n",
            " 1   x1      160000 non-null  float64\n",
            " 2   x2      160000 non-null  float64\n",
            " 3   x3      160000 non-null  float64\n",
            " 4   x4      160000 non-null  float64\n",
            " 5   x5      160000 non-null  float64\n",
            " 6   x6      160000 non-null  float64\n",
            " 7   x7      160000 non-null  float64\n",
            " 8   x8      160000 non-null  float64\n",
            " 9   x9      160000 non-null  float64\n",
            " 10  x10     160000 non-null  float64\n",
            " 11  x11     160000 non-null  float64\n",
            " 12  x12     160000 non-null  float64\n",
            " 13  x13     160000 non-null  float64\n",
            " 14  x14     160000 non-null  float64\n",
            " 15  x15     160000 non-null  float64\n",
            " 16  x16     160000 non-null  float64\n",
            " 17  x17     160000 non-null  float64\n",
            " 18  x18     160000 non-null  float64\n",
            " 19  x19     160000 non-null  float64\n",
            " 20  x20     160000 non-null  float64\n",
            " 21  x21     160000 non-null  float64\n",
            " 22  x22     160000 non-null  float64\n",
            " 23  x23     160000 non-null  float64\n",
            " 24  x25     160000 non-null  float64\n",
            " 25  x26     160000 non-null  float64\n",
            " 26  x27     160000 non-null  float64\n",
            " 27  x28     160000 non-null  float64\n",
            " 28  x31     160000 non-null  float64\n",
            " 29  x32     160000 non-null  float64\n",
            " 30  x33     160000 non-null  float64\n",
            " 31  x34     160000 non-null  float64\n",
            " 32  x35     160000 non-null  float64\n",
            " 33  x36     160000 non-null  float64\n",
            " 34  x37     160000 non-null  float64\n",
            " 35  x38     160000 non-null  float64\n",
            " 36  x39     160000 non-null  float64\n",
            " 37  x40     160000 non-null  float64\n",
            " 38  x41     160000 non-null  float64\n",
            " 39  x42     160000 non-null  float64\n",
            " 40  x43     160000 non-null  float64\n",
            " 41  x44     160000 non-null  float64\n",
            " 42  x45     160000 non-null  float64\n",
            " 43  x46     160000 non-null  float64\n",
            " 44  x47     160000 non-null  float64\n",
            " 45  x48     160000 non-null  float64\n",
            " 46  x49     160000 non-null  float64\n",
            "dtypes: float64(47)\n",
            "memory usage: 57.4 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.hstack([df_num,df_cat])"
      ],
      "metadata": {
        "id": "R4oaw8CgIkXV"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['y'].value_counts() # not balanced"
      ],
      "metadata": {
        "id": "xbzdACUdImVy",
        "outputId": "e215c90f-c1c0-47cc-d063-710e49c7db61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "y\n",
              "0    95803\n",
              "1    64197\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#add y plot"
      ],
      "metadata": {
        "id": "vUXTcWUZIoot"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## define cost function\n",
        "def cost_func(y_true,y_pred):\n",
        "    diff = y_pred - y_true\n",
        "    x = 0\n",
        "    for i in diff:\n",
        "        #false pos (y_pred -y_true == 1)\n",
        "        if i == 1:\n",
        "            x += 100\n",
        "        #false neg (y_pred -y_true ==-1)\n",
        "        elif i == -1:\n",
        "            x += 150\n",
        "    return(x)"
      ],
      "metadata": {
        "id": "HRJz__nu_bxm"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "skf = StratifiedKFold(n_splits=5,random_state=807,shuffle=True)\n",
        "skf.get_n_splits(X,y)"
      ],
      "metadata": {
        "id": "qIgBnSnHIqjU",
        "outputId": "9eea42a8-d400-4ebd-c4a6-b063457b1980",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logR = LogisticRegression(random_state=807,penalty='elasticnet',class_weight='balanced',solver='saga')\n",
        "params = {'C':[0.00001,0.0001,0.001,0.01,0.1,1,2,5,10],'l1_ratio':[0,0.001,0.01,0.1,0.25,0.5,0.75,0.9,0.99,1],'max_iter':[25,50,75]}\n",
        "\n",
        "lr_clf = GridSearchCV(estimator=logR,param_grid=params,n_jobs=-1,cv=skf)\n",
        "lr_clf.fit(X,y)"
      ],
      "metadata": {
        "id": "AcpquSzJO6SF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_params = lr_clf.best_params_\n",
        "lr_params"
      ],
      "metadata": {
        "id": "Eb5e2xOvO8pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "{'C': 0.001, 'l1_ratio': 0.5, 'max_iter': 50}"
      ],
      "metadata": {
        "id": "gfllGKF4pUMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = LogisticRegression(random_state=807,penalty='elasticnet',class_weight='balanced',solver='saga')\n",
        "model1.set_params(**lr_params)\n",
        "model1.fit(X,y)\n",
        "\n",
        "preds_m1 = cross_val_predict(model1,X,y,cv=skf)"
      ],
      "metadata": {
        "id": "RHUkzlw2PATu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cr_m1 = classification_report(y,preds_m1,output_dict=True)\n",
        "cr_m1\n"
      ],
      "metadata": {
        "id": "3katCHeBPQB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ConfusionMatrixDisplay.from_predictions(y,preds_m1)\n"
      ],
      "metadata": {
        "id": "ent8XHdUPayB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cost_m1 = cost_func(preds_m1,y)\n",
        "cost_m1"
      ],
      "metadata": {
        "id": "zawbBlwk_fbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier(random_state=807,criterion='gini',class_weight='balanced')\n",
        "params = {'n_estimators':[200,250,500],'max_features':['sqrt','log2',20]}\n",
        "rf_clf = GridSearchCV(rf,param_grid=params,cv=skf,n_jobs = -1)"
      ],
      "metadata": {
        "id": "4HWotr_VPhhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_clf.fit(X,y)"
      ],
      "metadata": {
        "id": "soQp5nQ8PkUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_params = rf_clf.best_params_\n",
        "rf_params"
      ],
      "metadata": {
        "id": "Hz6owkMTPlOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = RandomForestClassifier(random_state=807,criterion='gini',class_weight='balanced')\n",
        "model2.set_params(**rf_params)\n",
        "preds_m2 = cross_val_predict(model2,X,y,cv=skf)"
      ],
      "metadata": {
        "id": "hvIbks5bPnfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cr_m2 = classification_report(y,preds_m2,output_dict=True)\n",
        "cr_m2"
      ],
      "metadata": {
        "id": "Aaw8aFI7PpCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ConfusionMatrixDisplay.from_predictions(y,preds_m2)"
      ],
      "metadata": {
        "id": "BK6zSOo4Pq_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cost_m2 = cost_func(preds_m2,y)\n",
        "cost_m2\n"
      ],
      "metadata": {
        "id": "XqmhfmAx9Ge2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svc = NuSVC(random_state=807,class_weight='balanced',cache_size = 8192)\n",
        "params = {'nu':[0.0001,0.01, 0.1, 0.25, 0.5, 0.75, 0.99,1],'kernel':['linear','rbf','sigmoid']}\n",
        "svc_clf = GridSearchCV(svc,param_grid=params,cv=skf,n_jobs=-1)"
      ],
      "metadata": {
        "id": "zYBX6GGsPwEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svc_clf.fit(X,y)"
      ],
      "metadata": {
        "id": "EjZRywoCP6hm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svc_params = svc_clf.best_params_\n",
        "svc_params"
      ],
      "metadata": {
        "id": "yYiNL9AlP98A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3 = NuSVC(random_state=807,class_weight='balanced',cache_size = 2048)\n",
        "model3.set_params(**svc_params)\n",
        "preds_m3 = cross_val_predict(model3,X,y,cv=skf)"
      ],
      "metadata": {
        "id": "_58pwDIoQGHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cr_m3 = classification_report(y,preds_m3,output_dict=True)\n",
        "cr_m3"
      ],
      "metadata": {
        "id": "5AVAi2z8QPOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ConfusionMatrixDisplay.from_predictions(y,preds_m3)"
      ],
      "metadata": {
        "id": "j5nGHzRUQUOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cost_m3 = cost_func(preds_m3,y)\n",
        "cost_m3"
      ],
      "metadata": {
        "id": "sefhH4BT-Rd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "8946dca4-81fe-4bfb-be64-a384b53f9eb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cost_m3 \u001b[38;5;241m=\u001b[39m cost(\u001b[43mpreds_m3\u001b[49m,y)\n\u001b[1;32m      2\u001b[0m cost_m3\n",
            "\u001b[0;31mNameError\u001b[0m: name 'preds_m3' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "now = datetime.datetime.now\n",
        "import tensorflow.keras.backend as K\n"
      ],
      "metadata": {
        "id": "b04x7Mf5dBDd"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### doesn't work fully because predictions aren't binary\n",
        "def cost(y_true,y_pred):\n",
        "  diff = y_pred-y_true\n",
        "\n",
        "  error = K.switch(\n",
        "      K.equal(diff,1),K.constant(100,shape=y_pred.shape),\n",
        "      K.switch(\n",
        "          K.equal(diff,-1),K.constant(150,shape=y_pred.shape),\n",
        "          K.constant(0,shape=y_pred.shape)\n",
        "      )\n",
        "  )\n",
        "  return(K.sum(error))"
      ],
      "metadata": {
        "id": "h_kr7PGjLb42"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sets binary values for predictions\n",
        "def cost(y_true,y_pred):\n",
        "\n",
        "  bin_p = K.switch(K.greater_equal(y_pred,0.5),K.constant(1,shape=y_pred.shape),\n",
        "                   K.constant(0,shape=y_pred.shape))\n",
        "  diff = bin_p-y_true\n",
        "\n",
        "  error = K.switch(\n",
        "      K.equal(diff,1),K.constant(100,shape=y_pred.shape),\n",
        "      K.switch(\n",
        "          K.equal(diff,-1),K.constant(150,shape=y_pred.shape),\n",
        "          K.constant(0,shape=y_pred.shape)\n",
        "      )\n",
        "  )\n",
        "  return(K.sum(error))"
      ],
      "metadata": {
        "id": "4R5oVbvcO0V1"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "es = EarlyStopping(monitor='val_cost', mode='min',patience=10,restore_best_weights=True,start_from_epoch=40)\n",
        "fold_results = {}\n",
        "t_tot = now()\n",
        "for i, (train_index, test_index) in enumerate(skf.split(X,y)):\n",
        "  # building model within loop to reset weights each time\n",
        "  tf.keras.backend.clear_session()\n",
        "  tf.keras.utils.set_random_seed(807)\n",
        "  model4 = tf.keras.Sequential()\n",
        "  model4.add(tf.keras.Input(shape=(X.shape[1],)))\n",
        "  model4.add(tf.keras.layers.Dense(64, activation='relu',name=\"Relu1\"))\n",
        "  model4.add(tf.keras.layers.Dense(64, activation='relu', name=\"Relu2\"))\n",
        "  model4.add(tf.keras.layers.Dropout(0.4))\n",
        "  model4.add(tf.keras.layers.Dense(1, activation='sigmoid', name=\"Activation\"))\n",
        "  model4.compile(optimizer='adam',\n",
        "                loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "                metrics = [tf.keras.metrics.AUC(from_logits=False), 'accuracy',cost])\n",
        "  t_fold = now()\n",
        "  print('x_train shape:', X[train_index].shape)\n",
        "  model4.fit(X[train_index],y[train_index],epochs=100,batch_size=64,validation_split=0.2,callbacks=[es])\n",
        "  fold_results.update({i:{'predictions':model4.predict(X[test_index]).flatten(),'index':test_index,'y_true':y[test_index]}})\n",
        "  print('fold train/predict time: %s' % (now()-t_fold))\n",
        "print('total train/predict time: %s' % (now()-t_tot))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEjPIqNtVPF1",
        "outputId": "cf9e24b6-085b-4a4b-c452-4d5223e105fb"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (128000, 67)\n",
            "Epoch 1/100\n",
            "1600/1600 [==============================] - 6s 3ms/step - loss: 0.3797 - auc: 0.9067 - accuracy: 0.8289 - cost: 1409.5000 - val_loss: 0.2840 - val_auc: 0.9493 - val_accuracy: 0.8816 - val_cost: 975.0000\n",
            "Epoch 2/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.2641 - auc: 0.9560 - accuracy: 0.8947 - cost: 853.0000 - val_loss: 0.2302 - val_auc: 0.9666 - val_accuracy: 0.9101 - val_cost: 718.2500\n",
            "Epoch 3/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.2246 - auc: 0.9681 - accuracy: 0.9130 - cost: 705.7812 - val_loss: 0.1983 - val_auc: 0.9750 - val_accuracy: 0.9253 - val_cost: 602.5000\n",
            "Epoch 4/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1993 - auc: 0.9747 - accuracy: 0.9264 - cost: 597.6562 - val_loss: 0.1778 - val_auc: 0.9799 - val_accuracy: 0.9355 - val_cost: 519.3750\n",
            "Epoch 5/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1824 - auc: 0.9785 - accuracy: 0.9345 - cost: 533.4688 - val_loss: 0.1682 - val_auc: 0.9815 - val_accuracy: 0.9398 - val_cost: 482.5000\n",
            "Epoch 6/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1710 - auc: 0.9810 - accuracy: 0.9388 - cost: 498.8750 - val_loss: 0.1587 - val_auc: 0.9833 - val_accuracy: 0.9455 - val_cost: 440.5000\n",
            "Epoch 7/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1629 - auc: 0.9826 - accuracy: 0.9422 - cost: 471.0000 - val_loss: 0.1553 - val_auc: 0.9840 - val_accuracy: 0.9443 - val_cost: 458.3750\n",
            "Epoch 8/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1574 - auc: 0.9836 - accuracy: 0.9441 - cost: 454.9375 - val_loss: 0.1498 - val_auc: 0.9850 - val_accuracy: 0.9476 - val_cost: 435.3750\n",
            "Epoch 9/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1517 - auc: 0.9847 - accuracy: 0.9481 - cost: 423.5938 - val_loss: 0.1466 - val_auc: 0.9855 - val_accuracy: 0.9500 - val_cost: 395.5000\n",
            "Epoch 10/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1464 - auc: 0.9856 - accuracy: 0.9504 - cost: 403.6562 - val_loss: 0.1447 - val_auc: 0.9860 - val_accuracy: 0.9514 - val_cost: 394.6250\n",
            "Epoch 11/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1436 - auc: 0.9860 - accuracy: 0.9506 - cost: 404.0312 - val_loss: 0.1395 - val_auc: 0.9865 - val_accuracy: 0.9534 - val_cost: 383.1250\n",
            "Epoch 12/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1397 - auc: 0.9867 - accuracy: 0.9531 - cost: 382.3750 - val_loss: 0.1367 - val_auc: 0.9872 - val_accuracy: 0.9555 - val_cost: 370.3750\n",
            "Epoch 13/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1368 - auc: 0.9873 - accuracy: 0.9543 - cost: 374.2500 - val_loss: 0.1384 - val_auc: 0.9868 - val_accuracy: 0.9546 - val_cost: 367.5000\n",
            "Epoch 14/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1327 - auc: 0.9878 - accuracy: 0.9558 - cost: 362.8750 - val_loss: 0.1320 - val_auc: 0.9878 - val_accuracy: 0.9570 - val_cost: 346.1250\n",
            "Epoch 15/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1307 - auc: 0.9881 - accuracy: 0.9564 - cost: 357.2812 - val_loss: 0.1330 - val_auc: 0.9879 - val_accuracy: 0.9581 - val_cost: 329.2500\n",
            "Epoch 16/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1267 - auc: 0.9887 - accuracy: 0.9580 - cost: 344.0312 - val_loss: 0.1275 - val_auc: 0.9884 - val_accuracy: 0.9598 - val_cost: 326.8750\n",
            "Epoch 17/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1250 - auc: 0.9889 - accuracy: 0.9585 - cost: 340.6250 - val_loss: 0.1286 - val_auc: 0.9885 - val_accuracy: 0.9582 - val_cost: 346.3750\n",
            "Epoch 18/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1242 - auc: 0.9890 - accuracy: 0.9597 - cost: 330.8125 - val_loss: 0.1294 - val_auc: 0.9882 - val_accuracy: 0.9584 - val_cost: 329.3750\n",
            "Epoch 19/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1216 - auc: 0.9892 - accuracy: 0.9603 - cost: 325.7188 - val_loss: 0.1241 - val_auc: 0.9888 - val_accuracy: 0.9616 - val_cost: 312.3750\n",
            "Epoch 20/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1181 - auc: 0.9898 - accuracy: 0.9610 - cost: 320.4688 - val_loss: 0.1230 - val_auc: 0.9888 - val_accuracy: 0.9618 - val_cost: 303.8750\n",
            "Epoch 21/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1191 - auc: 0.9896 - accuracy: 0.9622 - cost: 310.5312 - val_loss: 0.1241 - val_auc: 0.9889 - val_accuracy: 0.9617 - val_cost: 306.3750\n",
            "Epoch 22/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1154 - auc: 0.9903 - accuracy: 0.9626 - cost: 308.4062 - val_loss: 0.1234 - val_auc: 0.9889 - val_accuracy: 0.9615 - val_cost: 306.3750\n",
            "Epoch 23/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1137 - auc: 0.9904 - accuracy: 0.9633 - cost: 301.8125 - val_loss: 0.1221 - val_auc: 0.9893 - val_accuracy: 0.9624 - val_cost: 299.8750\n",
            "Epoch 24/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1122 - auc: 0.9906 - accuracy: 0.9642 - cost: 294.0000 - val_loss: 0.1198 - val_auc: 0.9895 - val_accuracy: 0.9631 - val_cost: 301.0000\n",
            "Epoch 25/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1113 - auc: 0.9907 - accuracy: 0.9648 - cost: 290.6562 - val_loss: 0.1190 - val_auc: 0.9893 - val_accuracy: 0.9628 - val_cost: 302.0000\n",
            "Epoch 26/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1106 - auc: 0.9909 - accuracy: 0.9648 - cost: 289.5938 - val_loss: 0.1193 - val_auc: 0.9895 - val_accuracy: 0.9639 - val_cost: 291.2500\n",
            "Epoch 27/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1092 - auc: 0.9909 - accuracy: 0.9648 - cost: 289.7500 - val_loss: 0.1177 - val_auc: 0.9896 - val_accuracy: 0.9641 - val_cost: 296.0000\n",
            "Epoch 28/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1093 - auc: 0.9910 - accuracy: 0.9651 - cost: 288.0000 - val_loss: 0.1170 - val_auc: 0.9899 - val_accuracy: 0.9660 - val_cost: 274.7500\n",
            "Epoch 29/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1073 - auc: 0.9912 - accuracy: 0.9661 - cost: 279.7812 - val_loss: 0.1169 - val_auc: 0.9898 - val_accuracy: 0.9649 - val_cost: 278.2500\n",
            "Epoch 30/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1062 - auc: 0.9913 - accuracy: 0.9666 - cost: 275.6562 - val_loss: 0.1184 - val_auc: 0.9894 - val_accuracy: 0.9645 - val_cost: 294.1250\n",
            "Epoch 31/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1048 - auc: 0.9915 - accuracy: 0.9666 - cost: 275.8438 - val_loss: 0.1190 - val_auc: 0.9892 - val_accuracy: 0.9652 - val_cost: 289.1250\n",
            "Epoch 32/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1041 - auc: 0.9916 - accuracy: 0.9677 - cost: 265.9688 - val_loss: 0.1187 - val_auc: 0.9895 - val_accuracy: 0.9642 - val_cost: 303.8750\n",
            "Epoch 33/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1032 - auc: 0.9917 - accuracy: 0.9676 - cost: 267.6562 - val_loss: 0.1184 - val_auc: 0.9893 - val_accuracy: 0.9648 - val_cost: 278.7500\n",
            "Epoch 34/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1029 - auc: 0.9916 - accuracy: 0.9680 - cost: 263.7812 - val_loss: 0.1203 - val_auc: 0.9893 - val_accuracy: 0.9642 - val_cost: 284.8750\n",
            "Epoch 35/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1016 - auc: 0.9919 - accuracy: 0.9678 - cost: 266.4375 - val_loss: 0.1190 - val_auc: 0.9890 - val_accuracy: 0.9654 - val_cost: 280.3750\n",
            "Epoch 36/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1016 - auc: 0.9919 - accuracy: 0.9682 - cost: 262.5312 - val_loss: 0.1174 - val_auc: 0.9896 - val_accuracy: 0.9655 - val_cost: 279.6250\n",
            "Epoch 37/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1004 - auc: 0.9921 - accuracy: 0.9684 - cost: 261.6562 - val_loss: 0.1141 - val_auc: 0.9899 - val_accuracy: 0.9661 - val_cost: 279.1250\n",
            "Epoch 38/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1002 - auc: 0.9920 - accuracy: 0.9692 - cost: 255.0312 - val_loss: 0.1152 - val_auc: 0.9900 - val_accuracy: 0.9664 - val_cost: 270.2500\n",
            "Epoch 39/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0986 - auc: 0.9922 - accuracy: 0.9696 - cost: 251.5312 - val_loss: 0.1161 - val_auc: 0.9895 - val_accuracy: 0.9666 - val_cost: 272.1250\n",
            "Epoch 40/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0985 - auc: 0.9922 - accuracy: 0.9696 - cost: 251.1250 - val_loss: 0.1175 - val_auc: 0.9897 - val_accuracy: 0.9664 - val_cost: 266.7500\n",
            "Epoch 41/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0989 - auc: 0.9921 - accuracy: 0.9697 - cost: 250.8438 - val_loss: 0.1159 - val_auc: 0.9898 - val_accuracy: 0.9661 - val_cost: 275.6250\n",
            "Epoch 42/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0978 - auc: 0.9923 - accuracy: 0.9694 - cost: 252.6875 - val_loss: 0.1143 - val_auc: 0.9902 - val_accuracy: 0.9659 - val_cost: 272.8750\n",
            "Epoch 43/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0965 - auc: 0.9924 - accuracy: 0.9709 - cost: 240.2812 - val_loss: 0.1170 - val_auc: 0.9898 - val_accuracy: 0.9664 - val_cost: 274.3750\n",
            "Epoch 44/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0957 - auc: 0.9925 - accuracy: 0.9702 - cost: 246.5000 - val_loss: 0.1123 - val_auc: 0.9902 - val_accuracy: 0.9670 - val_cost: 270.7500\n",
            "Epoch 45/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0964 - auc: 0.9925 - accuracy: 0.9702 - cost: 246.2500 - val_loss: 0.1128 - val_auc: 0.9903 - val_accuracy: 0.9673 - val_cost: 263.7500\n",
            "Epoch 46/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0956 - auc: 0.9926 - accuracy: 0.9709 - cost: 239.6250 - val_loss: 0.1139 - val_auc: 0.9901 - val_accuracy: 0.9671 - val_cost: 272.3750\n",
            "Epoch 47/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0950 - auc: 0.9926 - accuracy: 0.9708 - cost: 241.8438 - val_loss: 0.1146 - val_auc: 0.9899 - val_accuracy: 0.9679 - val_cost: 261.2500\n",
            "Epoch 48/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0950 - auc: 0.9926 - accuracy: 0.9713 - cost: 238.0000 - val_loss: 0.1150 - val_auc: 0.9901 - val_accuracy: 0.9672 - val_cost: 261.0000\n",
            "Epoch 49/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0941 - auc: 0.9927 - accuracy: 0.9710 - cost: 239.7188 - val_loss: 0.1132 - val_auc: 0.9901 - val_accuracy: 0.9668 - val_cost: 273.3750\n",
            "Epoch 50/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0945 - auc: 0.9927 - accuracy: 0.9713 - cost: 237.5625 - val_loss: 0.1153 - val_auc: 0.9901 - val_accuracy: 0.9663 - val_cost: 269.5000\n",
            "Epoch 51/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0915 - auc: 0.9930 - accuracy: 0.9725 - cost: 227.5938 - val_loss: 0.1142 - val_auc: 0.9902 - val_accuracy: 0.9675 - val_cost: 261.6250\n",
            "Epoch 52/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0920 - auc: 0.9930 - accuracy: 0.9720 - cost: 231.0312 - val_loss: 0.1151 - val_auc: 0.9897 - val_accuracy: 0.9667 - val_cost: 274.6250\n",
            "Epoch 53/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0934 - auc: 0.9928 - accuracy: 0.9714 - cost: 236.8438 - val_loss: 0.1132 - val_auc: 0.9900 - val_accuracy: 0.9674 - val_cost: 268.8750\n",
            "Epoch 54/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0921 - auc: 0.9930 - accuracy: 0.9718 - cost: 233.2812 - val_loss: 0.1117 - val_auc: 0.9905 - val_accuracy: 0.9672 - val_cost: 271.2500\n",
            "Epoch 55/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0917 - auc: 0.9930 - accuracy: 0.9721 - cost: 231.0312 - val_loss: 0.1132 - val_auc: 0.9903 - val_accuracy: 0.9672 - val_cost: 266.5000\n",
            "Epoch 56/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0909 - auc: 0.9931 - accuracy: 0.9721 - cost: 231.3438 - val_loss: 0.1104 - val_auc: 0.9905 - val_accuracy: 0.9686 - val_cost: 253.2500\n",
            "Epoch 57/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0908 - auc: 0.9932 - accuracy: 0.9725 - cost: 227.5000 - val_loss: 0.1129 - val_auc: 0.9902 - val_accuracy: 0.9683 - val_cost: 256.5000\n",
            "Epoch 58/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0906 - auc: 0.9931 - accuracy: 0.9731 - cost: 222.8438 - val_loss: 0.1115 - val_auc: 0.9902 - val_accuracy: 0.9690 - val_cost: 252.5000\n",
            "Epoch 59/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0896 - auc: 0.9932 - accuracy: 0.9728 - cost: 225.5625 - val_loss: 0.1137 - val_auc: 0.9902 - val_accuracy: 0.9676 - val_cost: 268.8750\n",
            "Epoch 60/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0896 - auc: 0.9932 - accuracy: 0.9726 - cost: 226.7500 - val_loss: 0.1133 - val_auc: 0.9901 - val_accuracy: 0.9677 - val_cost: 261.6250\n",
            "Epoch 61/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0892 - auc: 0.9933 - accuracy: 0.9729 - cost: 224.9062 - val_loss: 0.1171 - val_auc: 0.9900 - val_accuracy: 0.9676 - val_cost: 261.0000\n",
            "Epoch 62/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0898 - auc: 0.9932 - accuracy: 0.9727 - cost: 225.7188 - val_loss: 0.1138 - val_auc: 0.9903 - val_accuracy: 0.9677 - val_cost: 258.6250\n",
            "Epoch 63/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0879 - auc: 0.9935 - accuracy: 0.9737 - cost: 218.3750 - val_loss: 0.1145 - val_auc: 0.9899 - val_accuracy: 0.9675 - val_cost: 263.3750\n",
            "Epoch 64/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0873 - auc: 0.9934 - accuracy: 0.9737 - cost: 217.1250 - val_loss: 0.1131 - val_auc: 0.9903 - val_accuracy: 0.9675 - val_cost: 267.8750\n",
            "Epoch 65/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0884 - auc: 0.9934 - accuracy: 0.9736 - cost: 219.0312 - val_loss: 0.1146 - val_auc: 0.9899 - val_accuracy: 0.9675 - val_cost: 266.6250\n",
            "Epoch 66/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0875 - auc: 0.9933 - accuracy: 0.9735 - cost: 219.5938 - val_loss: 0.1155 - val_auc: 0.9898 - val_accuracy: 0.9673 - val_cost: 266.8750\n",
            "Epoch 67/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0883 - auc: 0.9934 - accuracy: 0.9730 - cost: 223.5625 - val_loss: 0.1130 - val_auc: 0.9905 - val_accuracy: 0.9691 - val_cost: 242.1250\n",
            "Epoch 68/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0871 - auc: 0.9935 - accuracy: 0.9738 - cost: 217.5312 - val_loss: 0.1124 - val_auc: 0.9904 - val_accuracy: 0.9682 - val_cost: 264.2500\n",
            "Epoch 69/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0865 - auc: 0.9937 - accuracy: 0.9737 - cost: 217.6562 - val_loss: 0.1141 - val_auc: 0.9901 - val_accuracy: 0.9677 - val_cost: 262.7500\n",
            "Epoch 70/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0869 - auc: 0.9935 - accuracy: 0.9739 - cost: 216.4062 - val_loss: 0.1141 - val_auc: 0.9899 - val_accuracy: 0.9686 - val_cost: 254.5000\n",
            "Epoch 71/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0870 - auc: 0.9934 - accuracy: 0.9740 - cost: 215.6562 - val_loss: 0.1177 - val_auc: 0.9897 - val_accuracy: 0.9670 - val_cost: 258.6250\n",
            "Epoch 72/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0856 - auc: 0.9937 - accuracy: 0.9743 - cost: 212.4375 - val_loss: 0.1109 - val_auc: 0.9902 - val_accuracy: 0.9695 - val_cost: 248.2500\n",
            "Epoch 73/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0847 - auc: 0.9937 - accuracy: 0.9747 - cost: 209.5938 - val_loss: 0.1161 - val_auc: 0.9900 - val_accuracy: 0.9689 - val_cost: 248.5000\n",
            "Epoch 74/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0869 - auc: 0.9935 - accuracy: 0.9737 - cost: 218.1562 - val_loss: 0.1132 - val_auc: 0.9900 - val_accuracy: 0.9688 - val_cost: 250.1250\n",
            "Epoch 75/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0859 - auc: 0.9938 - accuracy: 0.9740 - cost: 216.2188 - val_loss: 0.1144 - val_auc: 0.9899 - val_accuracy: 0.9685 - val_cost: 255.5000\n",
            "Epoch 76/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0854 - auc: 0.9938 - accuracy: 0.9738 - cost: 217.7188 - val_loss: 0.1137 - val_auc: 0.9900 - val_accuracy: 0.9688 - val_cost: 253.6250\n",
            "Epoch 77/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0848 - auc: 0.9937 - accuracy: 0.9747 - cost: 209.6250 - val_loss: 0.1154 - val_auc: 0.9899 - val_accuracy: 0.9699 - val_cost: 245.0000\n",
            "1000/1000 [==============================] - 1s 961us/step\n",
            "fold train/predict time: 0:06:00.220033\n",
            "x_train shape: (128000, 67)\n",
            "Epoch 1/100\n",
            "1600/1600 [==============================] - 6s 3ms/step - loss: 0.3810 - auc: 0.9059 - accuracy: 0.8291 - cost: 1406.3750 - val_loss: 0.2823 - val_auc: 0.9494 - val_accuracy: 0.8826 - val_cost: 947.8750\n",
            "Epoch 2/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.2630 - auc: 0.9564 - accuracy: 0.8957 - cost: 844.9062 - val_loss: 0.2272 - val_auc: 0.9672 - val_accuracy: 0.9105 - val_cost: 726.5000\n",
            "Epoch 3/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.2208 - auc: 0.9691 - accuracy: 0.9151 - cost: 691.2188 - val_loss: 0.1969 - val_auc: 0.9754 - val_accuracy: 0.9245 - val_cost: 627.0000\n",
            "Epoch 4/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1946 - auc: 0.9758 - accuracy: 0.9292 - cost: 577.7500 - val_loss: 0.1786 - val_auc: 0.9799 - val_accuracy: 0.9334 - val_cost: 558.7500\n",
            "Epoch 5/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1772 - auc: 0.9795 - accuracy: 0.9362 - cost: 520.8125 - val_loss: 0.1683 - val_auc: 0.9817 - val_accuracy: 0.9400 - val_cost: 476.5000\n",
            "Epoch 6/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1670 - auc: 0.9816 - accuracy: 0.9402 - cost: 487.4688 - val_loss: 0.1602 - val_auc: 0.9833 - val_accuracy: 0.9430 - val_cost: 483.3750\n",
            "Epoch 7/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1587 - auc: 0.9832 - accuracy: 0.9443 - cost: 454.7500 - val_loss: 0.1512 - val_auc: 0.9847 - val_accuracy: 0.9475 - val_cost: 415.1250\n",
            "Epoch 8/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1527 - auc: 0.9843 - accuracy: 0.9472 - cost: 431.0312 - val_loss: 0.1444 - val_auc: 0.9856 - val_accuracy: 0.9507 - val_cost: 396.1250\n",
            "Epoch 9/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1485 - auc: 0.9851 - accuracy: 0.9488 - cost: 417.2188 - val_loss: 0.1405 - val_auc: 0.9862 - val_accuracy: 0.9531 - val_cost: 379.2500\n",
            "Epoch 10/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1418 - auc: 0.9862 - accuracy: 0.9519 - cost: 393.9688 - val_loss: 0.1382 - val_auc: 0.9866 - val_accuracy: 0.9534 - val_cost: 386.1250\n",
            "Epoch 11/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1388 - auc: 0.9866 - accuracy: 0.9538 - cost: 377.5000 - val_loss: 0.1347 - val_auc: 0.9870 - val_accuracy: 0.9551 - val_cost: 370.1250\n",
            "Epoch 12/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1335 - auc: 0.9875 - accuracy: 0.9552 - cost: 366.1562 - val_loss: 0.1324 - val_auc: 0.9875 - val_accuracy: 0.9572 - val_cost: 344.5000\n",
            "Epoch 13/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1310 - auc: 0.9878 - accuracy: 0.9567 - cost: 352.4688 - val_loss: 0.1320 - val_auc: 0.9876 - val_accuracy: 0.9571 - val_cost: 341.6250\n",
            "Epoch 14/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1275 - auc: 0.9885 - accuracy: 0.9584 - cost: 340.8125 - val_loss: 0.1325 - val_auc: 0.9875 - val_accuracy: 0.9571 - val_cost: 355.2500\n",
            "Epoch 15/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1253 - auc: 0.9888 - accuracy: 0.9592 - cost: 332.9375 - val_loss: 0.1287 - val_auc: 0.9881 - val_accuracy: 0.9588 - val_cost: 330.5000\n",
            "Epoch 16/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1228 - auc: 0.9890 - accuracy: 0.9600 - cost: 326.9062 - val_loss: 0.1276 - val_auc: 0.9882 - val_accuracy: 0.9590 - val_cost: 336.0000\n",
            "Epoch 17/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1215 - auc: 0.9892 - accuracy: 0.9609 - cost: 320.1562 - val_loss: 0.1257 - val_auc: 0.9884 - val_accuracy: 0.9607 - val_cost: 322.6250\n",
            "Epoch 18/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1198 - auc: 0.9894 - accuracy: 0.9618 - cost: 312.8438 - val_loss: 0.1270 - val_auc: 0.9882 - val_accuracy: 0.9605 - val_cost: 317.2500\n",
            "Epoch 19/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1185 - auc: 0.9896 - accuracy: 0.9626 - cost: 305.7812 - val_loss: 0.1251 - val_auc: 0.9887 - val_accuracy: 0.9619 - val_cost: 304.5000\n",
            "Epoch 20/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1154 - auc: 0.9900 - accuracy: 0.9632 - cost: 300.9375 - val_loss: 0.1220 - val_auc: 0.9889 - val_accuracy: 0.9625 - val_cost: 303.3750\n",
            "Epoch 21/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1149 - auc: 0.9901 - accuracy: 0.9638 - cost: 296.2188 - val_loss: 0.1212 - val_auc: 0.9886 - val_accuracy: 0.9625 - val_cost: 301.2500\n",
            "Epoch 22/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1141 - auc: 0.9902 - accuracy: 0.9638 - cost: 295.6250 - val_loss: 0.1219 - val_auc: 0.9887 - val_accuracy: 0.9622 - val_cost: 309.3750\n",
            "Epoch 23/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1126 - auc: 0.9904 - accuracy: 0.9650 - cost: 287.2500 - val_loss: 0.1200 - val_auc: 0.9890 - val_accuracy: 0.9628 - val_cost: 299.7500\n",
            "Epoch 24/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1105 - auc: 0.9907 - accuracy: 0.9658 - cost: 280.2812 - val_loss: 0.1202 - val_auc: 0.9889 - val_accuracy: 0.9641 - val_cost: 291.3750\n",
            "Epoch 25/100\n",
            "1600/1600 [==============================] - 6s 4ms/step - loss: 0.1103 - auc: 0.9906 - accuracy: 0.9658 - cost: 280.0312 - val_loss: 0.1172 - val_auc: 0.9895 - val_accuracy: 0.9645 - val_cost: 289.7500\n",
            "Epoch 26/100\n",
            "1600/1600 [==============================] - 8s 5ms/step - loss: 0.1086 - auc: 0.9909 - accuracy: 0.9662 - cost: 276.7188 - val_loss: 0.1153 - val_auc: 0.9897 - val_accuracy: 0.9654 - val_cost: 285.1250\n",
            "Epoch 27/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1063 - auc: 0.9911 - accuracy: 0.9667 - cost: 273.2188 - val_loss: 0.1174 - val_auc: 0.9893 - val_accuracy: 0.9654 - val_cost: 286.1250\n",
            "Epoch 28/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1045 - auc: 0.9914 - accuracy: 0.9680 - cost: 262.9375 - val_loss: 0.1201 - val_auc: 0.9888 - val_accuracy: 0.9645 - val_cost: 287.2500\n",
            "Epoch 29/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1048 - auc: 0.9914 - accuracy: 0.9676 - cost: 266.3125 - val_loss: 0.1164 - val_auc: 0.9893 - val_accuracy: 0.9654 - val_cost: 278.6250\n",
            "Epoch 30/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1035 - auc: 0.9916 - accuracy: 0.9679 - cost: 262.6562 - val_loss: 0.1137 - val_auc: 0.9899 - val_accuracy: 0.9664 - val_cost: 268.8750\n",
            "Epoch 31/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1028 - auc: 0.9917 - accuracy: 0.9685 - cost: 257.3438 - val_loss: 0.1156 - val_auc: 0.9893 - val_accuracy: 0.9658 - val_cost: 283.0000\n",
            "Epoch 32/100\n",
            "1600/1600 [==============================] - 6s 4ms/step - loss: 0.1032 - auc: 0.9916 - accuracy: 0.9685 - cost: 258.5625 - val_loss: 0.1154 - val_auc: 0.9894 - val_accuracy: 0.9655 - val_cost: 277.8750\n",
            "Epoch 33/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1013 - auc: 0.9919 - accuracy: 0.9692 - cost: 252.1875 - val_loss: 0.1142 - val_auc: 0.9899 - val_accuracy: 0.9665 - val_cost: 264.2500\n",
            "Epoch 34/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1007 - auc: 0.9919 - accuracy: 0.9692 - cost: 252.7812 - val_loss: 0.1156 - val_auc: 0.9896 - val_accuracy: 0.9655 - val_cost: 278.0000\n",
            "Epoch 35/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1008 - auc: 0.9919 - accuracy: 0.9691 - cost: 254.0938 - val_loss: 0.1162 - val_auc: 0.9896 - val_accuracy: 0.9657 - val_cost: 271.6250\n",
            "Epoch 36/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1000 - auc: 0.9920 - accuracy: 0.9704 - cost: 243.0312 - val_loss: 0.1147 - val_auc: 0.9898 - val_accuracy: 0.9655 - val_cost: 282.2500\n",
            "Epoch 37/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0980 - auc: 0.9923 - accuracy: 0.9699 - cost: 247.5625 - val_loss: 0.1141 - val_auc: 0.9898 - val_accuracy: 0.9666 - val_cost: 268.3750\n",
            "Epoch 38/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0991 - auc: 0.9922 - accuracy: 0.9696 - cost: 249.5938 - val_loss: 0.1140 - val_auc: 0.9896 - val_accuracy: 0.9655 - val_cost: 277.6250\n",
            "Epoch 39/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0970 - auc: 0.9922 - accuracy: 0.9706 - cost: 241.8125 - val_loss: 0.1139 - val_auc: 0.9899 - val_accuracy: 0.9669 - val_cost: 267.2500\n",
            "Epoch 40/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0969 - auc: 0.9923 - accuracy: 0.9710 - cost: 238.3750 - val_loss: 0.1122 - val_auc: 0.9899 - val_accuracy: 0.9668 - val_cost: 269.7500\n",
            "Epoch 41/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0968 - auc: 0.9924 - accuracy: 0.9705 - cost: 242.0625 - val_loss: 0.1138 - val_auc: 0.9895 - val_accuracy: 0.9663 - val_cost: 269.5000\n",
            "Epoch 42/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0976 - auc: 0.9923 - accuracy: 0.9704 - cost: 243.1875 - val_loss: 0.1113 - val_auc: 0.9900 - val_accuracy: 0.9672 - val_cost: 265.3750\n",
            "Epoch 43/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0959 - auc: 0.9925 - accuracy: 0.9711 - cost: 237.1250 - val_loss: 0.1130 - val_auc: 0.9898 - val_accuracy: 0.9664 - val_cost: 267.7500\n",
            "Epoch 44/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0959 - auc: 0.9925 - accuracy: 0.9712 - cost: 236.4688 - val_loss: 0.1149 - val_auc: 0.9897 - val_accuracy: 0.9657 - val_cost: 271.6250\n",
            "Epoch 45/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0959 - auc: 0.9924 - accuracy: 0.9710 - cost: 238.9375 - val_loss: 0.1144 - val_auc: 0.9896 - val_accuracy: 0.9656 - val_cost: 274.7500\n",
            "Epoch 46/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0948 - auc: 0.9924 - accuracy: 0.9717 - cost: 232.8438 - val_loss: 0.1128 - val_auc: 0.9900 - val_accuracy: 0.9669 - val_cost: 268.3750\n",
            "Epoch 47/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0950 - auc: 0.9925 - accuracy: 0.9718 - cost: 231.6250 - val_loss: 0.1122 - val_auc: 0.9904 - val_accuracy: 0.9663 - val_cost: 272.0000\n",
            "Epoch 48/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0942 - auc: 0.9927 - accuracy: 0.9716 - cost: 234.2500 - val_loss: 0.1134 - val_auc: 0.9899 - val_accuracy: 0.9677 - val_cost: 264.2500\n",
            "Epoch 49/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0934 - auc: 0.9929 - accuracy: 0.9711 - cost: 237.6250 - val_loss: 0.1130 - val_auc: 0.9900 - val_accuracy: 0.9671 - val_cost: 261.3750\n",
            "Epoch 50/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0919 - auc: 0.9931 - accuracy: 0.9725 - cost: 226.2500 - val_loss: 0.1146 - val_auc: 0.9899 - val_accuracy: 0.9671 - val_cost: 273.5000\n",
            "Epoch 51/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0925 - auc: 0.9929 - accuracy: 0.9722 - cost: 229.4062 - val_loss: 0.1124 - val_auc: 0.9899 - val_accuracy: 0.9673 - val_cost: 264.8750\n",
            "Epoch 52/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0914 - auc: 0.9930 - accuracy: 0.9720 - cost: 229.8438 - val_loss: 0.1144 - val_auc: 0.9895 - val_accuracy: 0.9669 - val_cost: 274.7500\n",
            "Epoch 53/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0922 - auc: 0.9930 - accuracy: 0.9721 - cost: 229.5938 - val_loss: 0.1140 - val_auc: 0.9898 - val_accuracy: 0.9672 - val_cost: 263.2500\n",
            "Epoch 54/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0906 - auc: 0.9931 - accuracy: 0.9728 - cost: 224.0000 - val_loss: 0.1163 - val_auc: 0.9897 - val_accuracy: 0.9654 - val_cost: 291.1250\n",
            "Epoch 55/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0904 - auc: 0.9931 - accuracy: 0.9727 - cost: 225.1562 - val_loss: 0.1165 - val_auc: 0.9897 - val_accuracy: 0.9668 - val_cost: 264.0000\n",
            "Epoch 56/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0893 - auc: 0.9933 - accuracy: 0.9732 - cost: 220.6562 - val_loss: 0.1125 - val_auc: 0.9899 - val_accuracy: 0.9664 - val_cost: 272.3750\n",
            "Epoch 57/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0918 - auc: 0.9930 - accuracy: 0.9720 - cost: 230.6875 - val_loss: 0.1140 - val_auc: 0.9896 - val_accuracy: 0.9672 - val_cost: 260.6250\n",
            "Epoch 58/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0882 - auc: 0.9934 - accuracy: 0.9734 - cost: 219.5625 - val_loss: 0.1126 - val_auc: 0.9899 - val_accuracy: 0.9686 - val_cost: 262.1250\n",
            "Epoch 59/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0888 - auc: 0.9934 - accuracy: 0.9734 - cost: 219.8125 - val_loss: 0.1152 - val_auc: 0.9899 - val_accuracy: 0.9666 - val_cost: 269.0000\n",
            "Epoch 60/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0890 - auc: 0.9933 - accuracy: 0.9727 - cost: 224.7812 - val_loss: 0.1141 - val_auc: 0.9898 - val_accuracy: 0.9675 - val_cost: 266.7500\n",
            "Epoch 61/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0894 - auc: 0.9932 - accuracy: 0.9735 - cost: 218.9062 - val_loss: 0.1130 - val_auc: 0.9898 - val_accuracy: 0.9672 - val_cost: 274.6250\n",
            "Epoch 62/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0881 - auc: 0.9934 - accuracy: 0.9736 - cost: 217.9688 - val_loss: 0.1123 - val_auc: 0.9899 - val_accuracy: 0.9672 - val_cost: 267.0000\n",
            "Epoch 63/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0881 - auc: 0.9935 - accuracy: 0.9734 - cost: 219.0625 - val_loss: 0.1115 - val_auc: 0.9904 - val_accuracy: 0.9675 - val_cost: 268.6250\n",
            "Epoch 64/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0881 - auc: 0.9933 - accuracy: 0.9736 - cost: 217.8125 - val_loss: 0.1108 - val_auc: 0.9902 - val_accuracy: 0.9677 - val_cost: 264.5000\n",
            "Epoch 65/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0877 - auc: 0.9934 - accuracy: 0.9742 - cost: 213.0000 - val_loss: 0.1128 - val_auc: 0.9900 - val_accuracy: 0.9671 - val_cost: 259.3750\n",
            "Epoch 66/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0872 - auc: 0.9935 - accuracy: 0.9737 - cost: 216.1875 - val_loss: 0.1132 - val_auc: 0.9900 - val_accuracy: 0.9680 - val_cost: 249.3750\n",
            "Epoch 67/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0865 - auc: 0.9938 - accuracy: 0.9736 - cost: 218.0938 - val_loss: 0.1141 - val_auc: 0.9900 - val_accuracy: 0.9676 - val_cost: 256.3750\n",
            "Epoch 68/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0877 - auc: 0.9935 - accuracy: 0.9739 - cost: 215.6250 - val_loss: 0.1130 - val_auc: 0.9902 - val_accuracy: 0.9682 - val_cost: 256.3750\n",
            "Epoch 69/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0861 - auc: 0.9936 - accuracy: 0.9743 - cost: 212.6250 - val_loss: 0.1152 - val_auc: 0.9901 - val_accuracy: 0.9675 - val_cost: 263.7500\n",
            "Epoch 70/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0868 - auc: 0.9935 - accuracy: 0.9744 - cost: 211.7500 - val_loss: 0.1124 - val_auc: 0.9900 - val_accuracy: 0.9680 - val_cost: 259.7500\n",
            "Epoch 71/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0857 - auc: 0.9937 - accuracy: 0.9744 - cost: 209.8750 - val_loss: 0.1140 - val_auc: 0.9899 - val_accuracy: 0.9684 - val_cost: 253.0000\n",
            "Epoch 72/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0852 - auc: 0.9937 - accuracy: 0.9745 - cost: 210.0312 - val_loss: 0.1154 - val_auc: 0.9897 - val_accuracy: 0.9673 - val_cost: 260.3750\n",
            "Epoch 73/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0862 - auc: 0.9936 - accuracy: 0.9749 - cost: 207.6250 - val_loss: 0.1109 - val_auc: 0.9903 - val_accuracy: 0.9683 - val_cost: 255.2500\n",
            "Epoch 74/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0850 - auc: 0.9937 - accuracy: 0.9748 - cost: 208.2500 - val_loss: 0.1129 - val_auc: 0.9899 - val_accuracy: 0.9677 - val_cost: 258.1250\n",
            "Epoch 75/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0852 - auc: 0.9938 - accuracy: 0.9751 - cost: 205.1250 - val_loss: 0.1112 - val_auc: 0.9903 - val_accuracy: 0.9685 - val_cost: 253.0000\n",
            "Epoch 76/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0847 - auc: 0.9937 - accuracy: 0.9746 - cost: 209.0312 - val_loss: 0.1118 - val_auc: 0.9901 - val_accuracy: 0.9689 - val_cost: 249.5000\n",
            "1000/1000 [==============================] - 1s 967us/step\n",
            "fold train/predict time: 0:06:12.088590\n",
            "x_train shape: (128000, 67)\n",
            "Epoch 1/100\n",
            "1600/1600 [==============================] - 6s 3ms/step - loss: 0.3786 - auc: 0.9072 - accuracy: 0.8287 - cost: 1412.4375 - val_loss: 0.2821 - val_auc: 0.9499 - val_accuracy: 0.8854 - val_cost: 913.5000\n",
            "Epoch 2/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.2644 - auc: 0.9560 - accuracy: 0.8940 - cost: 858.1250 - val_loss: 0.2321 - val_auc: 0.9661 - val_accuracy: 0.9098 - val_cost: 739.6250\n",
            "Epoch 3/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.2236 - auc: 0.9684 - accuracy: 0.9138 - cost: 701.4375 - val_loss: 0.1968 - val_auc: 0.9755 - val_accuracy: 0.9260 - val_cost: 586.8750\n",
            "Epoch 4/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1973 - auc: 0.9752 - accuracy: 0.9267 - cost: 596.2188 - val_loss: 0.1772 - val_auc: 0.9801 - val_accuracy: 0.9358 - val_cost: 507.6250\n",
            "Epoch 5/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1801 - auc: 0.9790 - accuracy: 0.9346 - cost: 533.0000 - val_loss: 0.1651 - val_auc: 0.9824 - val_accuracy: 0.9399 - val_cost: 496.5000\n",
            "Epoch 6/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1683 - auc: 0.9816 - accuracy: 0.9401 - cost: 489.9688 - val_loss: 0.1570 - val_auc: 0.9840 - val_accuracy: 0.9452 - val_cost: 429.5000\n",
            "Epoch 7/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1593 - auc: 0.9832 - accuracy: 0.9446 - cost: 452.0938 - val_loss: 0.1463 - val_auc: 0.9857 - val_accuracy: 0.9499 - val_cost: 404.7500\n",
            "Epoch 8/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1532 - auc: 0.9843 - accuracy: 0.9475 - cost: 429.3438 - val_loss: 0.1431 - val_auc: 0.9861 - val_accuracy: 0.9509 - val_cost: 401.2500\n",
            "Epoch 9/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1480 - auc: 0.9854 - accuracy: 0.9494 - cost: 414.6875 - val_loss: 0.1411 - val_auc: 0.9866 - val_accuracy: 0.9525 - val_cost: 399.5000\n",
            "Epoch 10/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1423 - auc: 0.9862 - accuracy: 0.9519 - cost: 394.1562 - val_loss: 0.1382 - val_auc: 0.9869 - val_accuracy: 0.9558 - val_cost: 352.3750\n",
            "Epoch 11/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1378 - auc: 0.9869 - accuracy: 0.9542 - cost: 375.1250 - val_loss: 0.1332 - val_auc: 0.9877 - val_accuracy: 0.9579 - val_cost: 331.6250\n",
            "Epoch 12/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1346 - auc: 0.9876 - accuracy: 0.9551 - cost: 366.9375 - val_loss: 0.1295 - val_auc: 0.9881 - val_accuracy: 0.9594 - val_cost: 322.1250\n",
            "Epoch 13/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1306 - auc: 0.9879 - accuracy: 0.9564 - cost: 357.3125 - val_loss: 0.1295 - val_auc: 0.9881 - val_accuracy: 0.9590 - val_cost: 322.8750\n",
            "Epoch 14/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1271 - auc: 0.9885 - accuracy: 0.9582 - cost: 342.6250 - val_loss: 0.1282 - val_auc: 0.9885 - val_accuracy: 0.9600 - val_cost: 305.8750\n",
            "Epoch 15/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1253 - auc: 0.9886 - accuracy: 0.9592 - cost: 333.3750 - val_loss: 0.1259 - val_auc: 0.9887 - val_accuracy: 0.9607 - val_cost: 312.8750\n",
            "Epoch 16/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1230 - auc: 0.9890 - accuracy: 0.9596 - cost: 329.8125 - val_loss: 0.1258 - val_auc: 0.9887 - val_accuracy: 0.9605 - val_cost: 329.2500\n",
            "Epoch 17/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1205 - auc: 0.9895 - accuracy: 0.9611 - cost: 319.4062 - val_loss: 0.1213 - val_auc: 0.9891 - val_accuracy: 0.9627 - val_cost: 299.2500\n",
            "Epoch 18/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1198 - auc: 0.9893 - accuracy: 0.9612 - cost: 318.7500 - val_loss: 0.1233 - val_auc: 0.9890 - val_accuracy: 0.9629 - val_cost: 293.7500\n",
            "Epoch 19/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1177 - auc: 0.9897 - accuracy: 0.9625 - cost: 308.2500 - val_loss: 0.1197 - val_auc: 0.9892 - val_accuracy: 0.9647 - val_cost: 284.8750\n",
            "Epoch 20/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1162 - auc: 0.9900 - accuracy: 0.9626 - cost: 308.2500 - val_loss: 0.1211 - val_auc: 0.9891 - val_accuracy: 0.9645 - val_cost: 285.0000\n",
            "Epoch 21/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1129 - auc: 0.9904 - accuracy: 0.9642 - cost: 294.9688 - val_loss: 0.1183 - val_auc: 0.9895 - val_accuracy: 0.9642 - val_cost: 290.8750\n",
            "Epoch 22/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1123 - auc: 0.9904 - accuracy: 0.9651 - cost: 287.1875 - val_loss: 0.1188 - val_auc: 0.9895 - val_accuracy: 0.9636 - val_cost: 296.0000\n",
            "Epoch 23/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1109 - auc: 0.9905 - accuracy: 0.9649 - cost: 288.5000 - val_loss: 0.1172 - val_auc: 0.9895 - val_accuracy: 0.9636 - val_cost: 297.5000\n",
            "Epoch 24/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1089 - auc: 0.9908 - accuracy: 0.9659 - cost: 280.6562 - val_loss: 0.1162 - val_auc: 0.9898 - val_accuracy: 0.9658 - val_cost: 277.5000\n",
            "Epoch 25/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1083 - auc: 0.9910 - accuracy: 0.9662 - cost: 277.5312 - val_loss: 0.1145 - val_auc: 0.9901 - val_accuracy: 0.9652 - val_cost: 281.2500\n",
            "Epoch 26/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1077 - auc: 0.9910 - accuracy: 0.9663 - cost: 278.1562 - val_loss: 0.1138 - val_auc: 0.9898 - val_accuracy: 0.9655 - val_cost: 277.6250\n",
            "Epoch 27/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1067 - auc: 0.9912 - accuracy: 0.9669 - cost: 272.5625 - val_loss: 0.1200 - val_auc: 0.9897 - val_accuracy: 0.9637 - val_cost: 285.2500\n",
            "Epoch 28/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1058 - auc: 0.9913 - accuracy: 0.9673 - cost: 269.1875 - val_loss: 0.1157 - val_auc: 0.9898 - val_accuracy: 0.9660 - val_cost: 270.6250\n",
            "Epoch 29/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1045 - auc: 0.9914 - accuracy: 0.9681 - cost: 262.7812 - val_loss: 0.1151 - val_auc: 0.9899 - val_accuracy: 0.9658 - val_cost: 283.6250\n",
            "Epoch 30/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1028 - auc: 0.9916 - accuracy: 0.9679 - cost: 264.4688 - val_loss: 0.1168 - val_auc: 0.9899 - val_accuracy: 0.9641 - val_cost: 302.3750\n",
            "Epoch 31/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1003 - auc: 0.9919 - accuracy: 0.9691 - cost: 254.8750 - val_loss: 0.1136 - val_auc: 0.9900 - val_accuracy: 0.9658 - val_cost: 275.8750\n",
            "Epoch 32/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1016 - auc: 0.9918 - accuracy: 0.9694 - cost: 252.8438 - val_loss: 0.1156 - val_auc: 0.9899 - val_accuracy: 0.9656 - val_cost: 272.7500\n",
            "Epoch 33/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1006 - auc: 0.9918 - accuracy: 0.9691 - cost: 254.6562 - val_loss: 0.1156 - val_auc: 0.9900 - val_accuracy: 0.9660 - val_cost: 270.8750\n",
            "Epoch 34/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0997 - auc: 0.9920 - accuracy: 0.9689 - cost: 256.7500 - val_loss: 0.1144 - val_auc: 0.9900 - val_accuracy: 0.9664 - val_cost: 275.2500\n",
            "Epoch 35/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0996 - auc: 0.9921 - accuracy: 0.9693 - cost: 253.3125 - val_loss: 0.1125 - val_auc: 0.9901 - val_accuracy: 0.9670 - val_cost: 264.7500\n",
            "Epoch 36/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0991 - auc: 0.9921 - accuracy: 0.9694 - cost: 252.5000 - val_loss: 0.1133 - val_auc: 0.9901 - val_accuracy: 0.9664 - val_cost: 271.5000\n",
            "Epoch 37/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0970 - auc: 0.9924 - accuracy: 0.9703 - cost: 246.2812 - val_loss: 0.1114 - val_auc: 0.9903 - val_accuracy: 0.9666 - val_cost: 265.8750\n",
            "Epoch 38/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0972 - auc: 0.9922 - accuracy: 0.9702 - cost: 245.7188 - val_loss: 0.1138 - val_auc: 0.9900 - val_accuracy: 0.9665 - val_cost: 267.5000\n",
            "Epoch 39/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0967 - auc: 0.9923 - accuracy: 0.9703 - cost: 244.3750 - val_loss: 0.1145 - val_auc: 0.9899 - val_accuracy: 0.9661 - val_cost: 277.3750\n",
            "Epoch 40/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0959 - auc: 0.9925 - accuracy: 0.9709 - cost: 239.3750 - val_loss: 0.1147 - val_auc: 0.9897 - val_accuracy: 0.9668 - val_cost: 268.2500\n",
            "Epoch 41/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0970 - auc: 0.9923 - accuracy: 0.9704 - cost: 244.0625 - val_loss: 0.1139 - val_auc: 0.9897 - val_accuracy: 0.9659 - val_cost: 276.7500\n",
            "Epoch 42/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0947 - auc: 0.9926 - accuracy: 0.9711 - cost: 239.1562 - val_loss: 0.1133 - val_auc: 0.9900 - val_accuracy: 0.9667 - val_cost: 264.8750\n",
            "Epoch 43/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0954 - auc: 0.9925 - accuracy: 0.9711 - cost: 239.1250 - val_loss: 0.1156 - val_auc: 0.9900 - val_accuracy: 0.9651 - val_cost: 275.1250\n",
            "Epoch 44/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0955 - auc: 0.9924 - accuracy: 0.9706 - cost: 242.2812 - val_loss: 0.1137 - val_auc: 0.9897 - val_accuracy: 0.9678 - val_cost: 260.5000\n",
            "Epoch 45/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0934 - auc: 0.9927 - accuracy: 0.9714 - cost: 236.3125 - val_loss: 0.1145 - val_auc: 0.9899 - val_accuracy: 0.9675 - val_cost: 264.0000\n",
            "Epoch 46/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0927 - auc: 0.9929 - accuracy: 0.9719 - cost: 232.3125 - val_loss: 0.1128 - val_auc: 0.9898 - val_accuracy: 0.9677 - val_cost: 264.0000\n",
            "Epoch 47/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0924 - auc: 0.9927 - accuracy: 0.9718 - cost: 232.7812 - val_loss: 0.1158 - val_auc: 0.9896 - val_accuracy: 0.9675 - val_cost: 263.6250\n",
            "Epoch 48/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0928 - auc: 0.9929 - accuracy: 0.9723 - cost: 227.9688 - val_loss: 0.1129 - val_auc: 0.9899 - val_accuracy: 0.9678 - val_cost: 261.0000\n",
            "Epoch 49/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0912 - auc: 0.9928 - accuracy: 0.9727 - cost: 226.0312 - val_loss: 0.1143 - val_auc: 0.9898 - val_accuracy: 0.9673 - val_cost: 263.0000\n",
            "Epoch 50/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0908 - auc: 0.9931 - accuracy: 0.9724 - cost: 227.9688 - val_loss: 0.1141 - val_auc: 0.9898 - val_accuracy: 0.9673 - val_cost: 272.8750\n",
            "Epoch 51/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0912 - auc: 0.9930 - accuracy: 0.9726 - cost: 226.6875 - val_loss: 0.1140 - val_auc: 0.9900 - val_accuracy: 0.9686 - val_cost: 250.6250\n",
            "Epoch 52/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0912 - auc: 0.9930 - accuracy: 0.9724 - cost: 228.6250 - val_loss: 0.1164 - val_auc: 0.9893 - val_accuracy: 0.9685 - val_cost: 259.0000\n",
            "Epoch 53/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0901 - auc: 0.9931 - accuracy: 0.9729 - cost: 224.1562 - val_loss: 0.1142 - val_auc: 0.9899 - val_accuracy: 0.9684 - val_cost: 260.5000\n",
            "Epoch 54/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0905 - auc: 0.9931 - accuracy: 0.9728 - cost: 225.4062 - val_loss: 0.1128 - val_auc: 0.9900 - val_accuracy: 0.9684 - val_cost: 252.5000\n",
            "Epoch 55/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0904 - auc: 0.9932 - accuracy: 0.9729 - cost: 224.3750 - val_loss: 0.1133 - val_auc: 0.9900 - val_accuracy: 0.9676 - val_cost: 256.8750\n",
            "Epoch 56/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0897 - auc: 0.9931 - accuracy: 0.9730 - cost: 222.8750 - val_loss: 0.1148 - val_auc: 0.9899 - val_accuracy: 0.9667 - val_cost: 271.3750\n",
            "Epoch 57/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0886 - auc: 0.9933 - accuracy: 0.9733 - cost: 221.2812 - val_loss: 0.1162 - val_auc: 0.9899 - val_accuracy: 0.9679 - val_cost: 264.7500\n",
            "Epoch 58/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0891 - auc: 0.9933 - accuracy: 0.9735 - cost: 219.5312 - val_loss: 0.1133 - val_auc: 0.9901 - val_accuracy: 0.9688 - val_cost: 252.1250\n",
            "Epoch 59/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0889 - auc: 0.9932 - accuracy: 0.9728 - cost: 224.9375 - val_loss: 0.1152 - val_auc: 0.9902 - val_accuracy: 0.9680 - val_cost: 251.5000\n",
            "Epoch 60/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0874 - auc: 0.9935 - accuracy: 0.9737 - cost: 217.9688 - val_loss: 0.1143 - val_auc: 0.9900 - val_accuracy: 0.9682 - val_cost: 248.8750\n",
            "Epoch 61/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0890 - auc: 0.9933 - accuracy: 0.9731 - cost: 222.3438 - val_loss: 0.1116 - val_auc: 0.9903 - val_accuracy: 0.9692 - val_cost: 245.6250\n",
            "Epoch 62/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0878 - auc: 0.9934 - accuracy: 0.9731 - cost: 222.3750 - val_loss: 0.1139 - val_auc: 0.9902 - val_accuracy: 0.9679 - val_cost: 257.1250\n",
            "Epoch 63/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0860 - auc: 0.9935 - accuracy: 0.9739 - cost: 215.9375 - val_loss: 0.1125 - val_auc: 0.9904 - val_accuracy: 0.9698 - val_cost: 244.2500\n",
            "Epoch 64/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0878 - auc: 0.9934 - accuracy: 0.9733 - cost: 220.7812 - val_loss: 0.1177 - val_auc: 0.9897 - val_accuracy: 0.9669 - val_cost: 267.1250\n",
            "Epoch 65/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0866 - auc: 0.9938 - accuracy: 0.9742 - cost: 213.2812 - val_loss: 0.1130 - val_auc: 0.9902 - val_accuracy: 0.9686 - val_cost: 253.6250\n",
            "Epoch 66/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0881 - auc: 0.9933 - accuracy: 0.9734 - cost: 220.6875 - val_loss: 0.1161 - val_auc: 0.9897 - val_accuracy: 0.9670 - val_cost: 277.0000\n",
            "Epoch 67/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0864 - auc: 0.9935 - accuracy: 0.9744 - cost: 213.0000 - val_loss: 0.1143 - val_auc: 0.9897 - val_accuracy: 0.9683 - val_cost: 256.5000\n",
            "Epoch 68/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0860 - auc: 0.9936 - accuracy: 0.9744 - cost: 211.9375 - val_loss: 0.1143 - val_auc: 0.9902 - val_accuracy: 0.9670 - val_cost: 270.1250\n",
            "Epoch 69/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0856 - auc: 0.9937 - accuracy: 0.9745 - cost: 211.8438 - val_loss: 0.1146 - val_auc: 0.9900 - val_accuracy: 0.9681 - val_cost: 254.7500\n",
            "Epoch 70/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0853 - auc: 0.9937 - accuracy: 0.9746 - cost: 210.0000 - val_loss: 0.1153 - val_auc: 0.9900 - val_accuracy: 0.9689 - val_cost: 252.7500\n",
            "Epoch 71/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0849 - auc: 0.9938 - accuracy: 0.9745 - cost: 211.1875 - val_loss: 0.1161 - val_auc: 0.9896 - val_accuracy: 0.9680 - val_cost: 259.2500\n",
            "Epoch 72/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0847 - auc: 0.9937 - accuracy: 0.9748 - cost: 208.5000 - val_loss: 0.1149 - val_auc: 0.9904 - val_accuracy: 0.9675 - val_cost: 256.6250\n",
            "Epoch 73/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0847 - auc: 0.9937 - accuracy: 0.9752 - cost: 205.5312 - val_loss: 0.1145 - val_auc: 0.9901 - val_accuracy: 0.9696 - val_cost: 243.5000\n",
            "Epoch 74/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0833 - auc: 0.9940 - accuracy: 0.9749 - cost: 207.8438 - val_loss: 0.1169 - val_auc: 0.9899 - val_accuracy: 0.9675 - val_cost: 266.7500\n",
            "Epoch 75/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0849 - auc: 0.9937 - accuracy: 0.9747 - cost: 209.3438 - val_loss: 0.1182 - val_auc: 0.9895 - val_accuracy: 0.9670 - val_cost: 262.1250\n",
            "Epoch 76/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0836 - auc: 0.9938 - accuracy: 0.9748 - cost: 208.2500 - val_loss: 0.1142 - val_auc: 0.9902 - val_accuracy: 0.9687 - val_cost: 251.6250\n",
            "Epoch 77/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0839 - auc: 0.9940 - accuracy: 0.9745 - cost: 210.7188 - val_loss: 0.1169 - val_auc: 0.9900 - val_accuracy: 0.9684 - val_cost: 251.0000\n",
            "Epoch 78/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0827 - auc: 0.9940 - accuracy: 0.9756 - cost: 202.6875 - val_loss: 0.1143 - val_auc: 0.9901 - val_accuracy: 0.9682 - val_cost: 256.0000\n",
            "Epoch 79/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0826 - auc: 0.9939 - accuracy: 0.9756 - cost: 202.9688 - val_loss: 0.1135 - val_auc: 0.9901 - val_accuracy: 0.9688 - val_cost: 255.1250\n",
            "Epoch 80/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0821 - auc: 0.9941 - accuracy: 0.9751 - cost: 206.4062 - val_loss: 0.1131 - val_auc: 0.9902 - val_accuracy: 0.9699 - val_cost: 242.1250\n",
            "Epoch 81/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0825 - auc: 0.9940 - accuracy: 0.9751 - cost: 205.5312 - val_loss: 0.1162 - val_auc: 0.9896 - val_accuracy: 0.9687 - val_cost: 254.7500\n",
            "Epoch 82/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0820 - auc: 0.9940 - accuracy: 0.9756 - cost: 202.4062 - val_loss: 0.1187 - val_auc: 0.9901 - val_accuracy: 0.9687 - val_cost: 245.5000\n",
            "Epoch 83/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0824 - auc: 0.9938 - accuracy: 0.9751 - cost: 206.5938 - val_loss: 0.1182 - val_auc: 0.9900 - val_accuracy: 0.9671 - val_cost: 269.7500\n",
            "Epoch 84/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0828 - auc: 0.9938 - accuracy: 0.9756 - cost: 202.6250 - val_loss: 0.1174 - val_auc: 0.9896 - val_accuracy: 0.9675 - val_cost: 266.3750\n",
            "Epoch 85/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0817 - auc: 0.9941 - accuracy: 0.9753 - cost: 204.2188 - val_loss: 0.1165 - val_auc: 0.9901 - val_accuracy: 0.9683 - val_cost: 253.0000\n",
            "Epoch 86/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0813 - auc: 0.9942 - accuracy: 0.9755 - cost: 202.8750 - val_loss: 0.1197 - val_auc: 0.9896 - val_accuracy: 0.9677 - val_cost: 259.1250\n",
            "Epoch 87/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0821 - auc: 0.9941 - accuracy: 0.9755 - cost: 202.7812 - val_loss: 0.1193 - val_auc: 0.9892 - val_accuracy: 0.9674 - val_cost: 268.3750\n",
            "Epoch 88/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0820 - auc: 0.9940 - accuracy: 0.9753 - cost: 204.6875 - val_loss: 0.1157 - val_auc: 0.9901 - val_accuracy: 0.9684 - val_cost: 251.0000\n",
            "Epoch 89/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0814 - auc: 0.9940 - accuracy: 0.9755 - cost: 203.3125 - val_loss: 0.1129 - val_auc: 0.9903 - val_accuracy: 0.9687 - val_cost: 257.7500\n",
            "Epoch 90/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0813 - auc: 0.9942 - accuracy: 0.9754 - cost: 203.7500 - val_loss: 0.1148 - val_auc: 0.9902 - val_accuracy: 0.9689 - val_cost: 252.3750\n",
            "1000/1000 [==============================] - 1s 984us/step\n",
            "fold train/predict time: 0:07:09.278184\n",
            "x_train shape: (128000, 67)\n",
            "Epoch 1/100\n",
            "1600/1600 [==============================] - 6s 3ms/step - loss: 0.3798 - auc: 0.9068 - accuracy: 0.8294 - cost: 1402.6250 - val_loss: 0.2823 - val_auc: 0.9503 - val_accuracy: 0.8815 - val_cost: 990.3750\n",
            "Epoch 2/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.2636 - auc: 0.9560 - accuracy: 0.8943 - cost: 855.1250 - val_loss: 0.2291 - val_auc: 0.9681 - val_accuracy: 0.9100 - val_cost: 696.6250\n",
            "Epoch 3/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.2199 - auc: 0.9693 - accuracy: 0.9158 - cost: 682.3750 - val_loss: 0.1955 - val_auc: 0.9757 - val_accuracy: 0.9276 - val_cost: 579.1250\n",
            "Epoch 4/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1933 - auc: 0.9759 - accuracy: 0.9291 - cost: 574.9062 - val_loss: 0.1729 - val_auc: 0.9805 - val_accuracy: 0.9365 - val_cost: 513.1250\n",
            "Epoch 5/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1741 - auc: 0.9803 - accuracy: 0.9370 - cost: 512.6562 - val_loss: 0.1623 - val_auc: 0.9826 - val_accuracy: 0.9420 - val_cost: 473.5000\n",
            "Epoch 6/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1620 - auc: 0.9827 - accuracy: 0.9422 - cost: 469.9062 - val_loss: 0.1517 - val_auc: 0.9847 - val_accuracy: 0.9473 - val_cost: 432.6250\n",
            "Epoch 7/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1537 - auc: 0.9841 - accuracy: 0.9465 - cost: 435.3438 - val_loss: 0.1454 - val_auc: 0.9859 - val_accuracy: 0.9510 - val_cost: 385.0000\n",
            "Epoch 8/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1448 - auc: 0.9857 - accuracy: 0.9501 - cost: 405.5312 - val_loss: 0.1384 - val_auc: 0.9867 - val_accuracy: 0.9532 - val_cost: 375.8750\n",
            "Epoch 9/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1401 - auc: 0.9865 - accuracy: 0.9519 - cost: 390.7500 - val_loss: 0.1344 - val_auc: 0.9873 - val_accuracy: 0.9553 - val_cost: 368.7500\n",
            "Epoch 10/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1339 - auc: 0.9876 - accuracy: 0.9550 - cost: 366.5625 - val_loss: 0.1331 - val_auc: 0.9876 - val_accuracy: 0.9556 - val_cost: 358.3750\n",
            "Epoch 11/100\n",
            "1600/1600 [==============================] - 6s 4ms/step - loss: 0.1306 - auc: 0.9879 - accuracy: 0.9570 - cost: 350.0312 - val_loss: 0.1311 - val_auc: 0.9879 - val_accuracy: 0.9570 - val_cost: 356.6250\n",
            "Epoch 12/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1281 - auc: 0.9882 - accuracy: 0.9577 - cost: 345.9688 - val_loss: 0.1259 - val_auc: 0.9887 - val_accuracy: 0.9591 - val_cost: 328.0000\n",
            "Epoch 13/100\n",
            "1600/1600 [==============================] - 6s 3ms/step - loss: 0.1247 - auc: 0.9887 - accuracy: 0.9598 - cost: 327.7500 - val_loss: 0.1265 - val_auc: 0.9888 - val_accuracy: 0.9589 - val_cost: 322.5000\n",
            "Epoch 14/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1213 - auc: 0.9892 - accuracy: 0.9603 - cost: 324.8750 - val_loss: 0.1222 - val_auc: 0.9893 - val_accuracy: 0.9606 - val_cost: 322.7500\n",
            "Epoch 15/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1173 - auc: 0.9897 - accuracy: 0.9624 - cost: 306.0312 - val_loss: 0.1203 - val_auc: 0.9892 - val_accuracy: 0.9621 - val_cost: 309.5000\n",
            "Epoch 16/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1154 - auc: 0.9901 - accuracy: 0.9629 - cost: 303.5312 - val_loss: 0.1202 - val_auc: 0.9893 - val_accuracy: 0.9625 - val_cost: 305.6250\n",
            "Epoch 17/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1130 - auc: 0.9904 - accuracy: 0.9637 - cost: 297.0312 - val_loss: 0.1198 - val_auc: 0.9893 - val_accuracy: 0.9628 - val_cost: 295.3750\n",
            "Epoch 18/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1112 - auc: 0.9906 - accuracy: 0.9653 - cost: 283.6875 - val_loss: 0.1192 - val_auc: 0.9895 - val_accuracy: 0.9632 - val_cost: 289.7500\n",
            "Epoch 19/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1110 - auc: 0.9905 - accuracy: 0.9651 - cost: 285.8750 - val_loss: 0.1173 - val_auc: 0.9896 - val_accuracy: 0.9639 - val_cost: 294.6250\n",
            "Epoch 20/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1095 - auc: 0.9908 - accuracy: 0.9654 - cost: 282.5312 - val_loss: 0.1175 - val_auc: 0.9896 - val_accuracy: 0.9647 - val_cost: 280.7500\n",
            "Epoch 21/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1069 - auc: 0.9911 - accuracy: 0.9660 - cost: 276.9688 - val_loss: 0.1200 - val_auc: 0.9896 - val_accuracy: 0.9630 - val_cost: 287.0000\n",
            "Epoch 22/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1061 - auc: 0.9913 - accuracy: 0.9665 - cost: 274.2188 - val_loss: 0.1197 - val_auc: 0.9898 - val_accuracy: 0.9639 - val_cost: 276.7500\n",
            "Epoch 23/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1065 - auc: 0.9912 - accuracy: 0.9673 - cost: 266.6250 - val_loss: 0.1170 - val_auc: 0.9898 - val_accuracy: 0.9640 - val_cost: 298.7500\n",
            "Epoch 24/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1043 - auc: 0.9914 - accuracy: 0.9673 - cost: 268.2500 - val_loss: 0.1198 - val_auc: 0.9894 - val_accuracy: 0.9643 - val_cost: 274.5000\n",
            "Epoch 25/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1032 - auc: 0.9916 - accuracy: 0.9680 - cost: 262.2812 - val_loss: 0.1154 - val_auc: 0.9897 - val_accuracy: 0.9650 - val_cost: 288.8750\n",
            "Epoch 26/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1022 - auc: 0.9917 - accuracy: 0.9681 - cost: 261.7812 - val_loss: 0.1139 - val_auc: 0.9901 - val_accuracy: 0.9655 - val_cost: 279.8750\n",
            "Epoch 27/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1000 - auc: 0.9920 - accuracy: 0.9689 - cost: 255.0938 - val_loss: 0.1153 - val_auc: 0.9900 - val_accuracy: 0.9658 - val_cost: 271.2500\n",
            "Epoch 28/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0999 - auc: 0.9920 - accuracy: 0.9695 - cost: 249.9375 - val_loss: 0.1126 - val_auc: 0.9903 - val_accuracy: 0.9660 - val_cost: 275.0000\n",
            "Epoch 29/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0985 - auc: 0.9922 - accuracy: 0.9690 - cost: 254.2812 - val_loss: 0.1132 - val_auc: 0.9899 - val_accuracy: 0.9666 - val_cost: 266.3750\n",
            "Epoch 30/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0993 - auc: 0.9922 - accuracy: 0.9698 - cost: 247.5938 - val_loss: 0.1145 - val_auc: 0.9899 - val_accuracy: 0.9657 - val_cost: 281.0000\n",
            "Epoch 31/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0978 - auc: 0.9923 - accuracy: 0.9703 - cost: 243.4375 - val_loss: 0.1148 - val_auc: 0.9899 - val_accuracy: 0.9649 - val_cost: 285.5000\n",
            "Epoch 32/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0972 - auc: 0.9923 - accuracy: 0.9704 - cost: 242.7188 - val_loss: 0.1138 - val_auc: 0.9901 - val_accuracy: 0.9666 - val_cost: 269.8750\n",
            "Epoch 33/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0965 - auc: 0.9924 - accuracy: 0.9702 - cost: 243.8750 - val_loss: 0.1132 - val_auc: 0.9899 - val_accuracy: 0.9663 - val_cost: 277.5000\n",
            "Epoch 34/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0955 - auc: 0.9926 - accuracy: 0.9709 - cost: 240.1562 - val_loss: 0.1106 - val_auc: 0.9902 - val_accuracy: 0.9680 - val_cost: 257.5000\n",
            "Epoch 35/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0953 - auc: 0.9925 - accuracy: 0.9713 - cost: 235.8750 - val_loss: 0.1176 - val_auc: 0.9898 - val_accuracy: 0.9646 - val_cost: 299.1250\n",
            "Epoch 36/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0948 - auc: 0.9925 - accuracy: 0.9715 - cost: 234.4688 - val_loss: 0.1128 - val_auc: 0.9901 - val_accuracy: 0.9666 - val_cost: 270.6250\n",
            "Epoch 37/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0948 - auc: 0.9924 - accuracy: 0.9710 - cost: 238.4375 - val_loss: 0.1126 - val_auc: 0.9904 - val_accuracy: 0.9666 - val_cost: 267.2500\n",
            "Epoch 38/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0933 - auc: 0.9928 - accuracy: 0.9719 - cost: 229.6250 - val_loss: 0.1158 - val_auc: 0.9900 - val_accuracy: 0.9664 - val_cost: 259.3750\n",
            "Epoch 39/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0935 - auc: 0.9927 - accuracy: 0.9716 - cost: 233.6875 - val_loss: 0.1133 - val_auc: 0.9900 - val_accuracy: 0.9679 - val_cost: 257.2500\n",
            "Epoch 40/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0940 - auc: 0.9927 - accuracy: 0.9716 - cost: 234.1250 - val_loss: 0.1120 - val_auc: 0.9900 - val_accuracy: 0.9673 - val_cost: 267.1250\n",
            "Epoch 41/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0921 - auc: 0.9929 - accuracy: 0.9724 - cost: 226.8125 - val_loss: 0.1129 - val_auc: 0.9902 - val_accuracy: 0.9676 - val_cost: 263.8750\n",
            "Epoch 42/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0916 - auc: 0.9930 - accuracy: 0.9728 - cost: 224.5625 - val_loss: 0.1121 - val_auc: 0.9900 - val_accuracy: 0.9675 - val_cost: 262.5000\n",
            "Epoch 43/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0932 - auc: 0.9928 - accuracy: 0.9720 - cost: 230.0000 - val_loss: 0.1099 - val_auc: 0.9906 - val_accuracy: 0.9680 - val_cost: 259.8750\n",
            "Epoch 44/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0917 - auc: 0.9930 - accuracy: 0.9727 - cost: 224.3438 - val_loss: 0.1162 - val_auc: 0.9897 - val_accuracy: 0.9672 - val_cost: 265.1250\n",
            "Epoch 45/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0915 - auc: 0.9929 - accuracy: 0.9726 - cost: 225.2188 - val_loss: 0.1134 - val_auc: 0.9898 - val_accuracy: 0.9670 - val_cost: 266.3750\n",
            "Epoch 46/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0900 - auc: 0.9930 - accuracy: 0.9726 - cost: 225.7500 - val_loss: 0.1110 - val_auc: 0.9902 - val_accuracy: 0.9687 - val_cost: 258.5000\n",
            "Epoch 47/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0897 - auc: 0.9932 - accuracy: 0.9729 - cost: 222.1875 - val_loss: 0.1093 - val_auc: 0.9905 - val_accuracy: 0.9681 - val_cost: 263.0000\n",
            "Epoch 48/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0898 - auc: 0.9932 - accuracy: 0.9734 - cost: 219.0312 - val_loss: 0.1125 - val_auc: 0.9900 - val_accuracy: 0.9677 - val_cost: 261.1250\n",
            "Epoch 49/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0882 - auc: 0.9935 - accuracy: 0.9735 - cost: 217.6562 - val_loss: 0.1096 - val_auc: 0.9904 - val_accuracy: 0.9692 - val_cost: 252.0000\n",
            "Epoch 50/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0888 - auc: 0.9933 - accuracy: 0.9731 - cost: 220.8125 - val_loss: 0.1103 - val_auc: 0.9906 - val_accuracy: 0.9701 - val_cost: 239.7500\n",
            "Epoch 51/100\n",
            "1600/1600 [==============================] - 6s 3ms/step - loss: 0.0879 - auc: 0.9935 - accuracy: 0.9737 - cost: 216.5938 - val_loss: 0.1127 - val_auc: 0.9900 - val_accuracy: 0.9688 - val_cost: 247.5000\n",
            "Epoch 52/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0875 - auc: 0.9937 - accuracy: 0.9738 - cost: 215.1875 - val_loss: 0.1155 - val_auc: 0.9897 - val_accuracy: 0.9676 - val_cost: 265.2500\n",
            "Epoch 53/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0873 - auc: 0.9933 - accuracy: 0.9742 - cost: 212.6562 - val_loss: 0.1120 - val_auc: 0.9901 - val_accuracy: 0.9678 - val_cost: 265.0000\n",
            "Epoch 54/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0880 - auc: 0.9933 - accuracy: 0.9740 - cost: 214.7500 - val_loss: 0.1120 - val_auc: 0.9902 - val_accuracy: 0.9682 - val_cost: 260.1250\n",
            "Epoch 55/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0878 - auc: 0.9934 - accuracy: 0.9741 - cost: 213.4375 - val_loss: 0.1138 - val_auc: 0.9900 - val_accuracy: 0.9681 - val_cost: 257.0000\n",
            "Epoch 56/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0862 - auc: 0.9935 - accuracy: 0.9741 - cost: 212.7500 - val_loss: 0.1119 - val_auc: 0.9899 - val_accuracy: 0.9697 - val_cost: 245.3750\n",
            "Epoch 57/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0858 - auc: 0.9936 - accuracy: 0.9745 - cost: 210.1875 - val_loss: 0.1150 - val_auc: 0.9900 - val_accuracy: 0.9678 - val_cost: 257.7500\n",
            "Epoch 58/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0853 - auc: 0.9938 - accuracy: 0.9742 - cost: 211.7500 - val_loss: 0.1125 - val_auc: 0.9902 - val_accuracy: 0.9684 - val_cost: 253.1250\n",
            "Epoch 59/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0856 - auc: 0.9936 - accuracy: 0.9748 - cost: 207.6250 - val_loss: 0.1117 - val_auc: 0.9902 - val_accuracy: 0.9683 - val_cost: 262.1250\n",
            "Epoch 60/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0852 - auc: 0.9937 - accuracy: 0.9751 - cost: 205.5625 - val_loss: 0.1131 - val_auc: 0.9901 - val_accuracy: 0.9689 - val_cost: 250.7500\n",
            "1000/1000 [==============================] - 1s 959us/step\n",
            "fold train/predict time: 0:04:52.545870\n",
            "x_train shape: (128000, 67)\n",
            "Epoch 1/100\n",
            "1600/1600 [==============================] - 6s 3ms/step - loss: 0.3810 - auc: 0.9062 - accuracy: 0.8298 - cost: 1404.8750 - val_loss: 0.2841 - val_auc: 0.9504 - val_accuracy: 0.8835 - val_cost: 912.0000\n",
            "Epoch 2/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.2654 - auc: 0.9556 - accuracy: 0.8936 - cost: 860.7188 - val_loss: 0.2306 - val_auc: 0.9668 - val_accuracy: 0.9096 - val_cost: 738.0000\n",
            "Epoch 3/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.2228 - auc: 0.9685 - accuracy: 0.9148 - cost: 690.7188 - val_loss: 0.1978 - val_auc: 0.9752 - val_accuracy: 0.9247 - val_cost: 608.7500\n",
            "Epoch 4/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1977 - auc: 0.9749 - accuracy: 0.9255 - cost: 604.7500 - val_loss: 0.1790 - val_auc: 0.9794 - val_accuracy: 0.9335 - val_cost: 536.5000\n",
            "Epoch 5/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1797 - auc: 0.9791 - accuracy: 0.9343 - cost: 535.0625 - val_loss: 0.1679 - val_auc: 0.9818 - val_accuracy: 0.9405 - val_cost: 473.6250\n",
            "Epoch 6/100\n",
            "1600/1600 [==============================] - 6s 3ms/step - loss: 0.1664 - auc: 0.9818 - accuracy: 0.9405 - cost: 485.1562 - val_loss: 0.1582 - val_auc: 0.9836 - val_accuracy: 0.9441 - val_cost: 468.6250\n",
            "Epoch 7/100\n",
            "1600/1600 [==============================] - 6s 4ms/step - loss: 0.1572 - auc: 0.9837 - accuracy: 0.9446 - cost: 452.0312 - val_loss: 0.1539 - val_auc: 0.9842 - val_accuracy: 0.9456 - val_cost: 437.5000\n",
            "Epoch 8/100\n",
            "1600/1600 [==============================] - 6s 4ms/step - loss: 0.1500 - auc: 0.9849 - accuracy: 0.9484 - cost: 419.5000 - val_loss: 0.1466 - val_auc: 0.9853 - val_accuracy: 0.9495 - val_cost: 410.6250\n",
            "Epoch 9/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1462 - auc: 0.9856 - accuracy: 0.9500 - cost: 408.8438 - val_loss: 0.1430 - val_auc: 0.9860 - val_accuracy: 0.9511 - val_cost: 399.7500\n",
            "Epoch 10/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1407 - auc: 0.9864 - accuracy: 0.9525 - cost: 388.0625 - val_loss: 0.1423 - val_auc: 0.9861 - val_accuracy: 0.9516 - val_cost: 399.0000\n",
            "Epoch 11/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1368 - auc: 0.9871 - accuracy: 0.9535 - cost: 379.8750 - val_loss: 0.1400 - val_auc: 0.9868 - val_accuracy: 0.9527 - val_cost: 402.6250\n",
            "Epoch 12/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1329 - auc: 0.9877 - accuracy: 0.9558 - cost: 361.3125 - val_loss: 0.1381 - val_auc: 0.9871 - val_accuracy: 0.9550 - val_cost: 348.0000\n",
            "Epoch 13/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1303 - auc: 0.9880 - accuracy: 0.9568 - cost: 352.7188 - val_loss: 0.1347 - val_auc: 0.9874 - val_accuracy: 0.9570 - val_cost: 340.0000\n",
            "Epoch 14/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1284 - auc: 0.9883 - accuracy: 0.9579 - cost: 344.1562 - val_loss: 0.1315 - val_auc: 0.9879 - val_accuracy: 0.9566 - val_cost: 346.2500\n",
            "Epoch 15/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1237 - auc: 0.9890 - accuracy: 0.9593 - cost: 332.5312 - val_loss: 0.1312 - val_auc: 0.9877 - val_accuracy: 0.9582 - val_cost: 340.1250\n",
            "Epoch 16/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1218 - auc: 0.9893 - accuracy: 0.9602 - cost: 324.8125 - val_loss: 0.1318 - val_auc: 0.9877 - val_accuracy: 0.9579 - val_cost: 335.1250\n",
            "Epoch 17/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1208 - auc: 0.9894 - accuracy: 0.9611 - cost: 318.4375 - val_loss: 0.1288 - val_auc: 0.9882 - val_accuracy: 0.9590 - val_cost: 330.6250\n",
            "Epoch 18/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1194 - auc: 0.9897 - accuracy: 0.9614 - cost: 316.1562 - val_loss: 0.1268 - val_auc: 0.9884 - val_accuracy: 0.9607 - val_cost: 313.1250\n",
            "Epoch 19/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1170 - auc: 0.9899 - accuracy: 0.9625 - cost: 307.6250 - val_loss: 0.1268 - val_auc: 0.9883 - val_accuracy: 0.9601 - val_cost: 320.1250\n",
            "Epoch 20/100\n",
            "1600/1600 [==============================] - 6s 4ms/step - loss: 0.1154 - auc: 0.9901 - accuracy: 0.9631 - cost: 302.8125 - val_loss: 0.1250 - val_auc: 0.9886 - val_accuracy: 0.9612 - val_cost: 314.2500\n",
            "Epoch 21/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1128 - auc: 0.9906 - accuracy: 0.9630 - cost: 302.6562 - val_loss: 0.1263 - val_auc: 0.9885 - val_accuracy: 0.9616 - val_cost: 312.2500\n",
            "Epoch 22/100\n",
            "1600/1600 [==============================] - 6s 4ms/step - loss: 0.1122 - auc: 0.9905 - accuracy: 0.9642 - cost: 293.9688 - val_loss: 0.1230 - val_auc: 0.9888 - val_accuracy: 0.9619 - val_cost: 316.2500\n",
            "Epoch 23/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1125 - auc: 0.9905 - accuracy: 0.9643 - cost: 293.2812 - val_loss: 0.1267 - val_auc: 0.9884 - val_accuracy: 0.9618 - val_cost: 319.6250\n",
            "Epoch 24/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1094 - auc: 0.9909 - accuracy: 0.9658 - cost: 280.0000 - val_loss: 0.1259 - val_auc: 0.9885 - val_accuracy: 0.9612 - val_cost: 322.7500\n",
            "Epoch 25/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1091 - auc: 0.9909 - accuracy: 0.9655 - cost: 282.6562 - val_loss: 0.1258 - val_auc: 0.9882 - val_accuracy: 0.9613 - val_cost: 318.1250\n",
            "Epoch 26/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1088 - auc: 0.9911 - accuracy: 0.9659 - cost: 279.2812 - val_loss: 0.1202 - val_auc: 0.9892 - val_accuracy: 0.9633 - val_cost: 299.2500\n",
            "Epoch 27/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1069 - auc: 0.9911 - accuracy: 0.9664 - cost: 275.5312 - val_loss: 0.1211 - val_auc: 0.9891 - val_accuracy: 0.9626 - val_cost: 302.5000\n",
            "Epoch 28/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1057 - auc: 0.9912 - accuracy: 0.9669 - cost: 272.6562 - val_loss: 0.1263 - val_auc: 0.9886 - val_accuracy: 0.9627 - val_cost: 299.2500\n",
            "Epoch 29/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1060 - auc: 0.9914 - accuracy: 0.9665 - cost: 275.4375 - val_loss: 0.1235 - val_auc: 0.9887 - val_accuracy: 0.9622 - val_cost: 300.8750\n",
            "Epoch 30/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1047 - auc: 0.9914 - accuracy: 0.9670 - cost: 272.2812 - val_loss: 0.1248 - val_auc: 0.9887 - val_accuracy: 0.9627 - val_cost: 308.1250\n",
            "Epoch 31/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1040 - auc: 0.9915 - accuracy: 0.9681 - cost: 261.8750 - val_loss: 0.1231 - val_auc: 0.9892 - val_accuracy: 0.9639 - val_cost: 286.8750\n",
            "Epoch 32/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1028 - auc: 0.9917 - accuracy: 0.9681 - cost: 262.1562 - val_loss: 0.1195 - val_auc: 0.9895 - val_accuracy: 0.9647 - val_cost: 293.2500\n",
            "Epoch 33/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1024 - auc: 0.9917 - accuracy: 0.9683 - cost: 259.5625 - val_loss: 0.1217 - val_auc: 0.9892 - val_accuracy: 0.9635 - val_cost: 290.0000\n",
            "Epoch 34/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1012 - auc: 0.9919 - accuracy: 0.9681 - cost: 261.5625 - val_loss: 0.1207 - val_auc: 0.9892 - val_accuracy: 0.9651 - val_cost: 288.7500\n",
            "Epoch 35/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0995 - auc: 0.9922 - accuracy: 0.9688 - cost: 257.1250 - val_loss: 0.1245 - val_auc: 0.9891 - val_accuracy: 0.9639 - val_cost: 285.5000\n",
            "Epoch 36/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1003 - auc: 0.9920 - accuracy: 0.9689 - cost: 255.4375 - val_loss: 0.1244 - val_auc: 0.9892 - val_accuracy: 0.9637 - val_cost: 299.5000\n",
            "Epoch 37/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0989 - auc: 0.9922 - accuracy: 0.9690 - cost: 255.7188 - val_loss: 0.1243 - val_auc: 0.9888 - val_accuracy: 0.9634 - val_cost: 293.5000\n",
            "Epoch 38/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0981 - auc: 0.9923 - accuracy: 0.9696 - cost: 250.1562 - val_loss: 0.1206 - val_auc: 0.9892 - val_accuracy: 0.9639 - val_cost: 295.2500\n",
            "Epoch 39/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0973 - auc: 0.9924 - accuracy: 0.9695 - cost: 250.8438 - val_loss: 0.1252 - val_auc: 0.9887 - val_accuracy: 0.9630 - val_cost: 308.5000\n",
            "Epoch 40/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0969 - auc: 0.9925 - accuracy: 0.9703 - cost: 244.1250 - val_loss: 0.1244 - val_auc: 0.9892 - val_accuracy: 0.9639 - val_cost: 278.0000\n",
            "Epoch 41/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0971 - auc: 0.9924 - accuracy: 0.9701 - cost: 246.0938 - val_loss: 0.1227 - val_auc: 0.9891 - val_accuracy: 0.9639 - val_cost: 291.1250\n",
            "Epoch 42/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0972 - auc: 0.9924 - accuracy: 0.9704 - cost: 243.5000 - val_loss: 0.1203 - val_auc: 0.9896 - val_accuracy: 0.9657 - val_cost: 278.5000\n",
            "Epoch 43/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0953 - auc: 0.9926 - accuracy: 0.9700 - cost: 245.9062 - val_loss: 0.1216 - val_auc: 0.9894 - val_accuracy: 0.9654 - val_cost: 289.8750\n",
            "Epoch 44/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0948 - auc: 0.9927 - accuracy: 0.9704 - cost: 243.5938 - val_loss: 0.1234 - val_auc: 0.9891 - val_accuracy: 0.9653 - val_cost: 286.0000\n",
            "Epoch 45/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0949 - auc: 0.9926 - accuracy: 0.9713 - cost: 236.5312 - val_loss: 0.1202 - val_auc: 0.9894 - val_accuracy: 0.9652 - val_cost: 280.8750\n",
            "Epoch 46/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0945 - auc: 0.9927 - accuracy: 0.9714 - cost: 235.6562 - val_loss: 0.1189 - val_auc: 0.9894 - val_accuracy: 0.9659 - val_cost: 273.5000\n",
            "Epoch 47/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0936 - auc: 0.9929 - accuracy: 0.9709 - cost: 239.6250 - val_loss: 0.1208 - val_auc: 0.9892 - val_accuracy: 0.9645 - val_cost: 288.2500\n",
            "Epoch 48/100\n",
            "1600/1600 [==============================] - 7s 4ms/step - loss: 0.0941 - auc: 0.9927 - accuracy: 0.9708 - cost: 239.9688 - val_loss: 0.1200 - val_auc: 0.9894 - val_accuracy: 0.9652 - val_cost: 282.1250\n",
            "Epoch 49/100\n",
            "1600/1600 [==============================] - 6s 3ms/step - loss: 0.0929 - auc: 0.9929 - accuracy: 0.9717 - cost: 233.1562 - val_loss: 0.1239 - val_auc: 0.9892 - val_accuracy: 0.9649 - val_cost: 285.6250\n",
            "Epoch 50/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0928 - auc: 0.9929 - accuracy: 0.9716 - cost: 233.4062 - val_loss: 0.1209 - val_auc: 0.9896 - val_accuracy: 0.9656 - val_cost: 271.6250\n",
            "Epoch 51/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0922 - auc: 0.9930 - accuracy: 0.9719 - cost: 231.4688 - val_loss: 0.1217 - val_auc: 0.9893 - val_accuracy: 0.9653 - val_cost: 285.6250\n",
            "Epoch 52/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0913 - auc: 0.9931 - accuracy: 0.9725 - cost: 226.5000 - val_loss: 0.1192 - val_auc: 0.9894 - val_accuracy: 0.9660 - val_cost: 277.5000\n",
            "Epoch 53/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0912 - auc: 0.9930 - accuracy: 0.9727 - cost: 223.5625 - val_loss: 0.1211 - val_auc: 0.9895 - val_accuracy: 0.9646 - val_cost: 281.1250\n",
            "Epoch 54/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0925 - auc: 0.9930 - accuracy: 0.9720 - cost: 230.5312 - val_loss: 0.1177 - val_auc: 0.9895 - val_accuracy: 0.9662 - val_cost: 280.5000\n",
            "Epoch 55/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0899 - auc: 0.9932 - accuracy: 0.9726 - cost: 225.9688 - val_loss: 0.1174 - val_auc: 0.9895 - val_accuracy: 0.9664 - val_cost: 273.7500\n",
            "Epoch 56/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0917 - auc: 0.9929 - accuracy: 0.9719 - cost: 231.2812 - val_loss: 0.1186 - val_auc: 0.9899 - val_accuracy: 0.9676 - val_cost: 266.1250\n",
            "Epoch 57/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0897 - auc: 0.9932 - accuracy: 0.9728 - cost: 223.4688 - val_loss: 0.1170 - val_auc: 0.9897 - val_accuracy: 0.9671 - val_cost: 270.6250\n",
            "Epoch 58/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0879 - auc: 0.9934 - accuracy: 0.9738 - cost: 215.4062 - val_loss: 0.1187 - val_auc: 0.9898 - val_accuracy: 0.9667 - val_cost: 269.1250\n",
            "Epoch 59/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0886 - auc: 0.9934 - accuracy: 0.9731 - cost: 221.0625 - val_loss: 0.1190 - val_auc: 0.9892 - val_accuracy: 0.9664 - val_cost: 273.5000\n",
            "Epoch 60/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0889 - auc: 0.9934 - accuracy: 0.9726 - cost: 226.1250 - val_loss: 0.1202 - val_auc: 0.9896 - val_accuracy: 0.9653 - val_cost: 276.8750\n",
            "Epoch 61/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0885 - auc: 0.9934 - accuracy: 0.9728 - cost: 224.7500 - val_loss: 0.1194 - val_auc: 0.9895 - val_accuracy: 0.9663 - val_cost: 267.5000\n",
            "Epoch 62/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0877 - auc: 0.9933 - accuracy: 0.9734 - cost: 218.5625 - val_loss: 0.1208 - val_auc: 0.9892 - val_accuracy: 0.9664 - val_cost: 272.0000\n",
            "Epoch 63/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0879 - auc: 0.9934 - accuracy: 0.9736 - cost: 216.9375 - val_loss: 0.1197 - val_auc: 0.9899 - val_accuracy: 0.9652 - val_cost: 281.2500\n",
            "Epoch 64/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0872 - auc: 0.9935 - accuracy: 0.9734 - cost: 218.5000 - val_loss: 0.1208 - val_auc: 0.9893 - val_accuracy: 0.9660 - val_cost: 276.0000\n",
            "Epoch 65/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0868 - auc: 0.9936 - accuracy: 0.9732 - cost: 220.5625 - val_loss: 0.1210 - val_auc: 0.9894 - val_accuracy: 0.9654 - val_cost: 274.2500\n",
            "Epoch 66/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0883 - auc: 0.9934 - accuracy: 0.9731 - cost: 222.1875 - val_loss: 0.1205 - val_auc: 0.9893 - val_accuracy: 0.9672 - val_cost: 265.1250\n",
            "Epoch 67/100\n",
            "1600/1600 [==============================] - 6s 4ms/step - loss: 0.0866 - auc: 0.9937 - accuracy: 0.9732 - cost: 220.0938 - val_loss: 0.1204 - val_auc: 0.9896 - val_accuracy: 0.9654 - val_cost: 283.0000\n",
            "Epoch 68/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0867 - auc: 0.9936 - accuracy: 0.9741 - cost: 213.1562 - val_loss: 0.1229 - val_auc: 0.9893 - val_accuracy: 0.9668 - val_cost: 274.2500\n",
            "Epoch 69/100\n",
            "1600/1600 [==============================] - 6s 3ms/step - loss: 0.0861 - auc: 0.9938 - accuracy: 0.9732 - cost: 221.1250 - val_loss: 0.1243 - val_auc: 0.9892 - val_accuracy: 0.9668 - val_cost: 269.1250\n",
            "Epoch 70/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0864 - auc: 0.9936 - accuracy: 0.9739 - cost: 214.7812 - val_loss: 0.1197 - val_auc: 0.9897 - val_accuracy: 0.9666 - val_cost: 263.2500\n",
            "Epoch 71/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0859 - auc: 0.9938 - accuracy: 0.9744 - cost: 212.0000 - val_loss: 0.1239 - val_auc: 0.9891 - val_accuracy: 0.9662 - val_cost: 272.2500\n",
            "Epoch 72/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0847 - auc: 0.9938 - accuracy: 0.9742 - cost: 212.8750 - val_loss: 0.1243 - val_auc: 0.9892 - val_accuracy: 0.9655 - val_cost: 271.7500\n",
            "Epoch 73/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0855 - auc: 0.9936 - accuracy: 0.9741 - cost: 213.9062 - val_loss: 0.1208 - val_auc: 0.9892 - val_accuracy: 0.9663 - val_cost: 269.8750\n",
            "Epoch 74/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0849 - auc: 0.9937 - accuracy: 0.9746 - cost: 209.0000 - val_loss: 0.1211 - val_auc: 0.9895 - val_accuracy: 0.9669 - val_cost: 266.1250\n",
            "Epoch 75/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0856 - auc: 0.9936 - accuracy: 0.9736 - cost: 217.2812 - val_loss: 0.1188 - val_auc: 0.9898 - val_accuracy: 0.9671 - val_cost: 265.6250\n",
            "Epoch 76/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0843 - auc: 0.9938 - accuracy: 0.9752 - cost: 204.6875 - val_loss: 0.1207 - val_auc: 0.9895 - val_accuracy: 0.9667 - val_cost: 272.0000\n",
            "Epoch 77/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0845 - auc: 0.9939 - accuracy: 0.9743 - cost: 212.3750 - val_loss: 0.1236 - val_auc: 0.9895 - val_accuracy: 0.9661 - val_cost: 266.7500\n",
            "Epoch 78/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0835 - auc: 0.9940 - accuracy: 0.9745 - cost: 209.4688 - val_loss: 0.1244 - val_auc: 0.9894 - val_accuracy: 0.9663 - val_cost: 276.2500\n",
            "Epoch 79/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0838 - auc: 0.9939 - accuracy: 0.9752 - cost: 204.5625 - val_loss: 0.1206 - val_auc: 0.9895 - val_accuracy: 0.9671 - val_cost: 265.5000\n",
            "Epoch 80/100\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0833 - auc: 0.9940 - accuracy: 0.9747 - cost: 209.2500 - val_loss: 0.1228 - val_auc: 0.9893 - val_accuracy: 0.9668 - val_cost: 269.0000\n",
            "1000/1000 [==============================] - 1s 983us/step\n",
            "fold train/predict time: 0:06:40.290182\n",
            "total train/predict time: 0:30:54.790014\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m4_results = fold_results"
      ],
      "metadata": {
        "id": "K3k5U4nVjizo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds_m4 = np.zeros(len(y))\n",
        "for i in fold_results.keys():\n",
        "  for j in range(len(fold_results.get(i).get('predictions'))):\n",
        "    idx = fold_results.get(i).get('index')[j]\n",
        "    preds_m4[idx] = np.round(fold_results.get(i).get('predictions')[j],0)\n",
        "m4_cost = cost_func(y,preds_m4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMZkaNGNeKT0",
        "outputId": "73214def-47ce-4f13-a23f-b84be2b35705"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "628150"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds_new = np.zeros(len(y))\n",
        "for i in fold_results.keys():\n",
        "  for j in range(len(fold_results.get(i).get('predictions'))):\n",
        "    idx = fold_results.get(i).get('index')[j]\n",
        "    if fold_results.get(i).get('predictions')[j] > 0.35:\n",
        "      preds_new[idx] = 1\n",
        "    else:\n",
        "      preds_new[idx] = 0\n",
        "m4_cost_t = cost_func(y,preds_new)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRXVWMi0VKc3",
        "outputId": "124f8710-2e33-4694-a620-559391a12312"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "616700"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "es = EarlyStopping(monitor='val_cost', mode='min',patience=10,restore_best_weights=True,start_from_epoch=10)\n",
        "fold_results = {}\n",
        "t_tot = now()\n",
        "for i, (train_index, test_index) in enumerate(skf.split(X,y)):\n",
        "  # building model within loop to reset weights each time\n",
        "  tf.keras.backend.clear_session()\n",
        "  tf.keras.utils.set_random_seed(807)\n",
        "  model5 = tf.keras.Sequential()\n",
        "  model5.add(tf.keras.Input(shape=(X.shape[1],)))\n",
        "  model5.add(tf.keras.layers.Dense(64, activation='relu',name=\"Relu1\"))\n",
        "  model5.add(tf.keras.layers.Dense(64, activation='relu', name=\"Relu2\"))\n",
        "  model5.add(tf.keras.layers.Dropout(0.4))\n",
        "  model5.add(tf.keras.layers.Dense(1, activation='sigmoid', name=\"Activation\"))\n",
        "  model5.compile(optimizer='adam',\n",
        "                loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "                metrics = [tf.keras.metrics.AUC(from_logits=False), 'accuracy',cost])\n",
        "  t_fold = now()\n",
        "  print('x_train shape:', X[train_index].shape)\n",
        "  model5.fit(X[train_index],y[train_index],epochs=10,batch_size=64,validation_split=0.2,callbacks=[es])\n",
        "  fold_results.update({i:{'predictions':model5.predict(X[test_index]).flatten(),'index':test_index,'y_true':y[test_index]}})\n",
        "  print('fold train/predict time: %s' % (now()-t_fold))\n",
        "print('total train/predict time: %s' % (now()-t_tot))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9khDvGk8hp0_",
        "outputId": "549993fd-d9cc-4ec2-cacb-e9bd358dc6a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (128000, 67)\n",
            "Epoch 1/10\n",
            "1600/1600 [==============================] - 6s 3ms/step - loss: 0.3797 - auc: 0.9067 - accuracy: 0.8289 - cost_metric: 250.0000 - val_loss: 0.2840 - val_auc: 0.9493 - val_accuracy: 0.8816 - val_cost_metric: 250.0000\n",
            "Epoch 2/10\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.2641 - auc: 0.9560 - accuracy: 0.8947 - cost_metric: 250.0000 - val_loss: 0.2302 - val_auc: 0.9666 - val_accuracy: 0.9101 - val_cost_metric: 250.0000\n",
            "Epoch 3/10\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.2246 - auc: 0.9681 - accuracy: 0.9130 - cost_metric: 250.0000 - val_loss: 0.1983 - val_auc: 0.9750 - val_accuracy: 0.9253 - val_cost_metric: 250.0000\n",
            "Epoch 4/10\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1993 - auc: 0.9747 - accuracy: 0.9265 - cost_metric: 250.0000 - val_loss: 0.1784 - val_auc: 0.9797 - val_accuracy: 0.9343 - val_cost_metric: 250.0000\n",
            "Epoch 5/10\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1824 - auc: 0.9785 - accuracy: 0.9347 - cost_metric: 250.0000 - val_loss: 0.1687 - val_auc: 0.9813 - val_accuracy: 0.9394 - val_cost_metric: 250.0000\n",
            "Epoch 6/10\n",
            "1600/1600 [==============================] - 4s 3ms/step - loss: 0.1709 - auc: 0.9810 - accuracy: 0.9390 - cost_metric: 250.0000 - val_loss: 0.1585 - val_auc: 0.9833 - val_accuracy: 0.9457 - val_cost_metric: 250.0000\n",
            "Epoch 7/10\n",
            "1600/1600 [==============================] - 4s 3ms/step - loss: 0.1631 - auc: 0.9825 - accuracy: 0.9423 - cost_metric: 250.0000 - val_loss: 0.1559 - val_auc: 0.9840 - val_accuracy: 0.9440 - val_cost_metric: 250.0000\n",
            "Epoch 8/10\n",
            "1600/1600 [==============================] - 4s 3ms/step - loss: 0.1573 - auc: 0.9836 - accuracy: 0.9447 - cost_metric: 250.0000 - val_loss: 0.1503 - val_auc: 0.9849 - val_accuracy: 0.9476 - val_cost_metric: 250.0000\n",
            "Epoch 9/10\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1514 - auc: 0.9847 - accuracy: 0.9482 - cost_metric: 250.0000 - val_loss: 0.1469 - val_auc: 0.9854 - val_accuracy: 0.9485 - val_cost_metric: 250.0000\n",
            "Epoch 10/10\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1464 - auc: 0.9856 - accuracy: 0.9503 - cost_metric: 250.0000 - val_loss: 0.1446 - val_auc: 0.9858 - val_accuracy: 0.9514 - val_cost_metric: 250.0000\n",
            "1000/1000 [==============================] - 1s 1ms/step\n",
            "fold train/predict time: 0:00:48.288325\n",
            "x_train shape: (128000, 67)\n",
            "Epoch 1/10\n",
            "1600/1600 [==============================] - 6s 3ms/step - loss: 0.3807 - auc: 0.9055 - accuracy: 0.8283 - cost_metric: 250.0000 - val_loss: 0.2731 - val_auc: 0.9529 - val_accuracy: 0.8871 - val_cost_metric: 250.0000\n",
            "Epoch 2/10\n",
            "1600/1600 [==============================] - 4s 3ms/step - loss: 0.2565 - auc: 0.9586 - accuracy: 0.8984 - cost_metric: 250.0000 - val_loss: 0.2187 - val_auc: 0.9699 - val_accuracy: 0.9149 - val_cost_metric: 250.0000\n",
            "Epoch 3/10\n",
            "1600/1600 [==============================] - 4s 3ms/step - loss: 0.2144 - auc: 0.9709 - accuracy: 0.9192 - cost_metric: 250.0000 - val_loss: 0.1925 - val_auc: 0.9767 - val_accuracy: 0.9288 - val_cost_metric: 250.0000\n",
            "Epoch 4/10\n",
            "1600/1600 [==============================] - 4s 3ms/step - loss: 0.1894 - auc: 0.9770 - accuracy: 0.9305 - cost_metric: 250.0000 - val_loss: 0.1805 - val_auc: 0.9791 - val_accuracy: 0.9345 - val_cost_metric: 250.0000\n",
            "Epoch 5/10\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1749 - auc: 0.9801 - accuracy: 0.9375 - cost_metric: 250.0000 - val_loss: 0.1679 - val_auc: 0.9821 - val_accuracy: 0.9398 - val_cost_metric: 250.0000\n",
            "Epoch 6/10\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1642 - auc: 0.9823 - accuracy: 0.9422 - cost_metric: 250.0000 - val_loss: 0.1618 - val_auc: 0.9828 - val_accuracy: 0.9437 - val_cost_metric: 250.0000\n",
            "Epoch 7/10\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1555 - auc: 0.9839 - accuracy: 0.9465 - cost_metric: 250.0000 - val_loss: 0.1481 - val_auc: 0.9849 - val_accuracy: 0.9507 - val_cost_metric: 250.0000\n",
            "Epoch 8/10\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1488 - auc: 0.9851 - accuracy: 0.9500 - cost_metric: 250.0000 - val_loss: 0.1403 - val_auc: 0.9862 - val_accuracy: 0.9538 - val_cost_metric: 250.0000\n",
            "Epoch 9/10\n",
            "1600/1600 [==============================] - 4s 3ms/step - loss: 0.1419 - auc: 0.9863 - accuracy: 0.9523 - cost_metric: 250.0000 - val_loss: 0.1351 - val_auc: 0.9870 - val_accuracy: 0.9557 - val_cost_metric: 250.0000\n",
            "Epoch 10/10\n",
            "1600/1600 [==============================] - 4s 3ms/step - loss: 0.1358 - auc: 0.9871 - accuracy: 0.9559 - cost_metric: 250.0000 - val_loss: 0.1340 - val_auc: 0.9873 - val_accuracy: 0.9554 - val_cost_metric: 250.0000\n",
            "1000/1000 [==============================] - 1s 1ms/step\n",
            "fold train/predict time: 0:00:48.408293\n",
            "x_train shape: (128000, 67)\n",
            "Epoch 1/10\n",
            "1600/1600 [==============================] - 6s 3ms/step - loss: 0.3787 - auc: 0.9070 - accuracy: 0.8302 - cost_metric: 250.0000 - val_loss: 0.2714 - val_auc: 0.9538 - val_accuracy: 0.8896 - val_cost_metric: 250.0000\n",
            "Epoch 2/10\n",
            "1600/1600 [==============================] - 4s 3ms/step - loss: 0.2540 - auc: 0.9592 - accuracy: 0.8991 - cost_metric: 250.0000 - val_loss: 0.2161 - val_auc: 0.9707 - val_accuracy: 0.9154 - val_cost_metric: 250.0000\n",
            "Epoch 3/10\n",
            "1600/1600 [==============================] - 4s 3ms/step - loss: 0.2132 - auc: 0.9712 - accuracy: 0.9196 - cost_metric: 250.0000 - val_loss: 0.1870 - val_auc: 0.9776 - val_accuracy: 0.9298 - val_cost_metric: 250.0000\n",
            "Epoch 4/10\n",
            "1600/1600 [==============================] - 4s 3ms/step - loss: 0.1893 - auc: 0.9769 - accuracy: 0.9310 - cost_metric: 250.0000 - val_loss: 0.1692 - val_auc: 0.9815 - val_accuracy: 0.9380 - val_cost_metric: 250.0000\n",
            "Epoch 5/10\n",
            "1600/1600 [==============================] - 4s 3ms/step - loss: 0.1737 - auc: 0.9804 - accuracy: 0.9372 - cost_metric: 250.0000 - val_loss: 0.1608 - val_auc: 0.9830 - val_accuracy: 0.9421 - val_cost_metric: 250.0000\n",
            "Epoch 6/10\n",
            "1600/1600 [==============================] - 4s 3ms/step - loss: 0.1638 - auc: 0.9822 - accuracy: 0.9423 - cost_metric: 250.0000 - val_loss: 0.1519 - val_auc: 0.9847 - val_accuracy: 0.9465 - val_cost_metric: 250.0000\n",
            "Epoch 7/10\n",
            "1600/1600 [==============================] - 4s 3ms/step - loss: 0.1563 - auc: 0.9836 - accuracy: 0.9463 - cost_metric: 250.0000 - val_loss: 0.1455 - val_auc: 0.9858 - val_accuracy: 0.9503 - val_cost_metric: 250.0000\n",
            "Epoch 8/10\n",
            "1600/1600 [==============================] - 4s 3ms/step - loss: 0.1498 - auc: 0.9848 - accuracy: 0.9493 - cost_metric: 250.0000 - val_loss: 0.1410 - val_auc: 0.9865 - val_accuracy: 0.9505 - val_cost_metric: 250.0000\n",
            "Epoch 9/10\n",
            "1600/1600 [==============================] - 4s 3ms/step - loss: 0.1434 - auc: 0.9859 - accuracy: 0.9518 - cost_metric: 250.0000 - val_loss: 0.1412 - val_auc: 0.9867 - val_accuracy: 0.9515 - val_cost_metric: 250.0000\n",
            "Epoch 10/10\n",
            "1600/1600 [==============================] - 4s 3ms/step - loss: 0.1379 - auc: 0.9867 - accuracy: 0.9541 - cost_metric: 250.0000 - val_loss: 0.1320 - val_auc: 0.9876 - val_accuracy: 0.9562 - val_cost_metric: 250.0000\n",
            "1000/1000 [==============================] - 1s 870us/step\n",
            "fold train/predict time: 0:00:45.081453\n",
            "x_train shape: (128000, 67)\n",
            "Epoch 1/10\n",
            "1600/1600 [==============================] - 6s 3ms/step - loss: 0.3834 - auc: 0.9047 - accuracy: 0.8268 - cost_metric: 250.0000 - val_loss: 0.2765 - val_auc: 0.9519 - val_accuracy: 0.8855 - val_cost_metric: 250.0000\n",
            "Epoch 2/10\n",
            "1600/1600 [==============================] - 4s 2ms/step - loss: 0.2592 - auc: 0.9575 - accuracy: 0.8965 - cost_metric: 250.0000 - val_loss: 0.2204 - val_auc: 0.9698 - val_accuracy: 0.9144 - val_cost_metric: 250.0000\n",
            "Epoch 3/10\n",
            "1600/1600 [==============================] - 4s 3ms/step - loss: 0.2144 - auc: 0.9709 - accuracy: 0.9187 - cost_metric: 250.0000 - val_loss: 0.1913 - val_auc: 0.9765 - val_accuracy: 0.9289 - val_cost_metric: 250.0000\n",
            "Epoch 4/10\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1916 - auc: 0.9765 - accuracy: 0.9297 - cost_metric: 250.0000 - val_loss: 0.1740 - val_auc: 0.9807 - val_accuracy: 0.9376 - val_cost_metric: 250.0000\n",
            "Epoch 5/10\n",
            "1600/1600 [==============================] - 4s 3ms/step - loss: 0.1778 - auc: 0.9796 - accuracy: 0.9364 - cost_metric: 250.0000 - val_loss: 0.1682 - val_auc: 0.9815 - val_accuracy: 0.9394 - val_cost_metric: 250.0000\n",
            "Epoch 6/10\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1664 - auc: 0.9819 - accuracy: 0.9408 - cost_metric: 250.0000 - val_loss: 0.1568 - val_auc: 0.9839 - val_accuracy: 0.9449 - val_cost_metric: 250.0000\n",
            "Epoch 7/10\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1582 - auc: 0.9836 - accuracy: 0.9442 - cost_metric: 250.0000 - val_loss: 0.1509 - val_auc: 0.9848 - val_accuracy: 0.9487 - val_cost_metric: 250.0000\n",
            "Epoch 8/10\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1522 - auc: 0.9847 - accuracy: 0.9477 - cost_metric: 250.0000 - val_loss: 0.1453 - val_auc: 0.9856 - val_accuracy: 0.9513 - val_cost_metric: 250.0000\n",
            "Epoch 9/10\n",
            "1600/1600 [==============================] - 4s 3ms/step - loss: 0.1454 - auc: 0.9857 - accuracy: 0.9504 - cost_metric: 250.0000 - val_loss: 0.1402 - val_auc: 0.9866 - val_accuracy: 0.9531 - val_cost_metric: 250.0000\n",
            "Epoch 10/10\n",
            "1600/1600 [==============================] - 4s 3ms/step - loss: 0.1406 - auc: 0.9865 - accuracy: 0.9529 - cost_metric: 250.0000 - val_loss: 0.1385 - val_auc: 0.9869 - val_accuracy: 0.9539 - val_cost_metric: 250.0000\n",
            "1000/1000 [==============================] - 1s 856us/step\n",
            "fold train/predict time: 0:00:47.733870\n",
            "x_train shape: (128000, 67)\n",
            "Epoch 1/10\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.3865 - auc: 0.9030 - accuracy: 0.8267 - cost_metric: 250.0000 - val_loss: 0.2836 - val_auc: 0.9510 - val_accuracy: 0.8826 - val_cost_metric: 250.0000\n",
            "Epoch 2/10\n",
            "1600/1600 [==============================] - 4s 3ms/step - loss: 0.2600 - auc: 0.9574 - accuracy: 0.8959 - cost_metric: 250.0000 - val_loss: 0.2208 - val_auc: 0.9694 - val_accuracy: 0.9155 - val_cost_metric: 250.0000\n",
            "Epoch 3/10\n",
            "1600/1600 [==============================] - 4s 3ms/step - loss: 0.2166 - auc: 0.9702 - accuracy: 0.9172 - cost_metric: 250.0000 - val_loss: 0.1922 - val_auc: 0.9763 - val_accuracy: 0.9277 - val_cost_metric: 250.0000\n",
            "Epoch 4/10\n",
            "1600/1600 [==============================] - 4s 3ms/step - loss: 0.1911 - auc: 0.9766 - accuracy: 0.9299 - cost_metric: 250.0000 - val_loss: 0.1754 - val_auc: 0.9801 - val_accuracy: 0.9355 - val_cost_metric: 250.0000\n",
            "Epoch 5/10\n",
            "1600/1600 [==============================] - 4s 3ms/step - loss: 0.1761 - auc: 0.9800 - accuracy: 0.9358 - cost_metric: 250.0000 - val_loss: 0.1650 - val_auc: 0.9823 - val_accuracy: 0.9410 - val_cost_metric: 250.0000\n",
            "Epoch 6/10\n",
            "1600/1600 [==============================] - 4s 3ms/step - loss: 0.1662 - auc: 0.9821 - accuracy: 0.9403 - cost_metric: 250.0000 - val_loss: 0.1599 - val_auc: 0.9830 - val_accuracy: 0.9434 - val_cost_metric: 250.0000\n",
            "Epoch 7/10\n",
            "1600/1600 [==============================] - 4s 3ms/step - loss: 0.1605 - auc: 0.9830 - accuracy: 0.9431 - cost_metric: 250.0000 - val_loss: 0.1532 - val_auc: 0.9844 - val_accuracy: 0.9467 - val_cost_metric: 250.0000\n",
            "Epoch 8/10\n",
            "1600/1600 [==============================] - 4s 3ms/step - loss: 0.1541 - auc: 0.9841 - accuracy: 0.9462 - cost_metric: 250.0000 - val_loss: 0.1499 - val_auc: 0.9850 - val_accuracy: 0.9491 - val_cost_metric: 250.0000\n",
            "Epoch 9/10\n",
            "1600/1600 [==============================] - 4s 3ms/step - loss: 0.1485 - auc: 0.9852 - accuracy: 0.9488 - cost_metric: 250.0000 - val_loss: 0.1432 - val_auc: 0.9860 - val_accuracy: 0.9516 - val_cost_metric: 250.0000\n",
            "Epoch 10/10\n",
            "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1453 - auc: 0.9858 - accuracy: 0.9504 - cost_metric: 250.0000 - val_loss: 0.1435 - val_auc: 0.9861 - val_accuracy: 0.9518 - val_cost_metric: 250.0000\n",
            "1000/1000 [==============================] - 1s 880us/step\n",
            "fold train/predict time: 0:00:46.157661\n",
            "total train/predict time: 0:03:56.016572\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model5_results = fold_results"
      ],
      "metadata": {
        "id": "Dgx85gYJjuWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds_m5 = np.zeros(len(y))\n",
        "for i in fold_results.keys():\n",
        "  for j in range(len(fold_results.get(i).get('predictions'))):\n",
        "    idx = fold_results.get(i).get('index')[j]\n",
        "    preds_m5[idx] = np.round(fold_results.get(i).get('predictions')[j],0)\n",
        "\n",
        "m5_cost = cost_func(y,preds_m5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0lDaXwGVq5f",
        "outputId": "08559b8e-5b2b-4030-9a22-ba3cec317a6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "960850"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds_new = np.zeros(len(y))\n",
        "for i in fold_results.keys():\n",
        "  for j in range(len(fold_results.get(i).get('predictions'))):\n",
        "    idx = fold_results.get(i).get('index')[j]\n",
        "    if fold_results.get(i).get('predictions')[j] > 0.4:\n",
        "      preds_new[idx] = 1\n",
        "    else:\n",
        "      preds_new[idx] = 0\n",
        "\n",
        "m5_cost_t = cost_func(y,preds_new)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKbDIA6RiBpY",
        "outputId": "9ebd9320-6f28-4ecd-a5fe-ffcfdc70badc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "708950"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model4.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jg2oQFvH2QS",
        "outputId": "e1bb0347-3744-44a1-ba0c-a4630212a863"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " Relu1 (Dense)               (None, 64)                4352      \n",
            "                                                                 \n",
            " Relu2 (Dense)               (None, 64)                4160      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " Activation (Dense)          (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8,577\n",
            "Trainable params: 8,577\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model5.summary()"
      ],
      "metadata": {
        "id": "RfNM29pgj5U2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Series(preds_m4-y).value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVNOkJr3oYGb",
        "outputId": "80da0d73-79e8-4000-bde2-49490bb1c24e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " 0.0    154724\n",
              "-1.0      3031\n",
              " 1.0      2245\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    }
  ]
}